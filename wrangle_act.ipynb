{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "## Dataset - WeRateDogs&trade; Twitter Archive\n",
    "\n",
    "***By: Kartik Nanduri***<br>\n",
    "**Date: 21st Nov, 2018.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests as req"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***<span style=\"color: red\">Important! uncomment the following files to run the book with out errors.</span>***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the folder structure.\n",
    "#os.rename('dataset/twitter-archive-enhanced.csv', 'twitter-archive-enhanced.csv')\n",
    "#import shutil\n",
    "#shutil.rmtree('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***<span style=\"color: green\">Important! once done, please recomment.</span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [x] **The file given at hand `twitter-archive-enhanced.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the requried files for this project are in the list files_list\n",
    "files_list = ['twitter-archive-enhanced.csv', 'image-predictions.tsv', 'tweet_json.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the twitter archive file\n",
    "archive = pd.read_csv(files_list[0])\n",
    "\n",
    "# taking at random entries for the archive file\n",
    "archive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [x] **Fetching the data from url and saving it to local drive - `image-predictions.tsv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file from internet using the requests library\n",
    "url = \"https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5bf60c69_image-predictions-3/image-predictions-3.tsv\"\n",
    "res = req.get(url)\n",
    "\n",
    "with open(files_list[1], mode = \"wb\") as op_file:\n",
    "    op_file.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if fetched the data right way\n",
    "img_pre_test = pd.read_csv(files_list[1], delimiter = \"\\t\", encoding = 'utf-8')\n",
    "img_pre_test.sample(2)\n",
    "\n",
    "# we did it the right way, Yay! it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [x] **Getting data from Twitter&trade;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries for accessing Twitter via API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up all the necessary placeholders for API\n",
    "consumer_key = 'xxx.xxx.xxx.xxx'\n",
    "consumer_secret = 'xxx.xxx.xxx.xxx'\n",
    "access_token = 'xxx.xxx.xxx.xxx'\n",
    "access_secret = 'xxx.xxx.xxx.xxx'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth_handler = auth,\n",
    "                 parser = tweepy.parsers.JSONParser(),\n",
    "                 wait_on_rate_limit = True, \n",
    "                 wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save(ids, api_ins, one_id = None):\n",
    "    '''\n",
    "    This function will fetch data with associated id in ids list\n",
    "    ids (List Object): a list all tweets\n",
    "    api_ins (Tweepy Object): api object instance, will be used to query twitter for data\n",
    "    one_id (int): use when you want to query only for one tweet\n",
    "    failed_ids (List Object): a list will be retured so that, this fuction can be called once again on those ids\n",
    "    '''\n",
    "    new_file_name = ''; failed_ids = []; tweet_df = []\n",
    "    \n",
    "    # checking if file exists\n",
    "    if os.path.exists(files_list[2]):\n",
    "        temp = [s for s in os.listdir() if \"tweet_json\" in s]\n",
    "        new_file_name = files_list[2].split('.')[0] + \"_\" + str(len(temp)) + \".txt\"\n",
    "    else:\n",
    "        new_file_name = files_list[2]\n",
    "    \n",
    "    # querying a list of ids\n",
    "    if one_id == None:\n",
    "        with open(new_file_name, mode = 'w') as outfile:\n",
    "            for one_id in ids:\n",
    "                try:\n",
    "                    content = api_ins.get_status(one_id, tweet_mode='extended')\n",
    "                    json.dump(content, outfile)\n",
    "                    outfile.write('\\n')\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "                    failed_ids.append(one_id)\n",
    "    \n",
    "    # querying a single id\n",
    "    else:\n",
    "        try:\n",
    "            content = api_ins.get_status(one_id, include_entities = True)\n",
    "            favorites = content['favorite_count']\n",
    "            retweets = content['retweet_count']\n",
    "            \n",
    "            tweet_df.append({'tweet_id': int(one_id),\n",
    "                        'favorites': int(favorites),\n",
    "                        'retweets': int(retweets)})\n",
    "            \n",
    "            return tweet_df\n",
    "                           \n",
    "        except Exception as e:\n",
    "            print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "            failed_ids.append(one_id)\n",
    "\n",
    "    return failed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the list of ids to the fuction fetch_and_save(), but in batches\n",
    "# given that we can request 900 request/15min - window, let's break our ids into\n",
    "tweet_ids = archive['tweet_id'].tolist()\n",
    "\n",
    "# set_one, two and three\n",
    "set_one = tweet_ids[0:900]; set_two = tweet_ids[900:1800]; set_three = tweet_ids[1800:]\n",
    "\n",
    "# checking the lengths so that we send 900 ids/requests.\n",
    "print(len(set_one), len(set_two), len(set_three), len(set_one)+len(set_two)+len(set_three))\n",
    "print(len(set_one)+len(set_two)+len(set_three) == len(tweet_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# fetching data 1st iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_one = fetch_and_save(set_one, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_one)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleeping for 6 mins, so that Rate Limit time is reduced\n",
    "print(\"Sleeping for 6 mins.\")\n",
    "time.sleep(360)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "# fetching data 2nd iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_two = fetch_and_save(set_two, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_two)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleeping for 6 mins, so that Rate Limit time is reduced\n",
    "print(\"Sleeping for 6 mins.\")\n",
    "time.sleep(360)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "# fetching data 3rd iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_three = fetch_and_save(set_three, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_three)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the failed ids into one master list\n",
    "failed_ids = test_one + test_two + test_three\n",
    "print(\"Total failed request are: {}. \\n\".format(len(failed_ids)))\n",
    "\n",
    "# ids that failed and the ones that passed\n",
    "indi_fail = []; success = []\n",
    "\n",
    "#for each failed id, lets try to fetch status individually.\n",
    "for failed_id in failed_ids:\n",
    "    temp = fetch_and_save(ids = None, api_ins = api, one_id = failed_id)\n",
    "    indi_fail.append(temp[0])\n",
    "\n",
    "# removing empty elements from list\n",
    "success = [x for x in indi_fail if not isinstance(x, (int))]\n",
    "indi_fail = [x for x in indi_fail if isinstance(x, (int))]\n",
    "\n",
    "# checking if there is change\n",
    "print(\"\\nWe were able to retrieve {} records, others failed.\".format(len(failed_ids) - len(indi_fail)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [x] **Okay, let's combine the successful jsons into one file, called the `tweet_master.txt`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combing all successful jsons into one master file\n",
    "json_1 = pd.read_json('tweet_json.txt', lines = True, encoding = 'utf-8')\n",
    "json_2 = pd.read_json('tweet_json_1.txt', lines = True, encoding = 'utf-8')\n",
    "json_3 = pd.read_json('tweet_json_2.txt', lines = True, encoding = 'utf-8')\n",
    "\n",
    "# total rows that we need to have in our resulting dataframe\n",
    "print(json_1.shape[0] + json_2.shape[0] + json_3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_master = pd.concat([json_1, json_2, json_3], ignore_index = True, join = 'outer', sort = True)\n",
    "json_master.to_json('tweet_master.txt', orient = 'records', lines = True)\n",
    "json_master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing objects that are not required.\n",
    "del archive, img_pre_test\n",
    "del json_1, json_2, json_3, json_master\n",
    "del indi_fail, end, start, test_one, test_two, test_three, set_one, set_two, set_three\n",
    "del consumer_key, consumer_secret, access_token, access_secret, auth, api\n",
    "\n",
    "# we are not removing success and files_list, making sure we stick to good programming practices - reusablity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [x] **Last thing to do is to tidy up our folder, let's get going.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'Checkupdate.csv',\n",
       " 'dataset',\n",
       " 'error.png',\n",
       " 'New Text Document.txt',\n",
       " 'README.md',\n",
       " 'test.py',\n",
       " 'text.csv',\n",
       " 'twitter-archive-enhanced.xlsx',\n",
       " 'twitter_text.csv',\n",
       " 'wrangle_act.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moving all data files under one folder - dataset\n",
    "# removing the temporary files, that acted as placeholders\n",
    "\n",
    "# creating the folder\n",
    "folder = 'dataset'\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "# we know that our master datasets for this project are\n",
    "# 1. twitter-archive-enhanced.csv\n",
    "# 2. image-predictions.tsv\n",
    "# 3. tweet_json_master.txt\n",
    "# let us move these files\n",
    "\n",
    "# updating our files_list\n",
    "files_list[-1] = 'tweet_master.txt'\n",
    "\n",
    "# moving only required files\n",
    "for file in files_list:\n",
    "    if os.path.exists(file):\n",
    "        os.rename(file, folder+'/'+file)\n",
    "\n",
    "# removing the tweet_json and tweet_json_1 files as they are not required anymore\n",
    "for file in [s for s in os.listdir() if \"tweet_json\" in s]:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    \n",
    "# lisitng the current directory\n",
    "os.listdir()\n",
    "\n",
    "# clean and neat, lets get with assessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming files_list\n",
    "for i in range(3):\n",
    "    files_list[i] = folder + '/'+ files_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [x] **Last thing to do is to extract `retweet_count` and `favourite_count` from `tweet_master.txt`, saving the result as .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending success to the master dataset.\n",
    "tweet_json = pd.read_json(files_list[2], lines = True, encoding = 'utf-8')\n",
    "tweet_json = tweet_json[['id', 'retweet_count','favorite_count']]\n",
    "tweet_json.rename(index = str,\n",
    "                  columns={'id' : 'tweet_id', 'retweet_count': 'retweets','favorite_count': 'favorites'},\n",
    "                  inplace = True)\n",
    "tweet_json = pd.concat([tweet_json, pd.DataFrame.from_dict(success)],\n",
    "                       ignore_index = True, sort = True)\n",
    "tweet_json.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe into master file\n",
    "tweet_json.to_json(files_list[2], orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Gathering\n",
    "\n",
    "- We know, that gathering is a the first step in wrangling.\n",
    "- We were successful in gathering from three different sources with different techniques:\n",
    "    - Data given at hand.\n",
    "    - Fetch from flat file stored on a server.\n",
    "    - From API.\n",
    "\n",
    "- There a total of 14 missing data points, tried a different ways for retrieving them, using the API as well as `twurl` of the `Ruby` package, but they were not to be found, as stated below in the highlighted section.\n",
    "\n",
    "***<span style=\"color: ##6c6cff\">So let's start with assessing the data.</span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![error](error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load up dataset, and starting assessing them.\n",
    "archive =  pd.read_csv(files_list[0], encoding = 'utf-8')\n",
    "img_pre = pd.read_csv(files_list[1], sep = '\\t', encoding = 'utf-8')\n",
    "retweets_fav = pd.read_json(files_list[2], lines = True, encoding =  'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues to sort!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing out archive - visual assessment\n",
    "archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic Assessment 1 - Information\n",
    "archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmtic Assessment 2 - Describe\n",
    "archive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates - tweet_ids\n",
    "sum(archive.tweet_id.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkin if we have more than one class of dogs assigned to dog\n",
    "# the following are the only combinations that are present in the dataset\n",
    "cond_1 = (archive['doggo'] == 'doggo') & (archive['floofer'] == 'floofer')\n",
    "cond_2 = (archive['doggo'] == 'doggo') & (archive['pupper'] == 'pupper')\n",
    "cond_3 = (archive['doggo'] == 'doggo') & (archive['puppo'] == 'puppo')\n",
    "\n",
    "# printing these entries\n",
    "archive[cond_1 | cond_2 | cond_3][['tweet_id', 'text', 'doggo', 'floofer', 'pupper', 'puppo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(archive[cond_1 | cond_2 | cond_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`twitter-archive-enhanced.csv`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- `rating_numerator` : has values such as 1, 3.. e.t.c - **Data Quality Dimension - `Consistency`**.\n",
    "- `rating_denominator` : have values, less than 10, for example, the tweet_id - 666287406224695296 has the number 2 as its value - **Data Quality Dimension - `Consistency`**. \n",
    "- We see that, Articles - `a`, `an`, `the` have been used to name dogs, as well as words such as `such`, `quite` - **Data Quality Dimension - `Validity`**.\n",
    "- There are instances where the names of dogs are in lowercases - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- `rating_numerator` : has a maximum value of 1766 - **Data Quality Dimension - `Consistency`**. \n",
    "- `rating_denominator` : has a maximum value of 170 - **Data Quality Dimension - `Consistency`**.\n",
    "- All in all, this dataset appears to be clean, except for `expanded_url` - we have about 59 instances missing - **Data Quality Dimension - `Completeness`**.\n",
    "- We can see that there are more than one class assigned to tweets, analyze and assign proper dog class so that melting is easy - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- we can see that, there are four classes of dogs `doggo`, `floofer`, `puppo`, `pupper`; these should a part of one unit - `dog_class` - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- `in_reply_to_status_id`, `retweeted_status_id`, `retweeted_status_user_id`, `in_reply_to_user_id` of type float64 must be converted into int - **Data Quality Dimension - `Validity`**.\n",
    "- `timestamp`, `retweeted_status_timestamp` of type object must be converted into datatime - **Data Quality Dimension - `Validity`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing img_predictions dataset\n",
    "img_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic Assessment - Information\n",
    "img_pre.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates\n",
    "img_pre[img_pre['jpg_url'].duplicated(keep = False)].sort_values(by = 'jpg_url')[['tweet_id', 'jpg_url']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **`image-predictions.tsv`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- We have few dog breeds that are represented in lowercase.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- We have about 281 images on a whole, that are missing with respect to our `twitter-archive-enhanced.csv` file - **Data Quality Dimension - `Completeness`**.\n",
    "- We can see that, we have about `66` duplicates **OR** a pair of tweets are pointing to same *`jpg_url`* - **Data Quality Dimension - `Accuracy`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- None. \n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing tweet_master dataset\n",
    "retweets_fav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_fav.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **`tweet_master.txt`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- None.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- We have about 14 missing records - **Data Quality Dimension - `Completeness`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- None.\n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Assessing\n",
    "\n",
    "- Completed the second step.\n",
    "- The following are the insights:\n",
    "    - from `twitter-archive-enhanced.csv` datset, the rating_numerator and denominator need to be fixed.\n",
    "    - the dataset also represents row values as columns, which needs to be fixed.\n",
    "    - the dataset also has structural issues such as wrong datatype assigned to a column.\n",
    "    - from `images-preductions.tsv` dataset, there is consistency issue with naming dog breeds.\n",
    "    - the dataset isn't complete when compared to `twitter-archive-enhanced.csv`, we have about 281 missing tweets.\n",
    "    - Also we have `jpg_urls'` that are pointing to a pair of tweets.\n",
    "    - `tweet_master.txt` dataset has about 14 missing records.\n",
    "    - the dataset alone hold the information about retweets and favourites - bad form of schema normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Important!, before we get to cleaning, let's drop rows from image-predictions, that are false in dog_1,_2 and _3, as they are not related to our dataset.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select those rows that are either true or false and not all false\n",
    "img_pre = img_pre[~((img_pre.p1_dog == False) & (img_pre.p2_dog == False) & (img_pre.p3_dog == False))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asserting the lenght to be 0\n",
    "assert len(img_pre[(img_pre.p1_dog == False) & (img_pre.p2_dog == False) & (img_pre.p3_dog == False)]) == 0, \"Check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2356 entries, 0 to 2355\n",
      "Data columns (total 30 columns):\n",
      "tweet_id                      2356 non-null int64\n",
      "in_reply_to_status_id         78 non-null float64\n",
      "in_reply_to_user_id           78 non-null float64\n",
      "timestamp                     2356 non-null object\n",
      "source                        2356 non-null object\n",
      "text                          2356 non-null object\n",
      "retweeted_status_id           181 non-null float64\n",
      "retweeted_status_user_id      181 non-null float64\n",
      "retweeted_status_timestamp    181 non-null object\n",
      "expanded_urls                 2297 non-null object\n",
      "rating_numerator              2356 non-null int64\n",
      "rating_denominator            2356 non-null int64\n",
      "name                          2356 non-null object\n",
      "doggo                         2356 non-null object\n",
      "floofer                       2356 non-null object\n",
      "pupper                        2356 non-null object\n",
      "puppo                         2356 non-null object\n",
      "jpg_url                       1751 non-null object\n",
      "img_num                       1751 non-null float64\n",
      "p1                            1751 non-null object\n",
      "p1_conf                       1751 non-null float64\n",
      "p1_dog                        1751 non-null object\n",
      "p2                            1751 non-null object\n",
      "p2_conf                       1751 non-null float64\n",
      "p2_dog                        1751 non-null object\n",
      "p3                            1751 non-null object\n",
      "p3_conf                       1751 non-null float64\n",
      "p3_dog                        1751 non-null object\n",
      "favorites                     2342 non-null float64\n",
      "retweets                      2342 non-null float64\n",
      "dtypes: float64(10), int64(3), object(17)\n",
      "memory usage: 570.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# the master dataset\n",
    "master_set = archive.merge(img_pre, how = 'left', on = ['tweet_id'])\n",
    "master_set = master_set.merge(retweets_fav, how = 'left', on = ['tweet_id'])\n",
    "files_list.append('dataset/master_set_raw.csv')\n",
    "master_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file to local disk.\n",
    "master_set.to_csv(files_list[3], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the master set\n",
    "master_set = pd.read_csv(files_list[3], encoding = 'utf-8')\n",
    "master_copy = master_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues to Clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Basic cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Assign proper class for the above 14 tweets before melting.\n",
    "- Delete *retweets* with *any duplicates* and get rid of *tweets with **no** images*.\n",
    "- Once done, drop the following columns:\n",
    "    1. `retweeted_status_id`\n",
    "    2. `retweeted_status_user_id`\n",
    "    3. `retweeted_status_timestamp`\n",
    "    4. `in_reply_to_status_id`\n",
    "    5. `in_reply_to_user_id`\n",
    "    \n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tweet_id  \\\n",
      "191   855851453814013952   \n",
      "200   854010172552949760   \n",
      "460   817777686764523521   \n",
      "531   808106460588765185   \n",
      "565   802265048156610565   \n",
      "575   801115127852503040   \n",
      "705   785639753186217984   \n",
      "733   781308096455073793   \n",
      "778   775898661951791106   \n",
      "822   770093767776997377   \n",
      "889   759793422261743616   \n",
      "956   751583847268179968   \n",
      "1063  741067306818797568   \n",
      "1113  733109485275860992   \n",
      "\n",
      "                                                                                                                                                                      text  \n",
      "191   Here's a puppo participating in the #ScienceMarch. Cleverly disguising her own doggo agenda. 13/10 would keep the planet habitable for https://t.co/cMhq16isel        \n",
      "200   At first I thought this was a shy doggo, but it's actually a Rare Canadian Floofer Owl. Amateurs would confuse the two. 11/10 only send dogs https://t.co/TXdT3tmuYk  \n",
      "460   This is Dido. She's playing the lead role in \"Pupper Stops to Catch Snow Before Resuming Shadow Box with Dried Apple.\" 13/10 (IG: didodoggo) https://t.co/m7isZrOBX7  \n",
      "531   Here we have Burke (pupper) and Dexter (doggo). Pupper wants to be exactly like doggo. Both 12/10 would pet at same time https://t.co/ANBpEYHaho                      \n",
      "565   Like doggo, like pupper version 2. Both 11/10 https://t.co/9IxWAXFqze                                                                                                 \n",
      "575   This is Bones. He's being haunted by another doggo of roughly the same size. 12/10 deep breaths pupper everything's fine https://t.co/55Dqe0SJNj                      \n",
      "705   This is Pinot. He's a sophisticated doggo. You can tell by the hat. Also pointier than your average pupper. Still 10/10 would pet cautiously https://t.co/f2wmLZTPHd  \n",
      "733   Pupper butt 1, Doggo 0. Both 12/10 https://t.co/WQvcPEpH2u                                                                                                            \n",
      "778   RT @dog_rates: Like father (doggo), like son (pupper). Both 12/10 https://t.co/pG2inLaOda                                                                             \n",
      "822   RT @dog_rates: This is just downright precious af. 12/10 for both pupper and doggo https://t.co/o5J479bZUC                                                            \n",
      "889   Meet Maggie &amp; Lila. Maggie is the doggo, Lila is the pupper. They are sisters. Both 12/10 would pet at the same time https://t.co/MYwR4DQKll                      \n",
      "956   Please stop sending it pictures that don't even have a doggo or pupper in them. Churlish af. 5/10 neat couch tho https://t.co/u2c9c7qSg8                              \n",
      "1063  This is just downright precious af. 12/10 for both pupper and doggo https://t.co/o5J479bZUC                                                                           \n",
      "1113  Like father (doggo), like son (pupper). Both 12/10 https://t.co/pG2inLaOda                                                                                            \n"
     ]
    }
   ],
   "source": [
    "# setting column width to -1\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "cond_1 = (master_copy['doggo'] == 'doggo') & (master_copy['floofer'] == 'floofer')\n",
    "cond_2 = (master_copy['doggo'] == 'doggo') & (master_copy['pupper'] == 'pupper')\n",
    "cond_3 = (master_copy['doggo'] == 'doggo') & (master_copy['puppo'] == 'puppo')\n",
    "print(master_copy[cond_1 | cond_2 | cond_3][['tweet_id', 'text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Assign the following:***\n",
    "1. 855851453814013952: puppo\n",
    "2. 854010172552949760: floofer\n",
    "3. 817777686764523521: pupper\n",
    "4. 808106460588765185: pupper\n",
    "5. 802265048156610565: pupper\n",
    "6. 801115127852503040: pupper\n",
    "7. 785639753186217984: pupper\n",
    "8. 781308096455073793: pupper\n",
    "9. 775898661951791106: pupper\n",
    "10. 770093767776997377: pupper\n",
    "11. 759793422261743616: pupper\n",
    "12. 751583847268179968: doggo\n",
    "13. 741067306818797568: doggo\n",
    "14. 733109485275860992: doggo\n",
    "\n",
    "**<span style=\"color: green\">I like puppies, so for most of the entries it is pupper!</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning values.\n",
    "master_copy.loc[master_copy['tweet_id'] == 855851453814013952, ['doggo', 'floofer', 'pupper']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 854010172552949760, ['doggo', 'pupper', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 817777686764523521, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 808106460588765185, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 802265048156610565, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 801115127852503040, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 785639753186217984, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 781308096455073793, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 775898661951791106, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 770093767776997377, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 759793422261743616, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 751583847268179968, ['pupper', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 741067306818797568, ['pupper', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 733109485275860992, ['pupper', 'floofer', 'puppo']] = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>855851453814013952</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>puppo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>854010172552949760</td>\n",
       "      <td>None</td>\n",
       "      <td>floofer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>817777686764523521</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>808106460588765185</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>802265048156610565</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>801115127852503040</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>785639753186217984</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>781308096455073793</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>775898661951791106</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>770093767776997377</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>759793422261743616</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>751583847268179968</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>741067306818797568</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>733109485275860992</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  doggo  floofer  pupper  puppo\n",
       "191   855851453814013952   None     None    None  puppo\n",
       "200   854010172552949760   None  floofer    None   None\n",
       "460   817777686764523521   None     None  pupper   None\n",
       "531   808106460588765185   None     None  pupper   None\n",
       "565   802265048156610565   None     None  pupper   None\n",
       "575   801115127852503040   None     None  pupper   None\n",
       "705   785639753186217984   None     None  pupper   None\n",
       "733   781308096455073793   None     None  pupper   None\n",
       "778   775898661951791106   None     None  pupper   None\n",
       "822   770093767776997377   None     None  pupper   None\n",
       "889   759793422261743616   None     None  pupper   None\n",
       "956   751583847268179968  doggo     None    None   None\n",
       "1063  741067306818797568  doggo     None    None   None\n",
       "1113  733109485275860992  doggo     None    None   None"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all values have been properly assigned\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "master_copy[cond_1 | cond_2 | cond_3][['tweet_id', 'doggo', 'floofer', 'pupper', 'puppo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting those, tweets that have no retweets\n",
    "master_copy = master_copy[pd.isnull(master_copy['retweeted_status_id'])]\n",
    "\n",
    "# deleting duplicates if any\n",
    "master_copy = master_copy.drop_duplicates()\n",
    "\n",
    "# deleting those tweets with no images.\n",
    "master_copy = master_copy.dropna(subset = ['jpg_url'])\n",
    "\n",
    "# reseting index\n",
    "master_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# droping columns\n",
    "master_copy = master_copy.drop(labels = ['retweeted_status_id',\n",
    "                                         'retweeted_status_user_id',\n",
    "                                         'retweeted_status_timestamp',\n",
    "                                         'in_reply_to_status_id', \n",
    "                                         'in_reply_to_user_id'],\n",
    "                               axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1686, 25)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after droping the columns, we should have about 25 dimensions/columns\n",
    "master_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Condense wide-format to long-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Condense `doggo`, `floofer`, `pupper`, `puppo` as `dog_class`.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Doggo: 57\n",
      "Count of Floofer: 8\n",
      "Count of Pupper: 173\n",
      "Count of Puppo: 22\n"
     ]
    }
   ],
   "source": [
    "# to make sure that we have \n",
    "doggo = master_copy.doggo.value_counts()['doggo']\n",
    "floofer = master_copy.floofer.value_counts()['floofer']\n",
    "pupper = master_copy.pupper.value_counts()['pupper']\n",
    "puppo = master_copy.puppo.value_counts()['puppo']\n",
    "\n",
    "# printing count of each class\n",
    "print(\"Count of Doggo: {}\\nCount of Floofer: {}\\nCount of Pupper: {}\\nCount of Puppo: {}\".format(doggo,\n",
    "                                                                                                 floofer,\n",
    "                                                                                                 pupper,\n",
    "                                                                                                 puppo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the columns that are to be melted\n",
    "columns_to_melt = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "columns_to_stay = [x for x in master_copy.columns.tolist() if x not in columns_to_melt]\n",
    "\n",
    "# melting the the columns into values\n",
    "master_copy = pd.melt(master_copy, id_vars = columns_to_stay, value_vars = columns_to_melt, \n",
    "                         var_name = 'stages', value_name = 'dog_class')\n",
    "\n",
    "# Delete column 'stages'\n",
    "master_copy = master_copy.drop('stages', 1)\n",
    "\n",
    "# dropping duplicates\n",
    "master_copy = master_copy.sort_values('dog_class').drop_duplicates('tweet_id', keep = 'last')\n",
    "master_copy.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assert\n",
    "assert doggo == master_copy.dog_class.value_counts()['doggo'], \"Some entries are missing\"\n",
    "assert floofer == master_copy.dog_class.value_counts()['floofer'], \"Some entries are missing\"\n",
    "assert pupper == master_copy.dog_class.value_counts()['pupper'], \"Some entries are missing\"\n",
    "assert puppo == master_copy.dog_class.value_counts()['puppo'], \"Some entries are missing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fix all inaccurate data.\n",
    "\n",
    "#### Define\n",
    "- fix names of dogs.\n",
    "- fix ratings.\n",
    "- check source column.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking source column\n",
    "master_copy.source.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red\">Okay! only three values, a categorical variable</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# assiging unique values to source.\n",
    "master_copy['source'] = master_copy['source'].apply(lambda x: re.findall(r'>(.*)<', x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>840268004936019968</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>This is Monty. He makes instantly regrettable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>717009362452090881</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>This is Smokey. He's having some sort of exist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>793226087023144960</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>This is Juno. She spooked me up real good, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>813217897535406080</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Here is Atlas. He went all out this year. 13/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>697575480820686848</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>This is Ole. He's not sure how to gravity. 8/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id              source  \\\n",
       "1150  840268004936019968  Twitter for iPhone   \n",
       "1520  717009362452090881  Twitter for iPhone   \n",
       "1006  793226087023144960  Twitter for iPhone   \n",
       "871   813217897535406080  Twitter for iPhone   \n",
       "232   697575480820686848  Twitter for iPhone   \n",
       "\n",
       "                                                   text  \n",
       "1150  This is Monty. He makes instantly regrettable ...  \n",
       "1520  This is Smokey. He's having some sort of exist...  \n",
       "1006  This is Juno. She spooked me up real good, but...  \n",
       "871   Here is Atlas. He went all out this year. 13/1...  \n",
       "232   This is Ole. He's not sure how to gravity. 8/1...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a look at sample of 5 rows\n",
    "master_copy.sample(5)[['tweet_id', 'source', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing names\n",
    "non_names = master_copy.name.str.islower()\n",
    "non_names = list(set(master_copy[non_names]['name'].tolist()))\n",
    "flag = master_copy.name.str.len() == 1 & master_copy.name.str.isupper()\n",
    "non_names.append(master_copy[flag][['tweet_id', 'name']]['name'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all garbage names with none, once done, we'll use the text field to extract names\n",
    "for name in master_copy.name:\n",
    "    if name in non_names:\n",
    "        master_copy.loc[master_copy['name'] == name, ['name']] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are any non_names after the operation\n",
    "assert len(master_copy[(master_copy.name.str.islower()) & (flag)]) == 0, \"Check code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The following are patterns observed in `text` field, we shall use the :***\n",
    "- This is [name] ..\n",
    "- Meet [name] ..\n",
    "- Say hello to [name] ..\n",
    "- .. named [name] ..\n",
    "- .. name is [name] ..\n",
    "\n",
    "We will treat those cases to get the names from the text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting names using regular expression.\n",
    "dog_names = []\n",
    "\n",
    "# assigning patterns\n",
    "pattern_1 = r'(T|t)his\\sis\\s([^.|,]*)'\n",
    "pattern_2 = r'Meet\\s([^.|,]*)'\n",
    "pattern_3 = r'Say\\shello\\sto\\s([^.|,]*)'\n",
    "pattern_4 = r'name\\sis\\s([^.|,]*)'\n",
    "\n",
    "# looping through text and extracting names\n",
    "for text in master_copy['text']:\n",
    "    # Start with 'This is '\n",
    "    if re.search(pattern_1, text):\n",
    "        # if our match has alternate name\n",
    "        if \"(\" in re.search(pattern_1, text).group(2):\n",
    "            dog_names.append(re.search(pattern_1, text).group(2).split()[0])\n",
    "        # if our match has AKA in it\n",
    "        elif \"AKA\" in re.search(pattern_1, text).group(2):\n",
    "            dog_names.append(re.search(pattern_1, text).group(2).split()[0])\n",
    "        # if our name has two dogs\n",
    "        elif '&amp;' in re.search(pattern_1, text).group(2):\n",
    "            temp = re.search(pattern_1, text).group(2).split()\n",
    "            if len(temp) == 1:\n",
    "                dog_names.append(temp[0])\n",
    "            elif len(temp) == 3:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-1])\n",
    "            else:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-2])\n",
    "        elif 'named' in re.search(pattern_1, text).group(2):\n",
    "            temp = re.search(pattern_1, text).group(2).split()\n",
    "            dog_names.append(temp[-1])\n",
    "        # just appending the name\n",
    "        else:\n",
    "            dog_names.append(re.search(pattern_1, text).group(2))\n",
    "    \n",
    "    # Start with 'Meet '\n",
    "    elif re.search(pattern_2, text):\n",
    "        # if our name has two dogs\n",
    "        if '&amp;' in re.search(pattern_2, text).group(1):\n",
    "            temp = re.search(pattern_2, text).group(1).split()\n",
    "            if len(temp) == 1:\n",
    "                dog_names.append(temp[0])\n",
    "            elif len(temp) == 3:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-1])\n",
    "            else:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-2])\n",
    "        # if our name has alternatives\n",
    "        elif '(' in re.search(pattern_2, text).group(1):\n",
    "            dog_names.append(re.search(pattern_2, text).group(1).split()[0])\n",
    "        # just appending the name\n",
    "        else:\n",
    "            dog_names.append(re.search(pattern_2, text).group(1))\n",
    "    \n",
    "    # Start with 'Say hello to '\n",
    "    elif re.search(pattern_3, text):\n",
    "        # if our match has alternate name\n",
    "        if '(' in re.search(pattern_3, text).group(1):\n",
    "            dog_names.append(re.search(pattern_3, text).group(1).split()[0])\n",
    "        # if our name has two dogs\n",
    "        elif '&amp;' in re.search(pattern_3, text).group(1):\n",
    "            temp = re.search(pattern_3, text).group(1).split()\n",
    "            if len(temp) == 1:\n",
    "                dog_names.append(temp[0])\n",
    "            elif len(temp) == 3:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-1])\n",
    "            else:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-2])\n",
    "        else:\n",
    "            dog_names.append(re.search(pattern_3, text).group(1))    \n",
    "    \n",
    "    # contains 'name is'\n",
    "    elif re.search(pattern_4, text):\n",
    "        if len(re.search(pattern_4, text).group(1).split()) == 1:\n",
    "            dog_names.append(re.search(pattern_4, text).group(1))\n",
    "        else:\n",
    "            temp = re.search(pattern_4, text).group(1).split()\n",
    "            dog_names.append(temp[0])\n",
    "        \n",
    "    # No name specified or other style\n",
    "    else:\n",
    "        dog_names.append('None')\n",
    "\n",
    "# adding this new set of names to our master_copy\n",
    "master_copy['dog_names'] = dog_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new non names.\n",
    "non_names = []\n",
    "pattern_4 = r'^[a-z].*'\n",
    "for name in master_copy['dog_names']:\n",
    "    if re.search(pattern_4, name):\n",
    "        master_copy.loc[master_copy['dog_names'] == name, ['dog_names']] = 'None'\n",
    "        non_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog_names with and to be replaced with |\n",
    "for name in master_copy['dog_names']:\n",
    "    master_copy['dog_names'] = master_copy['dog_names'].str.replace(pat = r'\\sand\\s', repl = \"|\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>dog_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>667495797102141441</td>\n",
       "      <td>This is Philippe from Soviet Russia. Commanding leader. Misplaced other boot. Hung flag himself. 9/10 charismatic af https://t.co/5NhPV8E45i</td>\n",
       "      <td>Philippe from Soviet Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>693280720173801472</td>\n",
       "      <td>This is Sadie and her 2 pups Shebang &amp;amp; Ruffalo. Sadie says single parenting is challenging but rewarding. All 10/10 https://t.co/UzbhwXcLne</td>\n",
       "      <td>Sadie|&amp;amp;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id  \\\n",
       "23   667495797102141441   \n",
       "160  693280720173801472   \n",
       "\n",
       "                                                                                                                                                text  \\\n",
       "23   This is Philippe from Soviet Russia. Commanding leader. Misplaced other boot. Hung flag himself. 9/10 charismatic af https://t.co/5NhPV8E45i      \n",
       "160  This is Sadie and her 2 pups Shebang &amp; Ruffalo. Sadie says single parenting is challenging but rewarding. All 10/10 https://t.co/UzbhwXcLne   \n",
       "\n",
       "                       dog_names  \n",
       "23   Philippe from Soviet Russia  \n",
       "160  Sadie|&amp;                  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to replace two cells, with names 'Sadie|&amp;', 'Phillippe ...',\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "master_copy[(master_copy['dog_names'] == \"Philippe from Soviet Russia\") |\n",
    "            (master_copy['dog_names'] == \"Sadie|&amp;\")][['tweet_id', 'text', 'dog_names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting them to correct ones\n",
    "# the following tweet_id we different from our regexs\n",
    "master_copy.loc[master_copy['dog_names'] == \"Philippe from Soviet Russia\", ['dog_names']] = 'Phillippe'\n",
    "master_copy.loc[master_copy['dog_names'] == \"Sadie|&amp;\", ['dog_names']] = 'Sadie|Shebang|Ruffalo'\n",
    "master_copy.loc[master_copy['tweet_id'] == 667509364010450944, ['dog_names']] = 'Tickles'\n",
    "master_copy.loc[master_copy['tweet_id'] == 667546741521195010, ['dog_names']] = 'George'\n",
    "master_copy.loc[master_copy['tweet_id'] == 667073648344346624, ['dog_names']] = 'Dave'\n",
    "master_copy.loc[master_copy['tweet_id'] == 667177989038297088, ['dog_names']] = 'Daryl'\n",
    "master_copy.loc[master_copy['tweet_id'] == 666835007768551424, ['dog_names']] = 'Cupit|Prencer'\n",
    "master_copy.loc[master_copy['tweet_id'] == 668221241640230912, ['dog_names']] = 'Bo|Smittens'\n",
    "master_copy.loc[master_copy['tweet_id'] == 668268907921326080, ['dog_names']] = 'Guss'\n",
    "master_copy.loc[master_copy['tweet_id'] == 666058600524156928, ['dog_names']] = 'Paul Rand'\n",
    "master_copy.loc[master_copy['tweet_id'] == 692142790915014657, ['dog_names']] = 'Teddy'\n",
    "master_copy.loc[master_copy['tweet_id'] == 684097758874210310, ['dog_names']] = 'Lupe'\n",
    "master_copy.loc[master_copy['tweet_id'] == 709198395643068416, ['dog_names']] = 'Cletus|Jerome|Alejandro|Burp|Titson'\n",
    "master_copy.loc[master_copy['tweet_id'] == 671743150407421952, ['dog_names']] = 'Jacob'\n",
    "master_copy.loc[master_copy['tweet_id'] == 669037058363662336, ['dog_names']] = 'Pancho|Peaches'\n",
    "master_copy.loc[master_copy['tweet_id'] == 669363888236994561, ['dog_names']] = 'Zeus'\n",
    "master_copy.loc[master_copy['tweet_id'] == 813217897535406080, ['dog_names']] = 'Atlas'\n",
    "master_copy.loc[master_copy['tweet_id'] == 856526610513747968, ['dog_names']] = 'Charlie'\n",
    "master_copy.loc[master_copy['tweet_id'] == 861288531465048066, ['dog_names']] = 'Boomer'\n",
    "master_copy.loc[master_copy['tweet_id'] == 863079547188785154, ['dog_names']] = 'Pipsy'\n",
    "master_copy.loc[master_copy['tweet_id'] == 844979544864018432, ['dog_names']] = 'Toby'\n",
    "master_copy.loc[master_copy['tweet_id'] == 836001077879255040, ['dog_names']] = 'Atlas'\n",
    "master_copy.loc[master_copy['tweet_id'] == 758041019896193024, ['dog_names']] = 'Teagan'\n",
    "master_copy.loc[master_copy['tweet_id'] == 765395769549590528, ['dog_names']] = 'Zoey'\n",
    "master_copy.loc[master_copy['tweet_id'] == 778408200802557953, ['dog_names']] = 'Loki'\n",
    "master_copy.loc[master_copy['tweet_id'] == 770069151037685760, ['dog_names']] = 'Carbon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweet_id',\n",
       " 'timestamp',\n",
       " 'source',\n",
       " 'text',\n",
       " 'expanded_urls',\n",
       " 'rating_numerator',\n",
       " 'rating_denominator',\n",
       " 'jpg_url',\n",
       " 'img_num',\n",
       " 'p1',\n",
       " 'p1_conf',\n",
       " 'p1_dog',\n",
       " 'p2',\n",
       " 'p2_conf',\n",
       " 'p2_dog',\n",
       " 'p3',\n",
       " 'p3_conf',\n",
       " 'p3_dog',\n",
       " 'favorites',\n",
       " 'retweets',\n",
       " 'dog_class',\n",
       " 'dog_names']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping column name\n",
    "master_copy = master_copy.drop(['name'], axis = 1)\n",
    "\n",
    "# printing columns in master_copy\n",
    "master_copy.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>dog_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>667793409583771648</td>\n",
       "      <td>Dogs only please. Small cows and other non canines will not be tolerated. Sick tattoos tho 8/10 https://t.co/s1z7mX4c9O</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>667832474953625600</td>\n",
       "      <td>THE EYES 12/10\\r\\n\\r\\nI'm sorry. These are supposed to be funny but your dogs are too adorable https://t.co/z1xPTgVLc7</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>667801013445750784</td>\n",
       "      <td>OMIGOD 12/10 https://t.co/SVMF4Frf1w</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>667491009379606528</td>\n",
       "      <td>Two dogs in this one. Both are rare Jujitsu Pythagoreans. One slightly whiter than other. Long legs. 7/10 and 8/10 https://t.co/ITxxcc4v9y</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>667044094246576128</td>\n",
       "      <td>12/10 gimme now https://t.co/QZAnwgnOMB</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>667138269671505920</td>\n",
       "      <td>Extremely intelligent dog here. Has learned to walk like human. Even has his own dog. Very impressive 10/10 https://t.co/0DvHAMdA4V</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>667176164155375616</td>\n",
       "      <td>These are strange dogs. All have toupees. Long neck for dogs. In a shed of sorts? Work in groups? 4/10 still petable https://t.co/PZxSarAfSN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>667192066997374976</td>\n",
       "      <td>*takes several long deep breaths* omg omg oMG OMG OMG OMGSJYBSNDUYWJO 12/10 https://t.co/QCugm5ydl6</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>667435689202614272</td>\n",
       "      <td>Ermergerd 12/10 https://t.co/PQni2sjPsm</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>666826780179869698</td>\n",
       "      <td>12/10 simply brilliant pup https://t.co/V6ZzG45zzG</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>666287406224695296</td>\n",
       "      <td>This is an Albanian 3 1/2 legged  Episcopalian. Loves well-polished hardwood flooring. Penis on the collar. 9/10 https://t.co/d9NcXFKwLv</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>666337882303524864</td>\n",
       "      <td>This is an extremely rare horned Parthenon. Not amused. Wears shoes. Overall very nice. 9/10 would pet aggressively https://t.co/QpRjllzWAL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>666345417576210432</td>\n",
       "      <td>Look at this jokester thinking seat belt laws don't apply to him. Great tongue tho 10/10 https://t.co/VFKG1vxGjB</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>666353288456101888</td>\n",
       "      <td>Here we have a mixed Asiago from the Galápagos Islands. Only one ear working. Big fan of marijuana carpet. 8/10 https://t.co/tltQ5w9aUO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>666373753744588802</td>\n",
       "      <td>Those are sunglasses and a jean jacket. 11/10 dog cool af https://t.co/uHXrPkUEyl</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>666396247373291520</td>\n",
       "      <td>Oh goodness. A super rare northeast Qdoba kangaroo mix. Massive feet. No pouch (disappointing). Seems alert. 9/10 https://t.co/Dc7b0E8qFE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>666407126856765440</td>\n",
       "      <td>This is a southern Vesuvius bumblegruff. Can drive a truck (wow). Made friends with 5 other nifty dogs (neat). 7/10 https://t.co/LopTBkKa8h</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>666421158376562688</td>\n",
       "      <td>*internally screaming* 12/10 https://t.co/YMcrXC2Y6R</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>666428276349472768</td>\n",
       "      <td>Here we have an Austrian Pulitzer. Collectors edition. Levitates (?). 7/10 would garden with https://t.co/NMQq6HIglK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>666273097616637952</td>\n",
       "      <td>Can take selfies 11/10 https://t.co/ws2AMaNwPW</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>666430724426358785</td>\n",
       "      <td>Oh boy what a pup! Sunglasses take this one to the next level. Weirdly folds front legs. Pretty big. 6/10 https://t.co/yECbFrSArM</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>666437273139982337</td>\n",
       "      <td>Here we see a lone northeastern Cumberbatch. Half ladybug. Only builds with bricks. Very confident with body. 7/10 https://t.co/7LtjBS0GPK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>666649482315059201</td>\n",
       "      <td>Cool dog. Enjoys couch. Low monotone bark. Very nice kicks. Pisses milk (must be rare). Can't go down stairs. 4/10 https://t.co/vXMKrJC81s</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>666435652385423360</td>\n",
       "      <td>\"Can you behave? You're ruining my wedding day\"\\r\\nDOG: idgaf this flashlight tastes good as hell\\r\\n\\r\\n10/10 https://t.co/GlFZPzqcEU</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>666102155909144576</td>\n",
       "      <td>Oh my. Here you are seeing an Adobe Setter giving birth to twins!!! The world is an amazing place. 11/10 https://t.co/11LvqN4WLq</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>666099513787052032</td>\n",
       "      <td>Can stand on stump for what seems like a while. Built that birdhouse? Impressive. Made friends with a squirrel. 8/10 https://t.co/Ri4nMTLq5C</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>666094000022159362</td>\n",
       "      <td>This appears to be a Mongolian Presbyterian mix. Very tired. Tongue slip confirmed. 9/10 would lie down with https://t.co/mnioXo3IfP</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>666029285002620928</td>\n",
       "      <td>This is a western brown Mitsubishi terrier. Upset about leaf. Actually 2 dogs here. 7/10 would walk the shit out of https://t.co/r7mOb2m0UI</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>666033412701032449</td>\n",
       "      <td>Here is a very happy pup. Big fan of well-maintained decks. Just look at that tongue. 9/10 would cuddle af https://t.co/y671yMhoiR</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>666044226329800704</td>\n",
       "      <td>This is a purebred Piers Morgan. Loves to Netflix and chill. Always looks like he forgot to unplug the iron. 6/10 https://t.co/DWnyCjf2mx</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>694356675654983680</td>\n",
       "      <td>This pupper only appears through the hole of a Funyun. Much like Phineas, this one is also mysterious af. 10/10 https://t.co/SQsEBWxPyG</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>703268521220972544</td>\n",
       "      <td>Happy Friday here's a sleepy pupper 12/10 https://t.co/eBcqv9SPkY</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>755955933503782912</td>\n",
       "      <td>Here's a frustrated pupper attempting to escape a pool of Frosted Flakes. 12/10 https://t.co/GAYViEweWr</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>700864154249383937</td>\n",
       "      <td>\"Pupper is a present to world. Here is a bow for pupper.\" 12/10 precious as hell https://t.co/ItSsE92gCW</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>854120357044912130</td>\n",
       "      <td>Sometimes you guys remind me just how impactful a pupper can be. Cooper will be remembered as a good boy by so many. 14/10 rest easy friend https://t.co/oBL7LEJEzR</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>705786532653883392</td>\n",
       "      <td>Seriously, add us 🐶 11/10 for sad wet pupper https://t.co/xwPE9faVZR</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>673956914389192708</td>\n",
       "      <td>This is one esteemed pupper. Just graduated college. 10/10 what a champ https://t.co/nyReCVRiyd</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>704859558691414016</td>\n",
       "      <td>Here is a heartbreaking scene of an incredible pupper being laid to rest. 10/10 RIP pupper https://t.co/81mvJ0rGRu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>704761120771465216</td>\n",
       "      <td>This pupper killed this great white in an epic sea battle. Now wears it as a trophy. Such brave. Much fierce. 13/10 https://t.co/Lu0ECu5tO5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>672622327801233409</td>\n",
       "      <td>This lil pupper is sad because we haven't found Kony yet. RT to spread awareness. 12/10 would pet firmly https://t.co/Cv7dRdcMvQ</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>751456908746354688</td>\n",
       "      <td>Here's a pupper that's very hungry but too lazy to get up and eat. 12/10 (vid by @RealDavidCortes) https://t.co/lsVAMBq6ex</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>674638615994089473</td>\n",
       "      <td>This pupper is fed up with being tickled. 12/10 I'm currently working on an elaborate heist to steal this dog https://t.co/F33n1hy3LL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>675334060156301312</td>\n",
       "      <td>Good morning here's a grass pupper. 12/10 https://t.co/2d68FmWGGs</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>837820167694528512</td>\n",
       "      <td>Here's a pupper before and after being asked \"who's a good girl?\" Unsure as h*ck. 12/10 hint hint it's you https://t.co/ORiK6jlgdH</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>675740360753160193</td>\n",
       "      <td>Here's a pupper licking in slow motion. 12/10 please enjoy https://t.co/AUJi8ujxw9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>700151421916807169</td>\n",
       "      <td>If a pupper gave that to me I'd probably start shaking and faint from all the joy. 11/10 https://t.co/o9aJVPB25n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>699036661657767936</td>\n",
       "      <td>HAPPY V-DAY FROM YOUR FAV PUPPER SQUAD 13/10 for all https://t.co/7u6VnZ1UFe</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>697616773278015490</td>\n",
       "      <td>This pupper doubles as a hallway rug. Very rare. Versatile af. 11/10 https://t.co/Jxd5pR02Cn</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>697596423848730625</td>\n",
       "      <td>Here's a pupper with a piece of pizza. Two of everybody's favorite things in one photo. 11/10 https://t.co/5USjFjKI7Z</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>699434518667751424</td>\n",
       "      <td>I know this is a tad late but here's a wonderful Valentine's Day pupper 12/10 https://t.co/hTE2PEwGvi</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>700143752053182464</td>\n",
       "      <td>When it's Janet from accounting's birthday but you can't eat the cake cuz it's chocolate. 10/10 hang in there pupper https://t.co/Fbdr5orUrJ</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>675898130735476737</td>\n",
       "      <td>I'm sure you've all seen this pupper. Not prepared at all for the flying disc of terror. 10/10 https://t.co/G0pQiFGM7O</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>674447403907457024</td>\n",
       "      <td>This pupper just wants a belly rub. This pupper has nothing to do w the tree being sideways now. 10/10 good pupper https://t.co/AyJ7Ohk71f</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>695095422348574720</td>\n",
       "      <td>This is just a beautiful pupper good shit evolution. 12/10 https://t.co/2L8pI0Z2Ib</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>756275833623502848</td>\n",
       "      <td>When ur older siblings get to play in the deep end but dad says ur not old enough. Maybe one day puppo. All 10/10 https://t.co/JrDAzMhwG9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>889665388333682689</td>\n",
       "      <td>Here's a puppo that seems to be on the fence about something haha no but seriously someone help her. 13/10 https://t.co/BxvuXk0UCm</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>825535076884762624</td>\n",
       "      <td>Here's a very loving and accepting puppo. Appears to have read her Constitution well. 14/10 would pat head approvingly https://t.co/6ao80wIpV1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>822872901745569793</td>\n",
       "      <td>Here's a super supportive puppo participating in the Toronto  #WomensMarch today. 13/10 https://t.co/nTz3FtorBc</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>752519690950500352</td>\n",
       "      <td>Hopefully this puppo on a swing will help get you through your Monday. 11/10 would push https://t.co/G54yClasz2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>855851453814013952</td>\n",
       "      <td>Here's a puppo participating in the #ScienceMarch. Cleverly disguising her own doggo agenda. 13/10 would keep the planet habitable for https://t.co/cMhq16isel</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  \\\n",
       "11    667793409583771648   \n",
       "13    667832474953625600   \n",
       "22    667801013445750784   \n",
       "24    667491009379606528   \n",
       "27    667044094246576128   \n",
       "32    667138269671505920   \n",
       "38    667176164155375616   \n",
       "41    667192066997374976   \n",
       "46    667435689202614272   \n",
       "57    666826780179869698   \n",
       "61    666287406224695296   \n",
       "62    666337882303524864   \n",
       "63    666345417576210432   \n",
       "64    666353288456101888   \n",
       "65    666373753744588802   \n",
       "66    666396247373291520   \n",
       "67    666407126856765440   \n",
       "69    666421158376562688   \n",
       "70    666428276349472768   \n",
       "71    666273097616637952   \n",
       "72    666430724426358785   \n",
       "73    666437273139982337   \n",
       "77    666649482315059201   \n",
       "83    666435652385423360   \n",
       "84    666102155909144576   \n",
       "85    666099513787052032   \n",
       "86    666094000022159362   \n",
       "88    666029285002620928   \n",
       "89    666033412701032449   \n",
       "90    666044226329800704   \n",
       "...                  ...   \n",
       "1598  694356675654983680   \n",
       "1600  703268521220972544   \n",
       "1603  755955933503782912   \n",
       "1611  700864154249383937   \n",
       "1613  854120357044912130   \n",
       "1618  705786532653883392   \n",
       "1625  673956914389192708   \n",
       "1630  704859558691414016   \n",
       "1631  704761120771465216   \n",
       "1633  672622327801233409   \n",
       "1634  751456908746354688   \n",
       "1637  674638615994089473   \n",
       "1638  675334060156301312   \n",
       "1641  837820167694528512   \n",
       "1643  675740360753160193   \n",
       "1644  700151421916807169   \n",
       "1647  699036661657767936   \n",
       "1651  697616773278015490   \n",
       "1652  697596423848730625   \n",
       "1655  699434518667751424   \n",
       "1656  700143752053182464   \n",
       "1657  675898130735476737   \n",
       "1661  674447403907457024   \n",
       "1662  695095422348574720   \n",
       "1664  756275833623502848   \n",
       "1671  889665388333682689   \n",
       "1676  825535076884762624   \n",
       "1681  822872901745569793   \n",
       "1684  752519690950500352   \n",
       "1685  855851453814013952   \n",
       "\n",
       "                                                                                                                                                                     text  \\\n",
       "11    Dogs only please. Small cows and other non canines will not be tolerated. Sick tattoos tho 8/10 https://t.co/s1z7mX4c9O                                               \n",
       "13    THE EYES 12/10\\r\\n\\r\\nI'm sorry. These are supposed to be funny but your dogs are too adorable https://t.co/z1xPTgVLc7                                                \n",
       "22    OMIGOD 12/10 https://t.co/SVMF4Frf1w                                                                                                                                  \n",
       "24    Two dogs in this one. Both are rare Jujitsu Pythagoreans. One slightly whiter than other. Long legs. 7/10 and 8/10 https://t.co/ITxxcc4v9y                            \n",
       "27    12/10 gimme now https://t.co/QZAnwgnOMB                                                                                                                               \n",
       "32    Extremely intelligent dog here. Has learned to walk like human. Even has his own dog. Very impressive 10/10 https://t.co/0DvHAMdA4V                                   \n",
       "38    These are strange dogs. All have toupees. Long neck for dogs. In a shed of sorts? Work in groups? 4/10 still petable https://t.co/PZxSarAfSN                          \n",
       "41    *takes several long deep breaths* omg omg oMG OMG OMG OMGSJYBSNDUYWJO 12/10 https://t.co/QCugm5ydl6                                                                   \n",
       "46    Ermergerd 12/10 https://t.co/PQni2sjPsm                                                                                                                               \n",
       "57    12/10 simply brilliant pup https://t.co/V6ZzG45zzG                                                                                                                    \n",
       "61    This is an Albanian 3 1/2 legged  Episcopalian. Loves well-polished hardwood flooring. Penis on the collar. 9/10 https://t.co/d9NcXFKwLv                              \n",
       "62    This is an extremely rare horned Parthenon. Not amused. Wears shoes. Overall very nice. 9/10 would pet aggressively https://t.co/QpRjllzWAL                           \n",
       "63    Look at this jokester thinking seat belt laws don't apply to him. Great tongue tho 10/10 https://t.co/VFKG1vxGjB                                                      \n",
       "64    Here we have a mixed Asiago from the Galápagos Islands. Only one ear working. Big fan of marijuana carpet. 8/10 https://t.co/tltQ5w9aUO                               \n",
       "65    Those are sunglasses and a jean jacket. 11/10 dog cool af https://t.co/uHXrPkUEyl                                                                                     \n",
       "66    Oh goodness. A super rare northeast Qdoba kangaroo mix. Massive feet. No pouch (disappointing). Seems alert. 9/10 https://t.co/Dc7b0E8qFE                             \n",
       "67    This is a southern Vesuvius bumblegruff. Can drive a truck (wow). Made friends with 5 other nifty dogs (neat). 7/10 https://t.co/LopTBkKa8h                           \n",
       "69    *internally screaming* 12/10 https://t.co/YMcrXC2Y6R                                                                                                                  \n",
       "70    Here we have an Austrian Pulitzer. Collectors edition. Levitates (?). 7/10 would garden with https://t.co/NMQq6HIglK                                                  \n",
       "71    Can take selfies 11/10 https://t.co/ws2AMaNwPW                                                                                                                        \n",
       "72    Oh boy what a pup! Sunglasses take this one to the next level. Weirdly folds front legs. Pretty big. 6/10 https://t.co/yECbFrSArM                                     \n",
       "73    Here we see a lone northeastern Cumberbatch. Half ladybug. Only builds with bricks. Very confident with body. 7/10 https://t.co/7LtjBS0GPK                            \n",
       "77    Cool dog. Enjoys couch. Low monotone bark. Very nice kicks. Pisses milk (must be rare). Can't go down stairs. 4/10 https://t.co/vXMKrJC81s                            \n",
       "83    \"Can you behave? You're ruining my wedding day\"\\r\\nDOG: idgaf this flashlight tastes good as hell\\r\\n\\r\\n10/10 https://t.co/GlFZPzqcEU                                \n",
       "84    Oh my. Here you are seeing an Adobe Setter giving birth to twins!!! The world is an amazing place. 11/10 https://t.co/11LvqN4WLq                                      \n",
       "85    Can stand on stump for what seems like a while. Built that birdhouse? Impressive. Made friends with a squirrel. 8/10 https://t.co/Ri4nMTLq5C                          \n",
       "86    This appears to be a Mongolian Presbyterian mix. Very tired. Tongue slip confirmed. 9/10 would lie down with https://t.co/mnioXo3IfP                                  \n",
       "88    This is a western brown Mitsubishi terrier. Upset about leaf. Actually 2 dogs here. 7/10 would walk the shit out of https://t.co/r7mOb2m0UI                           \n",
       "89    Here is a very happy pup. Big fan of well-maintained decks. Just look at that tongue. 9/10 would cuddle af https://t.co/y671yMhoiR                                    \n",
       "90    This is a purebred Piers Morgan. Loves to Netflix and chill. Always looks like he forgot to unplug the iron. 6/10 https://t.co/DWnyCjf2mx                             \n",
       "...                                                                                                                                         ...                             \n",
       "1598  This pupper only appears through the hole of a Funyun. Much like Phineas, this one is also mysterious af. 10/10 https://t.co/SQsEBWxPyG                               \n",
       "1600  Happy Friday here's a sleepy pupper 12/10 https://t.co/eBcqv9SPkY                                                                                                     \n",
       "1603  Here's a frustrated pupper attempting to escape a pool of Frosted Flakes. 12/10 https://t.co/GAYViEweWr                                                               \n",
       "1611  \"Pupper is a present to world. Here is a bow for pupper.\" 12/10 precious as hell https://t.co/ItSsE92gCW                                                              \n",
       "1613  Sometimes you guys remind me just how impactful a pupper can be. Cooper will be remembered as a good boy by so many. 14/10 rest easy friend https://t.co/oBL7LEJEzR   \n",
       "1618  Seriously, add us 🐶 11/10 for sad wet pupper https://t.co/xwPE9faVZR                                                                                                  \n",
       "1625  This is one esteemed pupper. Just graduated college. 10/10 what a champ https://t.co/nyReCVRiyd                                                                       \n",
       "1630  Here is a heartbreaking scene of an incredible pupper being laid to rest. 10/10 RIP pupper https://t.co/81mvJ0rGRu                                                    \n",
       "1631  This pupper killed this great white in an epic sea battle. Now wears it as a trophy. Such brave. Much fierce. 13/10 https://t.co/Lu0ECu5tO5                           \n",
       "1633  This lil pupper is sad because we haven't found Kony yet. RT to spread awareness. 12/10 would pet firmly https://t.co/Cv7dRdcMvQ                                      \n",
       "1634  Here's a pupper that's very hungry but too lazy to get up and eat. 12/10 (vid by @RealDavidCortes) https://t.co/lsVAMBq6ex                                            \n",
       "1637  This pupper is fed up with being tickled. 12/10 I'm currently working on an elaborate heist to steal this dog https://t.co/F33n1hy3LL                                 \n",
       "1638  Good morning here's a grass pupper. 12/10 https://t.co/2d68FmWGGs                                                                                                     \n",
       "1641  Here's a pupper before and after being asked \"who's a good girl?\" Unsure as h*ck. 12/10 hint hint it's you https://t.co/ORiK6jlgdH                                    \n",
       "1643  Here's a pupper licking in slow motion. 12/10 please enjoy https://t.co/AUJi8ujxw9                                                                                    \n",
       "1644  If a pupper gave that to me I'd probably start shaking and faint from all the joy. 11/10 https://t.co/o9aJVPB25n                                                      \n",
       "1647  HAPPY V-DAY FROM YOUR FAV PUPPER SQUAD 13/10 for all https://t.co/7u6VnZ1UFe                                                                                          \n",
       "1651  This pupper doubles as a hallway rug. Very rare. Versatile af. 11/10 https://t.co/Jxd5pR02Cn                                                                          \n",
       "1652  Here's a pupper with a piece of pizza. Two of everybody's favorite things in one photo. 11/10 https://t.co/5USjFjKI7Z                                                 \n",
       "1655  I know this is a tad late but here's a wonderful Valentine's Day pupper 12/10 https://t.co/hTE2PEwGvi                                                                 \n",
       "1656  When it's Janet from accounting's birthday but you can't eat the cake cuz it's chocolate. 10/10 hang in there pupper https://t.co/Fbdr5orUrJ                          \n",
       "1657  I'm sure you've all seen this pupper. Not prepared at all for the flying disc of terror. 10/10 https://t.co/G0pQiFGM7O                                                \n",
       "1661  This pupper just wants a belly rub. This pupper has nothing to do w the tree being sideways now. 10/10 good pupper https://t.co/AyJ7Ohk71f                            \n",
       "1662  This is just a beautiful pupper good shit evolution. 12/10 https://t.co/2L8pI0Z2Ib                                                                                    \n",
       "1664  When ur older siblings get to play in the deep end but dad says ur not old enough. Maybe one day puppo. All 10/10 https://t.co/JrDAzMhwG9                             \n",
       "1671  Here's a puppo that seems to be on the fence about something haha no but seriously someone help her. 13/10 https://t.co/BxvuXk0UCm                                    \n",
       "1676  Here's a very loving and accepting puppo. Appears to have read her Constitution well. 14/10 would pat head approvingly https://t.co/6ao80wIpV1                        \n",
       "1681  Here's a super supportive puppo participating in the Toronto  #WomensMarch today. 13/10 https://t.co/nTz3FtorBc                                                       \n",
       "1684  Hopefully this puppo on a swing will help get you through your Monday. 11/10 would push https://t.co/G54yClasz2                                                       \n",
       "1685  Here's a puppo participating in the #ScienceMarch. Cleverly disguising her own doggo agenda. 13/10 would keep the planet habitable for https://t.co/cMhq16isel        \n",
       "\n",
       "     dog_names  \n",
       "11    None      \n",
       "13    None      \n",
       "22    None      \n",
       "24    None      \n",
       "27    None      \n",
       "32    None      \n",
       "38    None      \n",
       "41    None      \n",
       "46    None      \n",
       "57    None      \n",
       "61    None      \n",
       "62    None      \n",
       "63    None      \n",
       "64    None      \n",
       "65    None      \n",
       "66    None      \n",
       "67    None      \n",
       "69    None      \n",
       "70    None      \n",
       "71    None      \n",
       "72    None      \n",
       "73    None      \n",
       "77    None      \n",
       "83    None      \n",
       "84    None      \n",
       "85    None      \n",
       "86    None      \n",
       "88    None      \n",
       "89    None      \n",
       "90    None      \n",
       "...    ...      \n",
       "1598  None      \n",
       "1600  None      \n",
       "1603  None      \n",
       "1611  None      \n",
       "1613  None      \n",
       "1618  None      \n",
       "1625  None      \n",
       "1630  None      \n",
       "1631  None      \n",
       "1633  None      \n",
       "1634  None      \n",
       "1637  None      \n",
       "1638  None      \n",
       "1641  None      \n",
       "1643  None      \n",
       "1644  None      \n",
       "1647  None      \n",
       "1651  None      \n",
       "1652  None      \n",
       "1655  None      \n",
       "1656  None      \n",
       "1657  None      \n",
       "1661  None      \n",
       "1662  None      \n",
       "1664  None      \n",
       "1671  None      \n",
       "1676  None      \n",
       "1681  None      \n",
       "1684  None      \n",
       "1685  None      \n",
       "\n",
       "[458 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting a dog_names that are none.\n",
    "master_copy[master_copy['dog_names'] == 'None'][['tweet_id', 'text', 'dog_names']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
