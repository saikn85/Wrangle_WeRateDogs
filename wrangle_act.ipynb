{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "## Dataset - WeRateDogs&trade; Twitter Archive\n",
    "\n",
    "***By: Kartik Nanduri***<br>\n",
    "**Date: 21st Nov, 2018.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests as req"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***<span style=\"color: red\">Important! uncomment the following files to run the book with out errors.</span>***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the folder structure.\n",
    "#os.rename('dataset/twitter-archive-enhanced.csv', 'twitter-archive-enhanced.csv')\n",
    "#import shutil\n",
    "#shutil.rmtree('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***<span style=\"color: green\">Important! once done, please recomment.</span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [x] **The file given at hand `twitter-archive-enhanced.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the requried files for this project are in the list files_list\n",
    "files_list = ['twitter-archive-enhanced.csv', 'image-predictions.tsv', 'tweet_json.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the twitter archive file\n",
    "archive = pd.read_csv(files_list[0])\n",
    "\n",
    "# taking at random entries for the archive file\n",
    "archive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [x] **Fetching the data from url and saving it to local drive - `image-predictions.tsv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file from internet using the requests library\n",
    "url = \"https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5bf60c69_image-predictions-3/image-predictions-3.tsv\"\n",
    "res = req.get(url)\n",
    "\n",
    "with open(files_list[1], mode = \"wb\") as op_file:\n",
    "    op_file.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if fetched the data right way\n",
    "img_pre_test = pd.read_csv(files_list[1], delimiter = \"\\t\", encoding = 'utf-8')\n",
    "img_pre_test.sample(2)\n",
    "\n",
    "# we did it the right way, Yay! it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [x] **Getting data from Twitter&trade;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries for accessing Twitter via API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up all the necessary placeholders for API\n",
    "consumer_key = 'xxx.xxx.xxx.xxx'\n",
    "consumer_secret = 'xxx.xxx.xxx.xxx'\n",
    "access_token = 'xxx.xxx.xxx.xxx'\n",
    "access_secret = 'xxx.xxx.xxx.xxx'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth_handler = auth,\n",
    "                 parser = tweepy.parsers.JSONParser(),\n",
    "                 wait_on_rate_limit = True, \n",
    "                 wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save(ids, api_ins, one_id = None):\n",
    "    '''\n",
    "    This function will fetch data with associated id in ids list\n",
    "    ids (List Object): a list all tweets\n",
    "    api_ins (Tweepy Object): api object instance, will be used to query twitter for data\n",
    "    one_id (int): use when you want to query only for one tweet\n",
    "    failed_ids (List Object): a list will be retured so that, this fuction can be called once again on those ids\n",
    "    '''\n",
    "    new_file_name = ''; failed_ids = []; tweet_df = []\n",
    "    \n",
    "    # checking if file exists\n",
    "    if os.path.exists(files_list[2]):\n",
    "        temp = [s for s in os.listdir() if \"tweet_json\" in s]\n",
    "        new_file_name = files_list[2].split('.')[0] + \"_\" + str(len(temp)) + \".txt\"\n",
    "    else:\n",
    "        new_file_name = files_list[2]\n",
    "    \n",
    "    # querying a list of ids\n",
    "    if one_id == None:\n",
    "        with open(new_file_name, mode = 'w') as outfile:\n",
    "            for one_id in ids:\n",
    "                try:\n",
    "                    content = api_ins.get_status(one_id, tweet_mode='extended')\n",
    "                    json.dump(content, outfile)\n",
    "                    outfile.write('\\n')\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "                    failed_ids.append(one_id)\n",
    "    \n",
    "    # querying a single id\n",
    "    else:\n",
    "        try:\n",
    "            content = api_ins.get_status(one_id, include_entities = True)\n",
    "            favorites = content['favorite_count']\n",
    "            retweets = content['retweet_count']\n",
    "            \n",
    "            tweet_df.append({'tweet_id': int(one_id),\n",
    "                        'favorites': int(favorites),\n",
    "                        'retweets': int(retweets)})\n",
    "            \n",
    "            return tweet_df\n",
    "                           \n",
    "        except Exception as e:\n",
    "            print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "            failed_ids.append(one_id)\n",
    "\n",
    "    return failed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the list of ids to the fuction fetch_and_save(), but in batches\n",
    "# given that we can request 900 request/15min - window, let's break our ids into\n",
    "tweet_ids = archive['tweet_id'].tolist()\n",
    "\n",
    "# set_one, two and three\n",
    "set_one = tweet_ids[0:900]; set_two = tweet_ids[900:1800]; set_three = tweet_ids[1800:]\n",
    "\n",
    "# checking the lengths so that we send 900 ids/requests.\n",
    "print(len(set_one), len(set_two), len(set_three), len(set_one)+len(set_two)+len(set_three))\n",
    "print(len(set_one)+len(set_two)+len(set_three) == len(tweet_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# fetching data 1st iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_one = fetch_and_save(set_one, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_one)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleeping for 6 mins, so that Rate Limit time is reduced\n",
    "print(\"Sleeping for 6 mins.\")\n",
    "time.sleep(360)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "# fetching data 2nd iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_two = fetch_and_save(set_two, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_two)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleeping for 6 mins, so that Rate Limit time is reduced\n",
    "print(\"Sleeping for 6 mins.\")\n",
    "time.sleep(360)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "# fetching data 3rd iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_three = fetch_and_save(set_three, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_three)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the failed ids into one master list\n",
    "failed_ids = test_one + test_two + test_three\n",
    "print(\"Total failed request are: {}. \\n\".format(len(failed_ids)))\n",
    "\n",
    "# ids that failed and the ones that passed\n",
    "indi_fail = []; success = []\n",
    "\n",
    "#for each failed id, lets try to fetch status individually.\n",
    "for failed_id in failed_ids:\n",
    "    temp = fetch_and_save(ids = None, api_ins = api, one_id = failed_id)\n",
    "    indi_fail.append(temp[0])\n",
    "\n",
    "# removing empty elements from list\n",
    "success = [x for x in indi_fail if not isinstance(x, (int))]\n",
    "indi_fail = [x for x in indi_fail if isinstance(x, (int))]\n",
    "\n",
    "# checking if there is change\n",
    "print(\"\\nWe were able to retrieve {} records, others failed.\".format(len(failed_ids) - len(indi_fail)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [x] **Okay, let's combine the successful jsons into one file, called the `tweet_master.txt`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combing all successful jsons into one master file\n",
    "json_1 = pd.read_json('tweet_json.txt', lines = True, encoding = 'utf-8')\n",
    "json_2 = pd.read_json('tweet_json_1.txt', lines = True, encoding = 'utf-8')\n",
    "json_3 = pd.read_json('tweet_json_2.txt', lines = True, encoding = 'utf-8')\n",
    "\n",
    "# total rows that we need to have in our resulting dataframe\n",
    "print(json_1.shape[0] + json_2.shape[0] + json_3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_master = pd.concat([json_1, json_2, json_3], ignore_index = True, join = 'outer', sort = True)\n",
    "json_master.to_json('tweet_master.txt', orient = 'records', lines = True)\n",
    "json_master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing objects that are not required.\n",
    "del archive, img_pre_test\n",
    "del json_1, json_2, json_3, json_master\n",
    "del indi_fail, end, start, test_one, test_two, test_three, set_one, set_two, set_three\n",
    "del consumer_key, consumer_secret, access_token, access_secret, auth, api\n",
    "\n",
    "# we are not removing success and files_list, making sure we stick to good programming practices - reusablity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [x] **Last thing to do is to tidy up our folder, let's get going.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'dataset',\n",
       " 'error.png',\n",
       " 'New Text Document.txt',\n",
       " 'README.md',\n",
       " 'twitter-archive-enhanced.xlsx',\n",
       " 'wrangle_act.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moving all data files under one folder - dataset\n",
    "# removing the temporary files, that acted as placeholders\n",
    "\n",
    "# creating the folder\n",
    "folder = 'dataset'\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "# we know that our master datasets for this project are\n",
    "# 1. twitter-archive-enhanced.csv\n",
    "# 2. image-predictions.tsv\n",
    "# 3. tweet_json_master.txt\n",
    "# let us move these files\n",
    "\n",
    "# updating our files_list\n",
    "files_list[-1] = 'tweet_master.txt'\n",
    "\n",
    "# moving only required files\n",
    "for file in files_list:\n",
    "    if os.path.exists(file):\n",
    "        os.rename(file, folder+'/'+file)\n",
    "\n",
    "# removing the tweet_json and tweet_json_1 files as they are not required anymore\n",
    "for file in [s for s in os.listdir() if \"tweet_json\" in s]:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    \n",
    "# lisitng the current directory\n",
    "os.listdir()\n",
    "\n",
    "# clean and neat, lets get with assessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming files_list\n",
    "for i in range(3):\n",
    "    files_list[i] = folder + '/'+ files_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [x] **Last thing to do is to extract `retweet_count` and `favourite_count` from `tweet_master.txt`, saving the result as .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending success to the master dataset.\n",
    "tweet_json = pd.read_json(files_list[2], lines = True, encoding = 'utf-8')\n",
    "tweet_json = tweet_json[['id', 'retweet_count','favorite_count']]\n",
    "tweet_json.rename(index = str,\n",
    "                  columns={'id' : 'tweet_id', 'retweet_count': 'retweets','favorite_count': 'favorites'},\n",
    "                  inplace = True)\n",
    "tweet_json = pd.concat([tweet_json, pd.DataFrame.from_dict(success)],\n",
    "                       ignore_index = True, sort = True)\n",
    "tweet_json.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe into master file\n",
    "tweet_json.to_json(files_list[2], orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Gathering\n",
    "\n",
    "- We know, that gathering is a the first step in wrangling.\n",
    "- We were successful in gathering from three different sources with different techniques:\n",
    "    - Data given at hand.\n",
    "    - Fetch from flat file stored on a server.\n",
    "    - From API.\n",
    "\n",
    "- There a total of 14 missing data points, tried a different ways for retrieving them, using the API as well as `twurl` of the `Ruby` package, but they were not to be found, as stated below in the highlighted section.\n",
    "\n",
    "***<span style=\"color: ##6c6cff\">So let's start with assessing the data.</span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![error](error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load up dataset, and starting assessing them.\n",
    "archive =  pd.read_csv(files_list[0], encoding = 'utf-8')\n",
    "img_pre = pd.read_csv(files_list[1], sep = '\\t', encoding = 'utf-8')\n",
    "retweets_fav = pd.read_json(files_list[2], lines = True, encoding =  'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues to sort!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing out archive - visual assessment\n",
    "archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic Assessment 1 - Information\n",
    "archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmtic Assessment 2 - Describe\n",
    "archive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates - tweet_ids\n",
    "sum(archive.tweet_id.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkin if we have more than one class of dogs assigned to dog\n",
    "# the following are the only combinations that are present in the dataset\n",
    "cond_1 = (archive['doggo'] == 'doggo') & (archive['floofer'] == 'floofer')\n",
    "cond_2 = (archive['doggo'] == 'doggo') & (archive['pupper'] == 'pupper')\n",
    "cond_3 = (archive['doggo'] == 'doggo') & (archive['puppo'] == 'puppo')\n",
    "\n",
    "# printing these entries\n",
    "archive[cond_1 | cond_2 | cond_3][['tweet_id', 'text', 'doggo', 'floofer', 'pupper', 'puppo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(archive[cond_1 | cond_2 | cond_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`twitter-archive-enhanced.csv`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- `rating_numerator` : has values such as 1, 3.. e.t.c - **Data Quality Dimension - `Consistency`**.\n",
    "- `rating_denominator` : have values, less than 10, for example, the tweet_id - 666287406224695296 has the number 2 as its value - **Data Quality Dimension - `Consistency`**. \n",
    "- We see that, Articles - `a`, `an`, `the` have been used to name dogs, as well as words such as `such`, `quite` - **Data Quality Dimension - `Validity`**.\n",
    "- There are instances where the names of dogs are in lowercases - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- `rating_numerator` : has a maximum value of 1766 - **Data Quality Dimension - `Consistency`**. \n",
    "- `rating_denominator` : has a maximum value of 170 - **Data Quality Dimension - `Consistency`**.\n",
    "- All in all, this dataset appears to be clean, except for `expanded_url` - we have about 59 instances missing - **Data Quality Dimension - `Completeness`**.\n",
    "- We can see that there are more than one class assigned to tweets, analyze and assign proper dog class so that melting is easy - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- we can see that, there are four classes of dogs `doggo`, `floofer`, `puppo`, `pupper`; these should a part of one unit - `dog_class` - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- `in_reply_to_status_id`, `retweeted_status_id`, `retweeted_status_user_id`, `in_reply_to_user_id` of type float64 must be converted into int - **Data Quality Dimension - `Validity`**.\n",
    "- `timestamp`, `retweeted_status_timestamp` of type object must be converted into datatime - **Data Quality Dimension - `Validity`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing img_predictions dataset\n",
    "img_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic Assessment - Information\n",
    "img_pre.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates\n",
    "img_pre[img_pre['jpg_url'].duplicated(keep = False)].sort_values(by = 'jpg_url')[['tweet_id', 'jpg_url']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **`image-predictions.tsv`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- We have few dog breeds that are represented in lowercase.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- We have about 281 images on a whole, that are missing with respect to our `twitter-archive-enhanced.csv` file - **Data Quality Dimension - `Completeness`**.\n",
    "- We can see that, we have about `66` duplicates **OR** a pair of tweets are pointing to same *`jpg_url`* - **Data Quality Dimension - `Accuracy`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- None. \n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing tweet_master dataset\n",
    "retweets_fav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_fav.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **`tweet_master.txt`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- None.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- We have about 14 missing records - **Data Quality Dimension - `Completeness`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- None.\n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Assessing\n",
    "\n",
    "- Completed the second step.\n",
    "- The following are the insights:\n",
    "    - from `twitter-archive-enhanced.csv` datset, the rating_numerator and denominator need to be fixed.\n",
    "    - the dataset also represents row values as columns, which needs to be fixed.\n",
    "    - the dataset also has structural issues such as wrong datatype assigned to a column.\n",
    "    - from `images-preductions.tsv` dataset, there is consistency issue with naming dog breeds.\n",
    "    - the dataset isn't complete when compared to `twitter-archive-enhanced.csv`, we have about 281 missing tweets.\n",
    "    - Also we have `jpg_urls'` that are pointing to a pair of tweets.\n",
    "    - `tweet_master.txt` dataset has about 14 missing records.\n",
    "    - the dataset alone hold the information about retweets and favourites - bad form of schema normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2356 entries, 0 to 2355\n",
      "Data columns (total 30 columns):\n",
      "tweet_id                      2356 non-null int64\n",
      "in_reply_to_status_id         78 non-null float64\n",
      "in_reply_to_user_id           78 non-null float64\n",
      "timestamp                     2356 non-null object\n",
      "source                        2356 non-null object\n",
      "text                          2356 non-null object\n",
      "retweeted_status_id           181 non-null float64\n",
      "retweeted_status_user_id      181 non-null float64\n",
      "retweeted_status_timestamp    181 non-null object\n",
      "expanded_urls                 2297 non-null object\n",
      "rating_numerator              2356 non-null int64\n",
      "rating_denominator            2356 non-null int64\n",
      "name                          2356 non-null object\n",
      "doggo                         2356 non-null object\n",
      "floofer                       2356 non-null object\n",
      "pupper                        2356 non-null object\n",
      "puppo                         2356 non-null object\n",
      "jpg_url                       2075 non-null object\n",
      "img_num                       2075 non-null float64\n",
      "p1                            2075 non-null object\n",
      "p1_conf                       2075 non-null float64\n",
      "p1_dog                        2075 non-null object\n",
      "p2                            2075 non-null object\n",
      "p2_conf                       2075 non-null float64\n",
      "p2_dog                        2075 non-null object\n",
      "p3                            2075 non-null object\n",
      "p3_conf                       2075 non-null float64\n",
      "p3_dog                        2075 non-null object\n",
      "favorites                     2342 non-null float64\n",
      "retweets                      2342 non-null float64\n",
      "dtypes: float64(10), int64(3), object(17)\n",
      "memory usage: 570.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# the master dataset\n",
    "master_set = archive.merge(img_pre, how = 'left', on = ['tweet_id'])\n",
    "master_set = master_set.merge(retweets_fav, how = 'left', on = ['tweet_id'])\n",
    "master_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file to local disk.\n",
    "files_list.append('dataset/master_set_raw.csv')\n",
    "master_set.to_csv(files_list[3], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the master set\n",
    "master_copy = master_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues to Clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Basic cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Assign proper class for the above 14 tweets before melting.\n",
    "- Delete *retweets* with *any duplicates* and get rid of *tweets with **no** images*.\n",
    "- Once done, drop the following columns:\n",
    "    1. `retweeted_status_id`\n",
    "    2. `retweeted_status_user_id`\n",
    "    3. `retweeted_status_timestamp`\n",
    "    4. `in_reply_to_status_id`\n",
    "    5. `in_reply_to_user_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tweet_id  \\\n",
      "191   855851453814013952   \n",
      "200   854010172552949760   \n",
      "460   817777686764523521   \n",
      "531   808106460588765185   \n",
      "565   802265048156610565   \n",
      "575   801115127852503040   \n",
      "705   785639753186217984   \n",
      "733   781308096455073793   \n",
      "778   775898661951791106   \n",
      "822   770093767776997377   \n",
      "889   759793422261743616   \n",
      "956   751583847268179968   \n",
      "1063  741067306818797568   \n",
      "1113  733109485275860992   \n",
      "\n",
      "                                                                                                                                                                      text  \n",
      "191   Here's a puppo participating in the #ScienceMarch. Cleverly disguising her own doggo agenda. 13/10 would keep the planet habitable for https://t.co/cMhq16isel        \n",
      "200   At first I thought this was a shy doggo, but it's actually a Rare Canadian Floofer Owl. Amateurs would confuse the two. 11/10 only send dogs https://t.co/TXdT3tmuYk  \n",
      "460   This is Dido. She's playing the lead role in \"Pupper Stops to Catch Snow Before Resuming Shadow Box with Dried Apple.\" 13/10 (IG: didodoggo) https://t.co/m7isZrOBX7  \n",
      "531   Here we have Burke (pupper) and Dexter (doggo). Pupper wants to be exactly like doggo. Both 12/10 would pet at same time https://t.co/ANBpEYHaho                      \n",
      "565   Like doggo, like pupper version 2. Both 11/10 https://t.co/9IxWAXFqze                                                                                                 \n",
      "575   This is Bones. He's being haunted by another doggo of roughly the same size. 12/10 deep breaths pupper everything's fine https://t.co/55Dqe0SJNj                      \n",
      "705   This is Pinot. He's a sophisticated doggo. You can tell by the hat. Also pointier than your average pupper. Still 10/10 would pet cautiously https://t.co/f2wmLZTPHd  \n",
      "733   Pupper butt 1, Doggo 0. Both 12/10 https://t.co/WQvcPEpH2u                                                                                                            \n",
      "778   RT @dog_rates: Like father (doggo), like son (pupper). Both 12/10 https://t.co/pG2inLaOda                                                                             \n",
      "822   RT @dog_rates: This is just downright precious af. 12/10 for both pupper and doggo https://t.co/o5J479bZUC                                                            \n",
      "889   Meet Maggie &amp; Lila. Maggie is the doggo, Lila is the pupper. They are sisters. Both 12/10 would pet at the same time https://t.co/MYwR4DQKll                      \n",
      "956   Please stop sending it pictures that don't even have a doggo or pupper in them. Churlish af. 5/10 neat couch tho https://t.co/u2c9c7qSg8                              \n",
      "1063  This is just downright precious af. 12/10 for both pupper and doggo https://t.co/o5J479bZUC                                                                           \n",
      "1113  Like father (doggo), like son (pupper). Both 12/10 https://t.co/pG2inLaOda                                                                                            \n"
     ]
    }
   ],
   "source": [
    "# setting column width to -1\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "cond_1 = (master_copy['doggo'] == 'doggo') & (master_copy['floofer'] == 'floofer')\n",
    "cond_2 = (master_copy['doggo'] == 'doggo') & (master_copy['pupper'] == 'pupper')\n",
    "cond_3 = (master_copy['doggo'] == 'doggo') & (master_copy['puppo'] == 'puppo')\n",
    "print(master_copy[cond_1 | cond_2 | cond_3][['tweet_id', 'text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Assign the following:***\n",
    "1. 855851453814013952: puppo\n",
    "2. 854010172552949760: floofer\n",
    "3. 817777686764523521: pupper\n",
    "4. 808106460588765185: pupper\n",
    "5. 802265048156610565: pupper\n",
    "6. 801115127852503040: pupper\n",
    "7. 785639753186217984: pupper\n",
    "8. 781308096455073793: pupper\n",
    "9. 775898661951791106: pupper\n",
    "10. 770093767776997377: pupper\n",
    "11. 759793422261743616: pupper\n",
    "12. 751583847268179968: doggo\n",
    "13. 741067306818797568: doggo\n",
    "14. 733109485275860992: doggo\n",
    "\n",
    "**<span style=\"color: green\">I like puppies, so for most of the entries it is pupper!</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning values.\n",
    "master_copy.loc[master_copy['tweet_id'] == 855851453814013952, ['doggo', 'floofer', 'pupper']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 854010172552949760, ['doggo', 'pupper', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 817777686764523521, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 808106460588765185, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 802265048156610565, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 801115127852503040, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 785639753186217984, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 781308096455073793, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 775898661951791106, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 770093767776997377, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 759793422261743616, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 751583847268179968, ['pupper', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 741067306818797568, ['pupper', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 733109485275860992, ['pupper', 'floofer', 'puppo']] = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>855851453814013952</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>puppo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>854010172552949760</td>\n",
       "      <td>None</td>\n",
       "      <td>floofer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>817777686764523521</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>808106460588765185</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>802265048156610565</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>801115127852503040</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>785639753186217984</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>781308096455073793</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>775898661951791106</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>770093767776997377</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>759793422261743616</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>751583847268179968</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>741067306818797568</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>733109485275860992</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  doggo  floofer  pupper  puppo\n",
       "191   855851453814013952   None     None    None  puppo\n",
       "200   854010172552949760   None  floofer    None   None\n",
       "460   817777686764523521   None     None  pupper   None\n",
       "531   808106460588765185   None     None  pupper   None\n",
       "565   802265048156610565   None     None  pupper   None\n",
       "575   801115127852503040   None     None  pupper   None\n",
       "705   785639753186217984   None     None  pupper   None\n",
       "733   781308096455073793   None     None  pupper   None\n",
       "778   775898661951791106   None     None  pupper   None\n",
       "822   770093767776997377   None     None  pupper   None\n",
       "889   759793422261743616   None     None  pupper   None\n",
       "956   751583847268179968  doggo     None    None   None\n",
       "1063  741067306818797568  doggo     None    None   None\n",
       "1113  733109485275860992  doggo     None    None   None"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all values have been properly assigned\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "master_copy[cond_1 | cond_2 | cond_3][['tweet_id', 'doggo', 'floofer', 'pupper', 'puppo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting those, tweets that have no retweets\n",
    "master_copy = master_copy[pd.isnull(master_copy['retweeted_status_id'])]\n",
    "\n",
    "# deleting duplicates if any\n",
    "master_copy = master_copy.drop_duplicates()\n",
    "\n",
    "# deleting those tweets with no images.\n",
    "master_copy = master_copy.dropna(subset = ['jpg_url'])\n",
    "\n",
    "# reseting index\n",
    "master_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# droping columns\n",
    "master_copy = master_copy.drop(labels = ['retweeted_status_id',\n",
    "                                         'retweeted_status_user_id',\n",
    "                                         'retweeted_status_timestamp',\n",
    "                                         'in_reply_to_status_id', \n",
    "                                         'in_reply_to_user_id'],\n",
    "                               axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1994, 25)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after droping the columns, we should have about 25 dimensions/columns\n",
    "master_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Condense wide-format to long-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Condense `doggo`, `floofer`, `pupper`, `puppo` as `dog_class`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Doggo: 66\n",
      "Count of Floofer: 8\n",
      "Count of Pupper: 209\n",
      "Count of Puppo: 23\n"
     ]
    }
   ],
   "source": [
    "# to make sure that we have \n",
    "doggo = master_copy.doggo.value_counts()['doggo']\n",
    "floofer = master_copy.floofer.value_counts()['floofer']\n",
    "pupper = master_copy.pupper.value_counts()['pupper']\n",
    "puppo = master_copy.puppo.value_counts()['puppo']\n",
    "\n",
    "# printing count of each class\n",
    "print(\"Count of Doggo: {}\\nCount of Floofer: {}\\nCount of Pupper: {}\\nCount of Puppo: {}\".format(doggo,\n",
    "                                                                                                 floofer,\n",
    "                                                                                                 pupper,\n",
    "                                                                                                 puppo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the columns that are to be melted\n",
    "columns_to_melt = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "columns_to_stay = [x for x in master_copy.columns.tolist() if x not in columns_to_melt]\n",
    "\n",
    "# melting the the columns into values\n",
    "master_copy = pd.melt(master_copy, id_vars = columns_to_stay, value_vars = columns_to_melt, \n",
    "                         var_name = 'stages', value_name = 'dog_class')\n",
    "\n",
    "# Delete column 'stages'\n",
    "master_copy = master_copy.drop('stages', 1)\n",
    "\n",
    "# dropping duplicates\n",
    "master_copy = master_copy.sort_values('dog_class').drop_duplicates('tweet_id', keep = 'last')\n",
    "master_copy.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assert\n",
    "assert doggo == master_copy.dog_class.value_counts()['doggo'], \"Some entries are missing\"\n",
    "assert floofer == master_copy.dog_class.value_counts()['floofer'], \"Some entries are missing\"\n",
    "assert pupper == master_copy.dog_class.value_counts()['pupper'], \"Some entries are missing\"\n",
    "assert puppo == master_copy.dog_class.value_counts()['puppo'], \"Some entries are missing\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
