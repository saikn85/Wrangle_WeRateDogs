{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "## Dataset - WeRateDogs&trade; Twitter Archive\n",
    "\n",
    "***By: Kartik Nanduri***<br>\n",
    "**Date: 21st Nov, 2018.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests as req"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***<span style=\"color: red\">Important! uncomment the following files to run the book with out errors.</span>***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the folder structure.\n",
    "#os.rename('dataset/twitter-archive-enhanced.csv', 'twitter-archive-enhanced.csv')\n",
    "#import shutil\n",
    "#shutil.rmtree('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***<span style=\"color: green\">Important! once done, please recomment.</span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [x] **The file given at hand `twitter-archive-enhanced.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the requried files for this project are in the list files_list\n",
    "files_list = ['twitter-archive-enhanced.csv', 'image-predictions.tsv', 'tweet_json.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the twitter archive file\n",
    "archive = pd.read_csv(files_list[0])\n",
    "\n",
    "# taking at random entries for the archive file\n",
    "archive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [x] **Fetching the data from url and saving it to local drive - `image-predictions.tsv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file from internet using the requests library\n",
    "url = \"https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5bf60c69_image-predictions-3/image-predictions-3.tsv\"\n",
    "res = req.get(url)\n",
    "\n",
    "with open(files_list[1], mode = \"wb\") as op_file:\n",
    "    op_file.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if fetched the data right way\n",
    "img_pre_test = pd.read_csv(files_list[1], delimiter = \"\\t\", encoding = 'utf-8')\n",
    "img_pre_test.sample(2)\n",
    "\n",
    "# we did it the right way, Yay! it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [x] **Getting data from Twitter&trade;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries for accessing Twitter via API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up all the necessary placeholders for API\n",
    "consumer_key = 'xxx.xxx.xxx.xxx'\n",
    "consumer_secret = 'xxx.xxx.xxx.xxx'\n",
    "access_token = 'xxx.xxx.xxx.xxx'\n",
    "access_secret = 'xxx.xxx.xxx.xxx'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth_handler = auth,\n",
    "                 parser = tweepy.parsers.JSONParser(),\n",
    "                 wait_on_rate_limit = True, \n",
    "                 wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save(ids, api_ins, one_id = None):\n",
    "    '''\n",
    "    This function will fetch data with associated id in ids list\n",
    "    ids (List Object): a list all tweets\n",
    "    api_ins (Tweepy Object): api object instance, will be used to query twitter for data\n",
    "    one_id (int): use when you want to query only for one tweet\n",
    "    failed_ids (List Object): a list will be retured so that, this fuction can be called once again on those ids\n",
    "    '''\n",
    "    new_file_name = ''; failed_ids = []; tweet_df = []\n",
    "    \n",
    "    # checking if file exists\n",
    "    if os.path.exists(files_list[2]):\n",
    "        temp = [s for s in os.listdir() if \"tweet_json\" in s]\n",
    "        new_file_name = files_list[2].split('.')[0] + \"_\" + str(len(temp)) + \".txt\"\n",
    "    else:\n",
    "        new_file_name = files_list[2]\n",
    "    \n",
    "    # querying a list of ids\n",
    "    if one_id == None:\n",
    "        with open(new_file_name, mode = 'w') as outfile:\n",
    "            for one_id in ids:\n",
    "                try:\n",
    "                    content = api_ins.get_status(one_id, tweet_mode='extended')\n",
    "                    json.dump(content, outfile)\n",
    "                    outfile.write('\\n')\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "                    failed_ids.append(one_id)\n",
    "    \n",
    "    # querying a single id\n",
    "    else:\n",
    "        try:\n",
    "            content = api_ins.get_status(one_id, include_entities = True)\n",
    "            favorites = content['favorite_count']\n",
    "            retweets = content['retweet_count']\n",
    "            \n",
    "            tweet_df.append({'tweet_id': int(one_id),\n",
    "                        'favorites': int(favorites),\n",
    "                        'retweets': int(retweets)})\n",
    "            \n",
    "            return tweet_df\n",
    "                           \n",
    "        except Exception as e:\n",
    "            print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "            failed_ids.append(one_id)\n",
    "\n",
    "    return failed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the list of ids to the fuction fetch_and_save(), but in batches\n",
    "# given that we can request 900 request/15min - window, let's break our ids into\n",
    "tweet_ids = archive['tweet_id'].tolist()\n",
    "\n",
    "# set_one, two and three\n",
    "set_one = tweet_ids[0:900]; set_two = tweet_ids[900:1800]; set_three = tweet_ids[1800:]\n",
    "\n",
    "# checking the lengths so that we send 900 ids/requests.\n",
    "print(len(set_one), len(set_two), len(set_three), len(set_one)+len(set_two)+len(set_three))\n",
    "print(len(set_one)+len(set_two)+len(set_three) == len(tweet_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# fetching data 1st iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_one = fetch_and_save(set_one, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_one)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleeping for 6 mins, so that Rate Limit time is reduced\n",
    "print(\"Sleeping for 6 mins.\")\n",
    "time.sleep(360)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "# fetching data 2nd iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_two = fetch_and_save(set_two, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_two)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleeping for 6 mins, so that Rate Limit time is reduced\n",
    "print(\"Sleeping for 6 mins.\")\n",
    "time.sleep(360)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "# fetching data 3rd iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_three = fetch_and_save(set_three, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_three)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the failed ids into one master list\n",
    "failed_ids = test_one + test_two + test_three\n",
    "print(\"Total failed request are: {}. \\n\".format(len(failed_ids)))\n",
    "\n",
    "# ids that failed and the ones that passed\n",
    "indi_fail = []; success = []\n",
    "\n",
    "#for each failed id, lets try to fetch status individually.\n",
    "for failed_id in failed_ids:\n",
    "    temp = fetch_and_save(ids = None, api_ins = api, one_id = failed_id)\n",
    "    indi_fail.append(temp[0])\n",
    "\n",
    "# removing empty elements from list\n",
    "success = [x for x in indi_fail if not isinstance(x, (int))]\n",
    "indi_fail = [x for x in indi_fail if isinstance(x, (int))]\n",
    "\n",
    "# checking if there is change\n",
    "print(\"\\nWe were able to retrieve {} records, others failed.\".format(len(failed_ids) - len(indi_fail)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [x] **Okay, let's combine the successful jsons into one file, called the `tweet_master.txt`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combing all successful jsons into one master file\n",
    "json_1 = pd.read_json('tweet_json.txt', lines = True, encoding = 'utf-8')\n",
    "json_2 = pd.read_json('tweet_json_1.txt', lines = True, encoding = 'utf-8')\n",
    "json_3 = pd.read_json('tweet_json_2.txt', lines = True, encoding = 'utf-8')\n",
    "\n",
    "# total rows that we need to have in our resulting dataframe\n",
    "print(json_1.shape[0] + json_2.shape[0] + json_3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_master = pd.concat([json_1, json_2, json_3], ignore_index = True, join = 'outer', sort = True)\n",
    "json_master.to_json('tweet_master.txt', orient = 'records', lines = True)\n",
    "json_master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing objects that are not required.\n",
    "del archive, img_pre_test\n",
    "del json_1, json_2, json_3, json_master\n",
    "del indi_fail, end, start, test_one, test_two, test_three, set_one, set_two, set_three\n",
    "del consumer_key, consumer_secret, access_token, access_secret, auth, api\n",
    "\n",
    "# we are not removing success and files_list, making sure we stick to good programming practices - reusablity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [x] **Last thing to do is to tidy up our folder, let's get going.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'aabcd.csv',\n",
       " 'dataset',\n",
       " 'error.png',\n",
       " 'New Text Document.txt',\n",
       " 'README.md',\n",
       " 'test.py',\n",
       " 'twitter-archive-enhanced.xlsx',\n",
       " 'twitter_text.csv',\n",
       " 'wrangle_act.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moving all data files under one folder - dataset\n",
    "# removing the temporary files, that acted as placeholders\n",
    "\n",
    "# creating the folder\n",
    "folder = 'dataset'\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "# we know that our master datasets for this project are\n",
    "# 1. twitter-archive-enhanced.csv\n",
    "# 2. image-predictions.tsv\n",
    "# 3. tweet_json_master.txt\n",
    "# let us move these files\n",
    "\n",
    "# updating our files_list\n",
    "files_list[-1] = 'tweet_master.txt'\n",
    "\n",
    "# moving only required files\n",
    "for file in files_list:\n",
    "    if os.path.exists(file):\n",
    "        os.rename(file, folder+'/'+file)\n",
    "\n",
    "# removing the tweet_json and tweet_json_1 files as they are not required anymore\n",
    "for file in [s for s in os.listdir() if \"tweet_json\" in s]:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    \n",
    "# lisitng the current directory\n",
    "os.listdir()\n",
    "\n",
    "# clean and neat, lets get with assessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming files_list\n",
    "for i in range(3):\n",
    "    files_list[i] = folder + '/'+ files_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [x] **Last thing to do is to extract `retweet_count` and `favourite_count` from `tweet_master.txt`, saving the result as .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending success to the master dataset.\n",
    "tweet_json = pd.read_json(files_list[2], lines = True, encoding = 'utf-8')\n",
    "tweet_json = tweet_json[['id', 'retweet_count','favorite_count']]\n",
    "tweet_json.rename(index = str,\n",
    "                  columns={'id' : 'tweet_id', 'retweet_count': 'retweets','favorite_count': 'favorites'},\n",
    "                  inplace = True)\n",
    "tweet_json = pd.concat([tweet_json, pd.DataFrame.from_dict(success)],\n",
    "                       ignore_index = True, sort = True)\n",
    "tweet_json.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe into master file\n",
    "tweet_json.to_json(files_list[2], orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Gathering\n",
    "\n",
    "- We know, that gathering is a the first step in wrangling.\n",
    "- We were successful in gathering from three different sources with different techniques:\n",
    "    - Data given at hand.\n",
    "    - Fetch from flat file stored on a server.\n",
    "    - From API.\n",
    "\n",
    "- There a total of 14 missing data points, tried a different ways for retrieving them, using the API as well as `twurl` of the `Ruby` package, but they were not to be found, as stated below in the highlighted section.\n",
    "\n",
    "***<span style=\"color: ##6c6cff\">So let's start with assessing the data.</span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![error](error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load up dataset, and starting assessing them.\n",
    "archive =  pd.read_csv(files_list[0], encoding = 'utf-8')\n",
    "img_pre = pd.read_csv(files_list[1], sep = '\\t', encoding = 'utf-8')\n",
    "retweets_fav = pd.read_json(files_list[2], lines = True, encoding =  'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues to sort!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing out archive - visual assessment\n",
    "archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic Assessment 1 - Information\n",
    "archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmtic Assessment 2 - Describe\n",
    "archive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates - tweet_ids\n",
    "sum(archive.tweet_id.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkin if we have more than one class of dogs assigned to dog\n",
    "# the following are the only combinations that are present in the dataset\n",
    "cond_1 = (archive['doggo'] == 'doggo') & (archive['floofer'] == 'floofer')\n",
    "cond_2 = (archive['doggo'] == 'doggo') & (archive['pupper'] == 'pupper')\n",
    "cond_3 = (archive['doggo'] == 'doggo') & (archive['puppo'] == 'puppo')\n",
    "\n",
    "# printing these entries\n",
    "archive[cond_1 | cond_2 | cond_3][['tweet_id', 'text', 'doggo', 'floofer', 'pupper', 'puppo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(archive[cond_1 | cond_2 | cond_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`twitter-archive-enhanced.csv`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- `rating_numerator` : has values such as 1, 3.. e.t.c - **Data Quality Dimension - `Consistency`**.\n",
    "- `rating_denominator` : have values, less than 10, for example, the tweet_id - 666287406224695296 has the number 2 as its value - **Data Quality Dimension - `Consistency`**. \n",
    "- We see that, Articles - `a`, `an`, `the` have been used to name dogs, as well as words such as `such`, `quite` - **Data Quality Dimension - `Validity`**.\n",
    "- There are instances where the names of dogs are in lowercases - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- `rating_numerator` : has a maximum value of 1766 - **Data Quality Dimension - `Consistency`**. \n",
    "- `rating_denominator` : has a maximum value of 170 - **Data Quality Dimension - `Consistency`**.\n",
    "- All in all, this dataset appears to be clean, except for `expanded_url` - we have about 59 instances missing - **Data Quality Dimension - `Completeness`**.\n",
    "- We can see that there are more than one class assigned to tweets, analyze and assign proper dog class so that melting is easy - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- we can see that, there are four classes of dogs `doggo`, `floofer`, `puppo`, `pupper`; these should a part of one unit - `dog_class` - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- `in_reply_to_status_id`, `retweeted_status_id`, `retweeted_status_user_id`, `in_reply_to_user_id` of type float64 must be converted into int - **Data Quality Dimension - `Validity`**.\n",
    "- `timestamp`, `retweeted_status_timestamp` of type object must be converted into datatime - **Data Quality Dimension - `Validity`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing img_predictions dataset\n",
    "img_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic Assessment - Information\n",
    "img_pre.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates\n",
    "img_pre[img_pre['jpg_url'].duplicated(keep = False)].sort_values(by = 'jpg_url')[['tweet_id', 'jpg_url']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **`image-predictions.tsv`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- We have few dog breeds that are represented in lowercase.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- We have about 281 images on a whole, that are missing with respect to our `twitter-archive-enhanced.csv` file - **Data Quality Dimension - `Completeness`**.\n",
    "- We can see that, we have about `66` duplicates **OR** a pair of tweets are pointing to same *`jpg_url`* - **Data Quality Dimension - `Accuracy`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- None. \n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing tweet_master dataset\n",
    "retweets_fav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_fav.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **`tweet_master.txt`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- None.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- We have about 14 missing records - **Data Quality Dimension - `Completeness`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- None.\n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Assessing\n",
    "\n",
    "- Completed the second step.\n",
    "- The following are the insights:\n",
    "    - from `twitter-archive-enhanced.csv` datset, the rating_numerator and denominator need to be fixed.\n",
    "    - the dataset also represents row values as columns, which needs to be fixed.\n",
    "    - the dataset also has structural issues such as wrong datatype assigned to a column.\n",
    "    - from `images-preductions.tsv` dataset, there is consistency issue with naming dog breeds.\n",
    "    - the dataset isn't complete when compared to `twitter-archive-enhanced.csv`, we have about 281 missing tweets.\n",
    "    - Also we have `jpg_urls'` that are pointing to a pair of tweets.\n",
    "    - `tweet_master.txt` dataset has about 14 missing records.\n",
    "    - the dataset alone hold the information about retweets and favourites - bad form of schema normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Important!, before we get to cleaning, let's drop rows from image-predictions, that are false in dog_1,_2 and _3, as they are not related to our dataset.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select those rows that are either true or false and not all false\n",
    "img_pre = img_pre[~((img_pre.p1_dog == False) & (img_pre.p2_dog == False) & (img_pre.p3_dog == False))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asserting the lenght to be 0\n",
    "assert len(img_pre[(img_pre.p1_dog == False) & (img_pre.p2_dog == False) & (img_pre.p3_dog == False)]) == 0, \"Check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2356 entries, 0 to 2355\n",
      "Data columns (total 30 columns):\n",
      "tweet_id                      2356 non-null int64\n",
      "in_reply_to_status_id         78 non-null float64\n",
      "in_reply_to_user_id           78 non-null float64\n",
      "timestamp                     2356 non-null object\n",
      "source                        2356 non-null object\n",
      "text                          2356 non-null object\n",
      "retweeted_status_id           181 non-null float64\n",
      "retweeted_status_user_id      181 non-null float64\n",
      "retweeted_status_timestamp    181 non-null object\n",
      "expanded_urls                 2297 non-null object\n",
      "rating_numerator              2356 non-null int64\n",
      "rating_denominator            2356 non-null int64\n",
      "name                          2356 non-null object\n",
      "doggo                         2356 non-null object\n",
      "floofer                       2356 non-null object\n",
      "pupper                        2356 non-null object\n",
      "puppo                         2356 non-null object\n",
      "jpg_url                       1751 non-null object\n",
      "img_num                       1751 non-null float64\n",
      "p1                            1751 non-null object\n",
      "p1_conf                       1751 non-null float64\n",
      "p1_dog                        1751 non-null object\n",
      "p2                            1751 non-null object\n",
      "p2_conf                       1751 non-null float64\n",
      "p2_dog                        1751 non-null object\n",
      "p3                            1751 non-null object\n",
      "p3_conf                       1751 non-null float64\n",
      "p3_dog                        1751 non-null object\n",
      "favorites                     2342 non-null float64\n",
      "retweets                      2342 non-null float64\n",
      "dtypes: float64(10), int64(3), object(17)\n",
      "memory usage: 570.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# the master dataset\n",
    "master_set = archive.merge(img_pre, how = 'left', on = ['tweet_id'])\n",
    "master_set = master_set.merge(retweets_fav, how = 'left', on = ['tweet_id'])\n",
    "files_list.append('dataset/master_set_raw.csv')\n",
    "master_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file to local disk.\n",
    "master_set.to_csv(files_list[3], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the master set\n",
    "master_set = pd.read_csv(files_list[3], encoding = 'utf-8')\n",
    "master_copy = master_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues to Clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Basic cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Assign proper class for the above 14 tweets before melting.\n",
    "- Delete *retweets* with *any duplicates* and get rid of *tweets with **no** images*.\n",
    "- Once done, drop the following columns:\n",
    "    1. `retweeted_status_id`\n",
    "    2. `retweeted_status_user_id`\n",
    "    3. `retweeted_status_timestamp`\n",
    "    4. `in_reply_to_status_id`\n",
    "    5. `in_reply_to_user_id`\n",
    "    \n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tweet_id  \\\n",
      "191   855851453814013952   \n",
      "200   854010172552949760   \n",
      "460   817777686764523521   \n",
      "531   808106460588765185   \n",
      "565   802265048156610565   \n",
      "575   801115127852503040   \n",
      "705   785639753186217984   \n",
      "733   781308096455073793   \n",
      "778   775898661951791106   \n",
      "822   770093767776997377   \n",
      "889   759793422261743616   \n",
      "956   751583847268179968   \n",
      "1063  741067306818797568   \n",
      "1113  733109485275860992   \n",
      "\n",
      "                                                                                                                                                                      text  \n",
      "191   Here's a puppo participating in the #ScienceMarch. Cleverly disguising her own doggo agenda. 13/10 would keep the planet habitable for https://t.co/cMhq16isel        \n",
      "200   At first I thought this was a shy doggo, but it's actually a Rare Canadian Floofer Owl. Amateurs would confuse the two. 11/10 only send dogs https://t.co/TXdT3tmuYk  \n",
      "460   This is Dido. She's playing the lead role in \"Pupper Stops to Catch Snow Before Resuming Shadow Box with Dried Apple.\" 13/10 (IG: didodoggo) https://t.co/m7isZrOBX7  \n",
      "531   Here we have Burke (pupper) and Dexter (doggo). Pupper wants to be exactly like doggo. Both 12/10 would pet at same time https://t.co/ANBpEYHaho                      \n",
      "565   Like doggo, like pupper version 2. Both 11/10 https://t.co/9IxWAXFqze                                                                                                 \n",
      "575   This is Bones. He's being haunted by another doggo of roughly the same size. 12/10 deep breaths pupper everything's fine https://t.co/55Dqe0SJNj                      \n",
      "705   This is Pinot. He's a sophisticated doggo. You can tell by the hat. Also pointier than your average pupper. Still 10/10 would pet cautiously https://t.co/f2wmLZTPHd  \n",
      "733   Pupper butt 1, Doggo 0. Both 12/10 https://t.co/WQvcPEpH2u                                                                                                            \n",
      "778   RT @dog_rates: Like father (doggo), like son (pupper). Both 12/10 https://t.co/pG2inLaOda                                                                             \n",
      "822   RT @dog_rates: This is just downright precious af. 12/10 for both pupper and doggo https://t.co/o5J479bZUC                                                            \n",
      "889   Meet Maggie &amp; Lila. Maggie is the doggo, Lila is the pupper. They are sisters. Both 12/10 would pet at the same time https://t.co/MYwR4DQKll                      \n",
      "956   Please stop sending it pictures that don't even have a doggo or pupper in them. Churlish af. 5/10 neat couch tho https://t.co/u2c9c7qSg8                              \n",
      "1063  This is just downright precious af. 12/10 for both pupper and doggo https://t.co/o5J479bZUC                                                                           \n",
      "1113  Like father (doggo), like son (pupper). Both 12/10 https://t.co/pG2inLaOda                                                                                            \n"
     ]
    }
   ],
   "source": [
    "# setting column width to -1\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "cond_1 = (master_copy['doggo'] == 'doggo') & (master_copy['floofer'] == 'floofer')\n",
    "cond_2 = (master_copy['doggo'] == 'doggo') & (master_copy['pupper'] == 'pupper')\n",
    "cond_3 = (master_copy['doggo'] == 'doggo') & (master_copy['puppo'] == 'puppo')\n",
    "print(master_copy[cond_1 | cond_2 | cond_3][['tweet_id', 'text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Assign the following:***\n",
    "1. 855851453814013952: puppo\n",
    "2. 854010172552949760: floofer\n",
    "3. 817777686764523521: pupper\n",
    "4. 808106460588765185: pupper\n",
    "5. 802265048156610565: pupper\n",
    "6. 801115127852503040: pupper\n",
    "7. 785639753186217984: pupper\n",
    "8. 781308096455073793: pupper\n",
    "9. 775898661951791106: pupper\n",
    "10. 770093767776997377: pupper\n",
    "11. 759793422261743616: pupper\n",
    "12. 751583847268179968: doggo\n",
    "13. 741067306818797568: doggo\n",
    "14. 733109485275860992: doggo\n",
    "\n",
    "**<span style=\"color: green\">I like puppies, so for most of the entries it is pupper!</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning values.\n",
    "master_copy.loc[master_copy['tweet_id'] == 855851453814013952, ['doggo', 'floofer', 'pupper']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 854010172552949760, ['doggo', 'pupper', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 817777686764523521, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 808106460588765185, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 802265048156610565, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 801115127852503040, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 785639753186217984, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 781308096455073793, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 775898661951791106, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 770093767776997377, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 759793422261743616, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 751583847268179968, ['pupper', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 741067306818797568, ['pupper', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 733109485275860992, ['pupper', 'floofer', 'puppo']] = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>855851453814013952</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>puppo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>854010172552949760</td>\n",
       "      <td>None</td>\n",
       "      <td>floofer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>817777686764523521</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>808106460588765185</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>802265048156610565</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>801115127852503040</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>785639753186217984</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>781308096455073793</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>775898661951791106</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>770093767776997377</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>759793422261743616</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>751583847268179968</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>741067306818797568</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>733109485275860992</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  doggo  floofer  pupper  puppo\n",
       "191   855851453814013952   None     None    None  puppo\n",
       "200   854010172552949760   None  floofer    None   None\n",
       "460   817777686764523521   None     None  pupper   None\n",
       "531   808106460588765185   None     None  pupper   None\n",
       "565   802265048156610565   None     None  pupper   None\n",
       "575   801115127852503040   None     None  pupper   None\n",
       "705   785639753186217984   None     None  pupper   None\n",
       "733   781308096455073793   None     None  pupper   None\n",
       "778   775898661951791106   None     None  pupper   None\n",
       "822   770093767776997377   None     None  pupper   None\n",
       "889   759793422261743616   None     None  pupper   None\n",
       "956   751583847268179968  doggo     None    None   None\n",
       "1063  741067306818797568  doggo     None    None   None\n",
       "1113  733109485275860992  doggo     None    None   None"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all values have been properly assigned\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "master_copy[cond_1 | cond_2 | cond_3][['tweet_id', 'doggo', 'floofer', 'pupper', 'puppo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting those, tweets that have no retweets\n",
    "master_copy = master_copy[pd.isnull(master_copy['retweeted_status_id'])]\n",
    "\n",
    "# deleting duplicates if any\n",
    "master_copy = master_copy.drop_duplicates()\n",
    "\n",
    "# deleting those tweets with no images.\n",
    "master_copy = master_copy.dropna(subset = ['jpg_url'])\n",
    "\n",
    "# reseting index\n",
    "master_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# droping columns\n",
    "master_copy = master_copy.drop(labels = ['retweeted_status_id',\n",
    "                                         'retweeted_status_user_id',\n",
    "                                         'retweeted_status_timestamp',\n",
    "                                         'in_reply_to_status_id', \n",
    "                                         'in_reply_to_user_id'],\n",
    "                               axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1686, 25)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after droping the columns, we should have about 25 dimensions/columns\n",
    "master_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Condense wide-format to long-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Condense `doggo`, `floofer`, `pupper`, `puppo` as `dog_class`.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Doggo: 57\n",
      "Count of Floofer: 8\n",
      "Count of Pupper: 173\n",
      "Count of Puppo: 22\n"
     ]
    }
   ],
   "source": [
    "# to make sure that we have \n",
    "doggo = master_copy.doggo.value_counts()['doggo']\n",
    "floofer = master_copy.floofer.value_counts()['floofer']\n",
    "pupper = master_copy.pupper.value_counts()['pupper']\n",
    "puppo = master_copy.puppo.value_counts()['puppo']\n",
    "\n",
    "# printing count of each class\n",
    "print(\"Count of Doggo: {}\\nCount of Floofer: {}\\nCount of Pupper: {}\\nCount of Puppo: {}\".format(doggo,\n",
    "                                                                                                 floofer,\n",
    "                                                                                                 pupper,\n",
    "                                                                                                 puppo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the columns that are to be melted\n",
    "columns_to_melt = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "columns_to_stay = [x for x in master_copy.columns.tolist() if x not in columns_to_melt]\n",
    "\n",
    "# melting the the columns into values\n",
    "master_copy = pd.melt(master_copy, id_vars = columns_to_stay, value_vars = columns_to_melt, \n",
    "                         var_name = 'stages', value_name = 'dog_class')\n",
    "\n",
    "# Delete column 'stages'\n",
    "master_copy = master_copy.drop('stages', 1)\n",
    "\n",
    "# dropping duplicates\n",
    "master_copy = master_copy.sort_values('dog_class').drop_duplicates('tweet_id', keep = 'last')\n",
    "master_copy.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assert\n",
    "assert doggo == master_copy.dog_class.value_counts()['doggo'], \"Some entries are missing\"\n",
    "assert floofer == master_copy.dog_class.value_counts()['floofer'], \"Some entries are missing\"\n",
    "assert pupper == master_copy.dog_class.value_counts()['pupper'], \"Some entries are missing\"\n",
    "assert puppo == master_copy.dog_class.value_counts()['puppo'], \"Some entries are missing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fix all inaccurate data.\n",
    "\n",
    "#### Define\n",
    "- fix names of dogs.\n",
    "- fix ratings.\n",
    "- check source column.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking source column\n",
    "master_copy.source.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red\">Okay! only three values, a categorical variable</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# assiging unique values to source.\n",
    "master_copy['source'] = master_copy['source'].apply(lambda x: re.findall(r'>(.*)<', x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>879008229531029506</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>This is Beau. That is Beau's balloon. He takes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>685315239903100929</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>I would like everyone to appreciate this pup's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>702217446468493312</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>I know it's tempting, but please stop sending ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>700847567345688576</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Meet Crouton. He's a Galapagos Boonwiddle. Has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>806219024703037440</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>We only rate dogs. Please stop sending in non-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id              source  \\\n",
       "106  879008229531029506  Twitter for iPhone   \n",
       "255  685315239903100929  Twitter for iPhone   \n",
       "438  702217446468493312  Twitter for iPhone   \n",
       "430  700847567345688576  Twitter for iPhone   \n",
       "899  806219024703037440  Twitter for iPhone   \n",
       "\n",
       "                                                  text  \n",
       "106  This is Beau. That is Beau's balloon. He takes...  \n",
       "255  I would like everyone to appreciate this pup's...  \n",
       "438  I know it's tempting, but please stop sending ...  \n",
       "430  Meet Crouton. He's a Galapagos Boonwiddle. Has...  \n",
       "899  We only rate dogs. Please stop sending in non-...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a look at sample of 5 rows\n",
    "master_copy.sample(5)[['tweet_id', 'source', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing names\n",
    "non_names = master_copy.name.str.islower()\n",
    "non_names = list(set(master_copy[non_names]['name'].tolist()))\n",
    "flag = master_copy.name.str.len() == 1 & master_copy.name.str.isupper()\n",
    "non_names.append(master_copy[flag][['tweet_id', 'name']]['name'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all garbage names with none, once done, we'll use the text field to extract names\n",
    "for name in master_copy.name:\n",
    "    if name in non_names:\n",
    "        master_copy.loc[master_copy['name'] == name, ['name']] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are any non_names after the operation\n",
    "assert len(master_copy[(master_copy.name.str.islower()) & (flag)]) == 0, \"Check code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The following are patterns observed in `text` field, we shall use the :***\n",
    "- This is [name] ..\n",
    "- Meet [name] ..\n",
    "- Say hello to [name] ..\n",
    "- .. named [name] ..\n",
    "- .. name is [name] ..\n",
    "\n",
    "We will treat those cases to get the names from the text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting names using regular expression.\n",
    "dog_names = []\n",
    "\n",
    "# assigning patterns\n",
    "pattern_1 = r'(T|t)his\\sis\\s([^.|,]*)'\n",
    "pattern_2 = r'Meet\\s([^.|,]*)'\n",
    "pattern_3 = r'Say\\shello\\sto\\s([^.|,]*)'\n",
    "pattern_4 = r'name\\sis\\s([^.|,]*)'\n",
    "\n",
    "# looping through text and extracting names\n",
    "for text in master_copy['text']:\n",
    "    # Start with 'This is '\n",
    "    if re.search(pattern_1, text):\n",
    "        # if our match has alternate name\n",
    "        if \"(\" in re.search(pattern_1, text).group(2):\n",
    "            dog_names.append(re.search(pattern_1, text).group(2).split()[0])\n",
    "        # if our match has AKA in it\n",
    "        elif \"AKA\" in re.search(pattern_1, text).group(2):\n",
    "            dog_names.append(re.search(pattern_1, text).group(2).split()[0])\n",
    "        # if our name has two dogs\n",
    "        elif '&amp;' in re.search(pattern_1, text).group(2):\n",
    "            temp = re.search(pattern_1, text).group(2).split()\n",
    "            if len(temp) == 1:\n",
    "                dog_names.append(temp[0])\n",
    "            elif len(temp) == 3:\n",
    "                dog_names.append(temp[0]+\"_\"+temp[-1])\n",
    "            else:\n",
    "                dog_names.append(temp[0]+\"_\"+temp[-2])\n",
    "        elif 'named' in re.search(pattern_1, text).group(2):\n",
    "            temp = re.search(pattern_1, text).group(2).split()\n",
    "            dog_names.append(temp[-1])\n",
    "        # just appending the name\n",
    "        else:\n",
    "            dog_names.append(re.search(pattern_1, text).group(2))\n",
    "    \n",
    "    # Start with 'Meet '\n",
    "    elif re.search(pattern_2, text):\n",
    "        # if our name has two dogs\n",
    "        if '&amp;' in re.search(pattern_2, text).group(1):\n",
    "            temp = re.search(pattern_2, text).group(1).split()\n",
    "            if len(temp) == 1:\n",
    "                dog_names.append(temp[0])\n",
    "            elif len(temp) == 3:\n",
    "                dog_names.append(temp[0]+\"_\"+temp[-1])\n",
    "            else:\n",
    "                dog_names.append(temp[0]+\"_\"+temp[-2])\n",
    "        # if our name has alternatives\n",
    "        elif '(' in re.search(pattern_2, text).group(1):\n",
    "            dog_names.append(re.search(pattern_2, text).group(1).split()[0])\n",
    "        # just appending the name\n",
    "        else:\n",
    "            dog_names.append(re.search(pattern_2, text).group(1))\n",
    "    \n",
    "    # Start with 'Say hello to '\n",
    "    elif re.search(pattern_3, text):\n",
    "        # if our match has alternate name\n",
    "        if '(' in re.search(pattern_3, text).group(1):\n",
    "            dog_names.append(re.search(pattern_3, text).group(1).split()[0])\n",
    "        # if our name has two dogs\n",
    "        elif '&amp;' in re.search(pattern_3, text).group(1):\n",
    "            temp = re.search(pattern_3, text).group(1).split()\n",
    "            if len(temp) == 1:\n",
    "                dog_names.append(temp[0])\n",
    "            elif len(temp) == 3:\n",
    "                dog_names.append(temp[0]+\"_\"+temp[-1])\n",
    "            else:\n",
    "                dog_names.append(temp[0]+\"_\"+temp[-2])\n",
    "        else:\n",
    "            dog_names.append(re.search(pattern_3, text).group(1))    \n",
    "    \n",
    "    # contains 'name is'\n",
    "    elif re.search(pattern_4, text):\n",
    "        if len(re.search(pattern_4, text).group(1).split()) == 1:\n",
    "            dog_names.append(re.search(pattern_4, text).group(1))\n",
    "        else:\n",
    "            temp = re.search(pattern_4, text).group(1).split()\n",
    "            dog_names.append(temp[0])\n",
    "        \n",
    "    # No name specified or other style\n",
    "    else:\n",
    "        dog_names.append('None')\n",
    "\n",
    "# adding this new set of names to our master_copy\n",
    "master_copy['dog_names'] = dog_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " 'Dook_Milo',\n",
       " 'Big',\n",
       " 'Naphaniel',\n",
       " 'Frank',\n",
       " 'Klint',\n",
       " 'Kial',\n",
       " 'Olive',\n",
       " 'Jessiga',\n",
       " 'Hall and Oates',\n",
       " 'Filup',\n",
       " 'Cheryl',\n",
       " 'Tedrick',\n",
       " 'Stu',\n",
       " 'Erik',\n",
       " 'Cleopatricia',\n",
       " 'Otis',\n",
       " 'Jiminy',\n",
       " 'Alfie',\n",
       " 'Philippe from Soviet Russia',\n",
       " 'Kohl',\n",
       " 'Carll',\n",
       " 'Keet',\n",
       " 'Clybe',\n",
       " 'Gabe',\n",
       " 'Pipsy',\n",
       " 'Bradlay',\n",
       " 'Churlie',\n",
       " 'Kenneth',\n",
       " 'Clarence',\n",
       " 'Timison',\n",
       " 'Joshwa',\n",
       " 'Genevieve',\n",
       " 'Fwed',\n",
       " 'Biden',\n",
       " 'Cupcake',\n",
       " 'Reese and Twips',\n",
       " 'Alfonso',\n",
       " 'Skittles',\n",
       " 'Torque',\n",
       " 'Bisquick',\n",
       " 'Ron',\n",
       " 'Jockson',\n",
       " 'Jareld',\n",
       " 'Jeph',\n",
       " 'Walter',\n",
       " 'Scout',\n",
       " 'Kreggory',\n",
       " 'Jimothy',\n",
       " 'Christoper',\n",
       " 'Johm',\n",
       " 'Lugan',\n",
       " 'Josep',\n",
       " 'Octaviath',\n",
       " 'Tilly',\n",
       " 'Nelly',\n",
       " 'Dante',\n",
       " 'Penny',\n",
       " 'Aja',\n",
       " 'Emmy',\n",
       " 'Shadow',\n",
       " 'Beau',\n",
       " 'Jack',\n",
       " 'Bailey',\n",
       " 'Maya',\n",
       " 'Canela',\n",
       " 'Jeffrey',\n",
       " 'Gerald',\n",
       " 'Ralphus',\n",
       " 'Zeke',\n",
       " 'Jim',\n",
       " 'Oliver',\n",
       " 'Mingus',\n",
       " 'Bruno',\n",
       " 'Koda',\n",
       " 'Zoey',\n",
       " 'Jax',\n",
       " 'Franklin',\n",
       " 'Darla',\n",
       " 'Archie',\n",
       " 'Ted',\n",
       " 'Waffles',\n",
       " 'Jimbo',\n",
       " 'Romeo',\n",
       " 'Jesse',\n",
       " 'Bella',\n",
       " 'Gary',\n",
       " 'Rey',\n",
       " 'Koko',\n",
       " 'Alfy',\n",
       " 'Stanley',\n",
       " 'Rusty',\n",
       " 'Noah',\n",
       " 'Kevin',\n",
       " 'Lola',\n",
       " 'Earl',\n",
       " 'Maisey',\n",
       " 'Duddles',\n",
       " 'Benedict',\n",
       " 'Venti',\n",
       " 'Nugget and Hank',\n",
       " 'Dunkin',\n",
       " 'Bodie',\n",
       " 'Sadie_&amp;',\n",
       " 'Freddery',\n",
       " 'Kobe',\n",
       " 'Chester',\n",
       " 'Oliviér',\n",
       " 'Brian',\n",
       " 'Colin',\n",
       " 'Tupawc',\n",
       " 'Lolo',\n",
       " 'Hazel',\n",
       " 'Cedrick',\n",
       " 'Vinscent',\n",
       " 'Peaches',\n",
       " 'Molly',\n",
       " 'Bob',\n",
       " 'Leo',\n",
       " 'Richie and Plip',\n",
       " 'Kingsley Wellensworth III',\n",
       " 'Edgar',\n",
       " 'Cody',\n",
       " 'Amber',\n",
       " 'Calvin',\n",
       " 'Cuddles',\n",
       " 'CeCe',\n",
       " 'Reagan',\n",
       " 'Claude',\n",
       " 'Calbert',\n",
       " 'Fillup',\n",
       " 'Nala',\n",
       " 'Thumas',\n",
       " 'Coops',\n",
       " 'Miley',\n",
       " 'Eriq',\n",
       " 'Wyatt',\n",
       " 'Berb',\n",
       " 'Reptar',\n",
       " 'Rosie',\n",
       " 'Oakley',\n",
       " 'Maximus',\n",
       " 'Charlie',\n",
       " 'Bentley',\n",
       " 'Ole',\n",
       " 'Cassie',\n",
       " 'Doug',\n",
       " 'Lily',\n",
       " 'Beau_Wilbur',\n",
       " 'Phred',\n",
       " 'Geoff',\n",
       " 'Obi',\n",
       " 'Pippa',\n",
       " 'Theodore',\n",
       " 'Perry',\n",
       " 'Bruiser_Charlie',\n",
       " 'Oreo',\n",
       " 'Rocky',\n",
       " 'Brooks',\n",
       " 'Lorelei',\n",
       " 'Tino',\n",
       " 'Kulet',\n",
       " 'Joey and Izzy',\n",
       " 'Taco',\n",
       " 'Darrel',\n",
       " 'Lulu',\n",
       " 'Griffin',\n",
       " 'Sebastian',\n",
       " 'Buddy',\n",
       " 'Tiger',\n",
       " 'Sadie',\n",
       " 'Flávio',\n",
       " 'Pubert',\n",
       " 'Brad',\n",
       " 'Lucky',\n",
       " 'Herald',\n",
       " 'Chipson',\n",
       " 'Frönq',\n",
       " 'Carl',\n",
       " 'Luca',\n",
       " 'Fynn_Taco',\n",
       " 'Durg',\n",
       " 'Covach',\n",
       " 'Crimson',\n",
       " 'Horace',\n",
       " 'Baxter',\n",
       " 'Samson',\n",
       " 'Louis',\n",
       " 'Gordon',\n",
       " 'Tyrone',\n",
       " 'Kramer',\n",
       " 'Marq',\n",
       " 'Opal',\n",
       " 'Trooper_Maya',\n",
       " 'Kilo',\n",
       " 'Dotsy',\n",
       " 'Kaia',\n",
       " 'Curtis',\n",
       " 'Siba',\n",
       " 'Michelangelope',\n",
       " 'Nico',\n",
       " 'Oscar',\n",
       " 'Zeus',\n",
       " 'Toby',\n",
       " 'Bubbles',\n",
       " 'Stephanus',\n",
       " 'Bluebert',\n",
       " 'Jennifur',\n",
       " 'Link',\n",
       " 'Ebby',\n",
       " 'Barclay',\n",
       " 'Suki',\n",
       " 'Travis and Flurp',\n",
       " 'Sora',\n",
       " 'Staniel',\n",
       " 'Milo and Amos',\n",
       " 'Roosevelt',\n",
       " 'Olive and Ruby',\n",
       " 'Alexanderson',\n",
       " 'Sunny and Roxy',\n",
       " 'Stubert',\n",
       " 'Opie and Clarkus',\n",
       " 'Lance',\n",
       " 'Piper',\n",
       " 'Kanu',\n",
       " 'Kane',\n",
       " 'Bowie',\n",
       " 'Carper',\n",
       " 'Fred-Rick',\n",
       " 'Colby',\n",
       " 'Bloop',\n",
       " 'Blitz',\n",
       " 'Sprout',\n",
       " 'Karll',\n",
       " 'Cilantro',\n",
       " 'Raymond',\n",
       " 'Enchilada',\n",
       " 'Wallace',\n",
       " 'Flurpson',\n",
       " 'Beemo',\n",
       " 'Sarge',\n",
       " 'Sugar',\n",
       " 'Goose',\n",
       " 'Lucy and Sophie',\n",
       " 'Sid_Murphy',\n",
       " 'Harnold',\n",
       " 'Neptune',\n",
       " 'Charleson',\n",
       " 'Berkeley',\n",
       " 'Panda',\n",
       " 'Schnitzel',\n",
       " 'Jangle',\n",
       " 'Ivar',\n",
       " 'Reginald',\n",
       " 'Ralphé',\n",
       " 'Gunner',\n",
       " 'Lucia',\n",
       " 'Reese',\n",
       " 'Bode',\n",
       " 'Cooper',\n",
       " 'Zara',\n",
       " 'Max',\n",
       " 'Lucy',\n",
       " 'Terrenth',\n",
       " 'Dexter',\n",
       " 'Vincent',\n",
       " 'Crouton',\n",
       " 'Chet',\n",
       " 'Yoda',\n",
       " 'Coopson',\n",
       " 'Bilbo',\n",
       " 'Rilo',\n",
       " 'Rambo_Kiwi',\n",
       " 'Socks',\n",
       " 'Fiji',\n",
       " 'Murphy',\n",
       " 'Kara',\n",
       " 'Bobble',\n",
       " 'Bear',\n",
       " 'Smokey',\n",
       " 'Coco',\n",
       " 'Mister',\n",
       " 'Malikai',\n",
       " 'Klevin',\n",
       " 'Rodney',\n",
       " 'Billy',\n",
       " 'Karma',\n",
       " 'Vince',\n",
       " 'Cecil',\n",
       " 'Olaf',\n",
       " 'Watson',\n",
       " 'Tater',\n",
       " 'Lacy',\n",
       " 'Walker',\n",
       " 'River',\n",
       " 'Katie',\n",
       " 'Ellie',\n",
       " 'Tucker',\n",
       " 'Kathmandu',\n",
       " 'Jebberson',\n",
       " 'Clarkus',\n",
       " 'Keurig',\n",
       " 'Harper',\n",
       " 'Rufus',\n",
       " 'Remington',\n",
       " 'Brody',\n",
       " 'Alejandro',\n",
       " 'JD',\n",
       " 'Dylan',\n",
       " 'Mojo',\n",
       " 'Gizmo',\n",
       " 'Patch',\n",
       " 'Tuco',\n",
       " 'Bradley',\n",
       " 'Scruffers',\n",
       " 'Malcolm',\n",
       " 'Penelope',\n",
       " 'Maggie',\n",
       " 'Sage',\n",
       " 'Amy',\n",
       " 'Jett',\n",
       " 'Cal',\n",
       " 'Holly',\n",
       " 'Sparky',\n",
       " 'Lou',\n",
       " 'Ridley',\n",
       " 'Snickers',\n",
       " 'Mac',\n",
       " 'Frankie',\n",
       " 'Steve',\n",
       " 'Sandy',\n",
       " 'Kip',\n",
       " 'Kendall',\n",
       " 'Shnuggles',\n",
       " 'George',\n",
       " 'Larry',\n",
       " 'Dot',\n",
       " 'Andy',\n",
       " 'Pumpkin',\n",
       " 'Kreg',\n",
       " 'Pippin',\n",
       " 'Franq and Pablo',\n",
       " 'Jackie',\n",
       " 'Darby',\n",
       " 'Kenzie',\n",
       " 'Taz',\n",
       " 'Leonidas',\n",
       " 'Ed',\n",
       " 'Terry',\n",
       " 'Gustaf',\n",
       " 'Mason',\n",
       " 'Trigger',\n",
       " 'Leroi',\n",
       " 'Jaycob',\n",
       " 'Jeremy',\n",
       " 'Chaz',\n",
       " 'Ruby',\n",
       " 'Bernie',\n",
       " 'Terrance',\n",
       " 'Lambeau',\n",
       " 'Clarq',\n",
       " 'Ralf',\n",
       " 'Kirk',\n",
       " 'Tessa',\n",
       " 'Dug',\n",
       " 'Saydee',\n",
       " 'Chip',\n",
       " 'Samsom',\n",
       " 'Billl',\n",
       " 'Ruffles',\n",
       " 'Bobb',\n",
       " 'Sophie',\n",
       " 'DayZ',\n",
       " 'Jo',\n",
       " 'Chuk',\n",
       " 'Gòrdón',\n",
       " 'Spark',\n",
       " 'Amélie',\n",
       " 'Marvin',\n",
       " 'Maks',\n",
       " 'Timofy',\n",
       " 'Zeek',\n",
       " 'Berta',\n",
       " 'Hanz',\n",
       " 'Winifred',\n",
       " 'Kevon',\n",
       " 'Banditt',\n",
       " 'Jomathan',\n",
       " 'Ronduh',\n",
       " 'Kollin',\n",
       " 'Herb',\n",
       " 'Spork',\n",
       " 'Willy',\n",
       " 'Sam',\n",
       " 'Hank and Sully',\n",
       " 'Paull',\n",
       " 'Bloo',\n",
       " 'Scooter',\n",
       " 'Pluto',\n",
       " 'Danny',\n",
       " 'Gin_Tonic',\n",
       " 'Traviss',\n",
       " 'Creg',\n",
       " 'Antony',\n",
       " 'Ester',\n",
       " 'Alfredo',\n",
       " 'Nigel',\n",
       " 'Daisy',\n",
       " 'Linda',\n",
       " 'Skye',\n",
       " 'Shawwn',\n",
       " 'Kloey',\n",
       " 'Andru',\n",
       " 'Julio',\n",
       " 'Ben_Carson',\n",
       " 'Liam',\n",
       " 'Winston',\n",
       " 'Wally',\n",
       " 'Sandra',\n",
       " 'Hemry',\n",
       " 'Butters',\n",
       " 'Raphael',\n",
       " 'Riley',\n",
       " 'Crumpet',\n",
       " 'Jerome',\n",
       " 'Crystal',\n",
       " 'Grizz',\n",
       " 'Tango',\n",
       " 'Dash',\n",
       " 'Wilson',\n",
       " 'Tug',\n",
       " 'Chompsky',\n",
       " 'Jessifer',\n",
       " 'Tyrus',\n",
       " 'Juckson',\n",
       " 'Tassy_Bee',\n",
       " 'Derek',\n",
       " 'Humphrey',\n",
       " 'Ralph',\n",
       " 'Axel',\n",
       " 'Rubio',\n",
       " 'Hurley',\n",
       " 'Griswold',\n",
       " 'Alice',\n",
       " 'Ozzie',\n",
       " 'Aspen',\n",
       " 'Cheesy',\n",
       " 'Glacier',\n",
       " 'Carly',\n",
       " 'Apollo',\n",
       " 'Ulysses',\n",
       " 'Todo',\n",
       " 'Asher',\n",
       " 'Evy',\n",
       " 'William',\n",
       " 'Mike',\n",
       " 'Hunter',\n",
       " 'Sabertooth',\n",
       " 'Willie',\n",
       " 'Bruce',\n",
       " 'Emmie',\n",
       " 'Ozzy',\n",
       " 'Reggie',\n",
       " 'Goliath',\n",
       " 'Percy',\n",
       " 'Sammy',\n",
       " 'Ash',\n",
       " 'Jeb_Bush',\n",
       " 'Tedders',\n",
       " 'Laela',\n",
       " 'Moe',\n",
       " 'Ava',\n",
       " 'Aiden',\n",
       " 'Acro',\n",
       " 'Mitch',\n",
       " 'Bobbay',\n",
       " 'Kenny',\n",
       " 'Lenny',\n",
       " 'Jonah',\n",
       " 'Donny',\n",
       " 'Striker',\n",
       " 'Bert',\n",
       " 'Duke',\n",
       " 'Vinnie',\n",
       " 'Penny_Gizmo',\n",
       " 'Arnold',\n",
       " 'Yoshi',\n",
       " 'Wylie',\n",
       " 'Buddah',\n",
       " 'Puff',\n",
       " 'Stefan',\n",
       " 'Brandi and Harley',\n",
       " 'Swagger',\n",
       " 'Bauer',\n",
       " 'Tyr',\n",
       " 'Baron',\n",
       " 'Layla',\n",
       " 'Eleanor',\n",
       " 'Sky',\n",
       " 'Oshie',\n",
       " 'Alf',\n",
       " 'Olivia',\n",
       " 'Mary',\n",
       " 'Titan',\n",
       " 'Halo',\n",
       " 'Lennon',\n",
       " 'Stephan',\n",
       " 'Ito',\n",
       " 'Kyro',\n",
       " 'Gus',\n",
       " 'Phil',\n",
       " 'Pavlov',\n",
       " 'Augie',\n",
       " 'Akumi',\n",
       " 'Sunny',\n",
       " 'Flash',\n",
       " 'Harlso',\n",
       " 'Jimison',\n",
       " 'Paisley',\n",
       " 'Hobbes',\n",
       " 'Albus',\n",
       " 'Crawford',\n",
       " 'Cash',\n",
       " 'Howie',\n",
       " 'Comet',\n",
       " 'Dudley',\n",
       " 'Binky',\n",
       " 'Timber',\n",
       " 'Chloe',\n",
       " 'Tebow',\n",
       " 'Strudel',\n",
       " 'Ken',\n",
       " 'Eugene_Patti',\n",
       " 'Autumn',\n",
       " 'Florence',\n",
       " 'Tom',\n",
       " 'Wafer',\n",
       " 'Finn',\n",
       " 'Anna and Elsa',\n",
       " 'Jazzy',\n",
       " 'Major',\n",
       " 'Zeke the Wonder Dog',\n",
       " 'Bo',\n",
       " 'Stella',\n",
       " 'Rizzy',\n",
       " 'Brownie',\n",
       " 'Eli',\n",
       " 'Happy',\n",
       " 'Mosby',\n",
       " 'Misty',\n",
       " 'Mabel',\n",
       " 'Levi',\n",
       " 'Remus',\n",
       " 'BeBe',\n",
       " 'Monster',\n",
       " 'Pupcasso',\n",
       " 'Robin',\n",
       " 'Nida',\n",
       " 'Benji',\n",
       " 'Butter',\n",
       " 'Loomis',\n",
       " 'Mairi',\n",
       " 'Stormy',\n",
       " 'Hero',\n",
       " 'Doc',\n",
       " 'Dallas',\n",
       " 'Mattie',\n",
       " 'Dale',\n",
       " 'Logan',\n",
       " 'Rory',\n",
       " 'Lincoln',\n",
       " 'Moose',\n",
       " 'Newt',\n",
       " 'Maude',\n",
       " 'Iroh',\n",
       " 'Longfellow',\n",
       " 'Chef',\n",
       " 'Mauve and Murphy',\n",
       " 'Milo',\n",
       " 'Ronnie',\n",
       " 'Winnie',\n",
       " 'Marley',\n",
       " 'Django',\n",
       " 'Shooter',\n",
       " 'Juno',\n",
       " 'Dave',\n",
       " 'Moreton',\n",
       " 'Fiona',\n",
       " 'Dobby',\n",
       " 'Clark',\n",
       " 'Nimbus',\n",
       " 'Mack',\n",
       " 'Yogi',\n",
       " 'Snicku',\n",
       " 'Pancake',\n",
       " 'Sweet Pea',\n",
       " 'Einstein',\n",
       " 'Arya',\n",
       " 'Marlee',\n",
       " 'Cermet',\n",
       " 'Callie',\n",
       " 'Oscar and Oliver',\n",
       " 'Thor',\n",
       " 'Trooper',\n",
       " 'Milky',\n",
       " 'Vixen',\n",
       " 'Tuck',\n",
       " 'Furzey',\n",
       " 'Cannon',\n",
       " 'Rontu',\n",
       " 'Jerry',\n",
       " 'Odin',\n",
       " 'Noosh',\n",
       " 'Kyle',\n",
       " 'Snoop',\n",
       " 'Sailor',\n",
       " 'Jarod',\n",
       " 'Benny',\n",
       " 'Lorenzo',\n",
       " 'Burt',\n",
       " 'Dewey',\n",
       " 'Rumble',\n",
       " 'Boomer',\n",
       " 'Lassie',\n",
       " 'Kody',\n",
       " 'Harry',\n",
       " 'Monkey',\n",
       " 'Zooey',\n",
       " 'Quinn',\n",
       " 'Belle',\n",
       " 'Meatball',\n",
       " 'Jarvis',\n",
       " 'Lili',\n",
       " 'Harold',\n",
       " 'Mia',\n",
       " 'Lilly',\n",
       " 'Scooter and his son Montoya',\n",
       " 'Dutch',\n",
       " 'Blue',\n",
       " 'Kuyu',\n",
       " 'Eevee',\n",
       " 'Orion',\n",
       " 'Willow',\n",
       " 'Ike',\n",
       " 'Pawnd',\n",
       " 'Ralphie',\n",
       " 'Boots',\n",
       " 'Brutus and Jersey',\n",
       " 'Alexander Hamilpup',\n",
       " 'Beebop',\n",
       " 'Mutt Ryan',\n",
       " 'Poppy',\n",
       " 'Bronte',\n",
       " 'Sojourner',\n",
       " 'Monty',\n",
       " 'Maddie and Gunner',\n",
       " 'Dawn',\n",
       " 'Indie',\n",
       " 'Tycho',\n",
       " 'Hank',\n",
       " 'Sadie and Daisy',\n",
       " 'Margo',\n",
       " 'Luna',\n",
       " 'Brady',\n",
       " 'Odie',\n",
       " 'Arlo',\n",
       " 'Lipton',\n",
       " 'Chester_Harold',\n",
       " 'Tobi',\n",
       " 'Mookie',\n",
       " 'Godi',\n",
       " 'Kirby',\n",
       " 'Rocco',\n",
       " 'Atlas',\n",
       " 'Rose',\n",
       " 'Albert',\n",
       " 'Bonaparte',\n",
       " 'Indie and Jupiter',\n",
       " 'Sephie',\n",
       " 'Bo and Ty',\n",
       " 'Shelby',\n",
       " 'Emma',\n",
       " 'Shiloh',\n",
       " 'Corey',\n",
       " 'Gustav',\n",
       " 'Brandonald',\n",
       " 'Boston',\n",
       " 'Lilli_Honey',\n",
       " 'Spencer',\n",
       " 'Arlen and Thumpelina',\n",
       " 'Abby',\n",
       " 'Leela',\n",
       " 'Shadoe',\n",
       " 'Brudge',\n",
       " 'Rupert',\n",
       " 'Kayla',\n",
       " 'Louie',\n",
       " 'Bell',\n",
       " 'Philbert',\n",
       " 'Chevy',\n",
       " 'Oakley and Charlie',\n",
       " 'Glenn',\n",
       " 'Bentley and Millie',\n",
       " 'Stark',\n",
       " 'Harvey',\n",
       " 'Kota and her son Benedict',\n",
       " 'Aubie',\n",
       " 'Gromit',\n",
       " 'Tove',\n",
       " 'Blanket',\n",
       " 'Brat',\n",
       " 'Angel',\n",
       " 'Geno',\n",
       " 'Leonard',\n",
       " 'Gilbert',\n",
       " 'Tripp',\n",
       " 'Dietrich',\n",
       " 'Lilah',\n",
       " 'Spanky',\n",
       " 'Jameson',\n",
       " 'Stewie',\n",
       " 'Arnie',\n",
       " 'Meyer',\n",
       " 'Jax_Jil',\n",
       " 'Beckham',\n",
       " 'Cora',\n",
       " 'Bookstore and Seaweed',\n",
       " 'Gert',\n",
       " 'Devón',\n",
       " 'Atticus',\n",
       " 'Grizzie',\n",
       " 'Tayzie',\n",
       " 'Ace',\n",
       " 'Dex',\n",
       " 'Huxley',\n",
       " 'Chadrick',\n",
       " 'Hermione',\n",
       " 'Remy',\n",
       " 'Storkson',\n",
       " 'Finley',\n",
       " 'Heinrich',\n",
       " 'Loki',\n",
       " 'Charles',\n",
       " 'Shakespeare',\n",
       " 'Ralpher',\n",
       " 'Bungalo',\n",
       " 'Sprinkles',\n",
       " \"O'Malley\",\n",
       " 'Dakota',\n",
       " 'Livvie',\n",
       " 'Strider',\n",
       " 'Mya',\n",
       " 'Jay',\n",
       " 'Wesley',\n",
       " 'Maxaroni',\n",
       " 'Solomon',\n",
       " 'Aldrick',\n",
       " 'Grey',\n",
       " 'Rorie',\n",
       " 'Nollie',\n",
       " 'Al Cabone',\n",
       " 'Jackson',\n",
       " 'Chase',\n",
       " 'Timmy',\n",
       " 'Bretagne',\n",
       " 'Klein',\n",
       " 'Cali',\n",
       " 'DonDon',\n",
       " 'Ziva',\n",
       " 'Simba',\n",
       " 'Hercules',\n",
       " 'Dixie',\n",
       " 'Fizz',\n",
       " 'Davey',\n",
       " 'Mimosa',\n",
       " 'Meera',\n",
       " 'Rhino',\n",
       " 'Pete',\n",
       " 'Emanuel',\n",
       " 'Miguel',\n",
       " 'Smiley',\n",
       " 'Barney',\n",
       " 'Wishes',\n",
       " 'Napolean',\n",
       " 'Sundance',\n",
       " 'Chubbs',\n",
       " 'Lenox',\n",
       " 'Astrid',\n",
       " 'Sobe',\n",
       " 'Sampson',\n",
       " 'Duchess',\n",
       " 'Cupid',\n",
       " 'Pilot',\n",
       " 'Kellogg',\n",
       " 'Deacon',\n",
       " 'Doobert',\n",
       " 'Divine Doggo',\n",
       " 'Rizzo',\n",
       " 'Grizzwald',\n",
       " 'Petrick',\n",
       " 'Blu',\n",
       " 'Baloo',\n",
       " 'Chuckles',\n",
       " 'Adele',\n",
       " 'Luther',\n",
       " 'Finnegus',\n",
       " 'Aqua',\n",
       " 'Roscoe',\n",
       " 'Rueben',\n",
       " 'Rooney',\n",
       " 'Buckley',\n",
       " 'Lillie',\n",
       " 'Edmund',\n",
       " 'Ashleigh',\n",
       " 'Maggie_Lila',\n",
       " 'Ginger',\n",
       " 'Dido',\n",
       " 'Rinna',\n",
       " 'Godzilla pupper',\n",
       " 'Kona',\n",
       " 'Huck',\n",
       " 'Grady',\n",
       " 'Kawhi',\n",
       " 'Hector',\n",
       " 'Banjo',\n",
       " 'Mona',\n",
       " 'Bones',\n",
       " 'Birf',\n",
       " 'Hubertson',\n",
       " 'Brandy',\n",
       " 'Trip',\n",
       " 'Craig',\n",
       " 'Toffee',\n",
       " 'Blakely',\n",
       " 'Ollie Vue',\n",
       " 'Chelsea',\n",
       " 'Eve',\n",
       " 'Sansa',\n",
       " 'Jeffri',\n",
       " 'Millie',\n",
       " 'Willem',\n",
       " 'Obie',\n",
       " 'Schnozz',\n",
       " 'Jamesy',\n",
       " 'Ralphson',\n",
       " 'Zoe',\n",
       " 'Jed',\n",
       " 'Bubba',\n",
       " 'Sierra',\n",
       " 'Ambrose',\n",
       " 'Rolf',\n",
       " 'Rover',\n",
       " 'Jazz',\n",
       " 'Scott',\n",
       " 'Kaiya',\n",
       " 'Mollie',\n",
       " 'Superpup',\n",
       " 'Trevith',\n",
       " 'Pickles',\n",
       " 'Rufio',\n",
       " 'Pepper',\n",
       " 'Gidget',\n",
       " 'Gerbald',\n",
       " 'Snoopy',\n",
       " 'Diogi',\n",
       " 'Tonks',\n",
       " 'Shikha',\n",
       " 'Bayley',\n",
       " 'Stuart',\n",
       " 'Pablo',\n",
       " 'Betty']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have some new non names, we are going to set them as None,\n",
    "# because these good dogs have no names.\n",
    "non_names = []\n",
    "pattern_4 = r'^[a-z].*'\n",
    "for name in master_copy['dog_names']:\n",
    "    if re.search(pattern_4, name):\n",
    "        master_copy.loc[master_copy['dog_names'] == name, ['dog_names']] = 'None'\n",
    "        non_names.append(re.search(pattern_4, name).group())\n",
    "\n",
    "# printing unique names form dog_names\n",
    "master_copy['dog_names'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
