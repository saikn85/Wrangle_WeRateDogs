{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "## Dataset - WeRateDogs&trade; Twitter Archive\n",
    "\n",
    "***By: Kartik Nanduri***<br>\n",
    "**Date: 1st Dec, 2018.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests as req"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***<span style=\"color: red\">Important! uncomment the following files to run the book with out errors.</span>***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the folder structure.\n",
    "#os.rename('dataset/twitter-archive-enhanced.csv', 'twitter-archive-enhanced.csv')\n",
    "#import shutil\n",
    "#shutil.rmtree('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***<span style=\"color: green\">Important! once done, please recomment.</span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [x] **The file given at hand `twitter-archive-enhanced.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the requried files for this project are in the list files_list\n",
    "files_list = ['twitter-archive-enhanced.csv', 'image-predictions.tsv', 'tweet_json.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the twitter archive file\n",
    "archive = pd.read_csv(files_list[0])\n",
    "\n",
    "# taking at random entries for the archive file\n",
    "archive.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [x] **Fetching the data from url and saving it to local drive - `image-predictions.tsv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file from internet using the requests library\n",
    "url = \"https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5bf60c69_image-predictions-3/image-predictions-3.tsv\"\n",
    "res = req.get(url)\n",
    "\n",
    "with open(files_list[1], mode = \"wb\") as op_file:\n",
    "    op_file.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if fetched the data right way\n",
    "img_pre_test = pd.read_csv(files_list[1], delimiter = \"\\t\", encoding = 'utf-8')\n",
    "img_pre_test.sample(2)\n",
    "\n",
    "# we did it the right way, Yay! it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [x] **Getting data from Twitter&trade;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries for accessing Twitter via API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up all the necessary placeholders for API\n",
    "consumer_key = 'xxx.xxx.xxx.xxx'\n",
    "consumer_secret = 'xxx.xxx.xxx.xxx'\n",
    "access_token = 'xxx.xxx.xxx.xxx'\n",
    "access_secret = 'xxx.xxx.xxx.xxx'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth_handler = auth,\n",
    "                 parser = tweepy.parsers.JSONParser(),\n",
    "                 wait_on_rate_limit = True, \n",
    "                 wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save(ids, api_ins, one_id = None):\n",
    "    '''\n",
    "    This function will fetch data with associated id in ids list\n",
    "    ids (List Object): a list all tweets\n",
    "    api_ins (Tweepy Object): api object instance, will be used to query twitter for data\n",
    "    one_id (int): use when you want to query only for one tweet\n",
    "    failed_ids (List Object): a list will be retured so that, this fuction can be called once again on those ids\n",
    "    '''\n",
    "    new_file_name = ''; failed_ids = []; tweet_df = []\n",
    "    \n",
    "    # checking if file exists\n",
    "    if os.path.exists(files_list[2]):\n",
    "        temp = [s for s in os.listdir() if \"tweet_json\" in s]\n",
    "        new_file_name = files_list[2].split('.')[0] + \"_\" + str(len(temp)) + \".txt\"\n",
    "    else:\n",
    "        new_file_name = files_list[2]\n",
    "    \n",
    "    # querying a list of ids\n",
    "    if one_id == None:\n",
    "        with open(new_file_name, mode = 'w') as outfile:\n",
    "            for one_id in ids:\n",
    "                try:\n",
    "                    content = api_ins.get_status(one_id, tweet_mode='extended')\n",
    "                    json.dump(content, outfile)\n",
    "                    outfile.write('\\n')\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "                    failed_ids.append(one_id)\n",
    "    \n",
    "    # querying a single id\n",
    "    else:\n",
    "        try:\n",
    "            content = api_ins.get_status(one_id, include_entities = True)\n",
    "            favorites = content['favorite_count']\n",
    "            retweets = content['retweet_count']\n",
    "            \n",
    "            tweet_df.append({'tweet_id': int(one_id),\n",
    "                        'favorites': int(favorites),\n",
    "                        'retweets': int(retweets)})\n",
    "            \n",
    "            return tweet_df\n",
    "                           \n",
    "        except Exception as e:\n",
    "            print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "            failed_ids.append(one_id)\n",
    "\n",
    "    return failed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the list of ids to the fuction fetch_and_save()\n",
    "tweet_ids = archive['tweet_id'].tolist()\n",
    "len(tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching data\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "errors = fetch_and_save(tweet_ids, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the failed ids into one master list\n",
    "print(\"Total failed request are: {}. \\n\".format(len(errors)))\n",
    "\n",
    "# ids that failed and the ones that passed\n",
    "indi_fail = []; success = []\n",
    "\n",
    "#for each failed id, lets try to fetch status individually.\n",
    "for error in errors:\n",
    "    temp = fetch_and_save(ids = None, api_ins = api, one_id = error)\n",
    "    indi_fail.append(temp[0])\n",
    "\n",
    "# removing empty elements from list\n",
    "success = [x for x in indi_fail if not isinstance(x, (int))]\n",
    "indi_fail = [x for x in indi_fail if isinstance(x, (int))]\n",
    "\n",
    "# checking if there is change\n",
    "print(\"\\nWe were able to retrieve {} records, others failed.\".format(len(errors) - len(indi_fail)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the results of success\n",
    "success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [x] **Okay, let's read `retweets` and `favourite count` from `tweet_json.txt`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading tweet_json.txt\n",
    "tweets = pd.read_json(files_list[2], lines = True, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's select only the following columns retweet_count, favorite_count, id\n",
    "tweets = tweets[['id', 'favorite_count', 'retweet_count']]\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns\n",
    "tweets.rename(columns={'id': 'tweet_id',\n",
    "                       'favorite_count': 'favorites',\n",
    "                       'retweet_count': 'retweets'},\n",
    "              inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concating the dataframes into one\n",
    "success = pd.DataFrame(success, columns = ['tweet_id',\n",
    "                                           'favorites',\n",
    "                                           'retweets'])\n",
    "tweet_master = pd.concat([tweets, success],\n",
    "                         ignore_index = True,\n",
    "                         sort = True,)\n",
    "tweet_master.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy of the archive\n",
    "archive_copy = archive.copy()\n",
    "\n",
    "# marking the id null that we failed to retrieve in archive\n",
    "for a_id in indi_fail:\n",
    "    archive_copy.loc[archive_copy['tweet_id'] == a_id, ['tweet_id']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a checking if we did it right\n",
    "len(archive_copy[archive_copy['tweet_id'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the new file to our files list\n",
    "files_list.append('archive_copy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the contents of tweet_master to a file\n",
    "import numpy as np\n",
    "tweet_master['tweet_id'] = tweet_master['tweet_id'].astype(np.int64)\n",
    "tweet_master.to_csv('tweet_master.csv', index = False)\n",
    "\n",
    "# saving the updated version of our archived-enhanced.csv\n",
    "archive_copy['tweet_id'] = archive_copy['tweet_id'].astype(np.int64)\n",
    "archive_copy.to_csv(files_list[3], index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [x] **Last thing to do is to tidy up our folder, let's get going.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'act_report.ipynb',\n",
       " 'dataset',\n",
       " 'error.png',\n",
       " 'New Text Document.txt',\n",
       " 'README.md',\n",
       " 'wrangle_act.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moving all data files under one folder - dataset\n",
    "# removing the temporary files, that acted as placeholders\n",
    "\n",
    "# creating the folder\n",
    "folder = 'dataset'\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "# we know that our master datasets for this project are\n",
    "# 1. twitter-archive-enhanced.csv\n",
    "# 2. image-predictions.tsv\n",
    "# 3. tweet_json.txt\n",
    "# 4. tweet_master.csv\n",
    "# let us move these files\n",
    "\n",
    "# updating our files_list\n",
    "files_list.append('tweet_master.csv')\n",
    "\n",
    "# moving only required files\n",
    "for file in files_list:\n",
    "    if os.path.exists(file):\n",
    "        os.rename(file, folder+'/'+file)\n",
    "    \n",
    "# lisitng the current directory\n",
    "os.listdir()\n",
    "\n",
    "# clean and neat, lets get with assessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming files_list\n",
    "for i in range(len(files_list)):\n",
    "    files_list[i] = folder + '/'+ files_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Gathering\n",
    "\n",
    "- We know, that gathering is a the first step in wrangling.\n",
    "- We were successful in gathering from three different sources with different techniques:\n",
    "    - Data given at hand.\n",
    "    - Fetch from flat file stored on a server.\n",
    "    - From API.\n",
    "\n",
    "- There a total of 14 missing data points, tried a different ways for retrieving them, using the API as well as `twurl` of the `Ruby` package, but they were not to be found, as stated below in the highlighted section.\n",
    "\n",
    "***<span style=\"color: ##6c6cff\">So let's start with assessing the data.</span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![error](error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/twitter-archive-enhanced.csv',\n",
       " 'dataset/image-predictions.tsv',\n",
       " 'dataset/tweet_json.txt',\n",
       " 'dataset/archive_copy.csv',\n",
       " 'dataset/tweet_master.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load up dataset, and starting assessing them.\n",
    "archive =  pd.read_csv(files_list[-2], encoding = 'utf-8')\n",
    "img_pre = pd.read_csv(files_list[1], sep = '\\t', encoding = 'utf-8')\n",
    "tweet_master = pd.read_csv(files_list[-1], encoding =  'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues to sort!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing out archive - visual assessment\n",
    "archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic Assessment 1 - Information\n",
    "archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmtic Assessment 2 - Describe\n",
    "archive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates - tweet_ids\n",
    "# these are the 0's for which the api failed to retrive data\n",
    "sum(archive.tweet_id.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkin if we have more than one class of dogs assigned to dog\n",
    "# the following are the only combinations that are present in the dataset\n",
    "cond_1 = (archive['doggo'] == 'doggo') & (archive['floofer'] == 'floofer')\n",
    "cond_2 = (archive['doggo'] == 'doggo') & (archive['pupper'] == 'pupper')\n",
    "cond_3 = (archive['doggo'] == 'doggo') & (archive['puppo'] == 'puppo')\n",
    "\n",
    "# printing these entries\n",
    "archive[cond_1 | cond_2 | cond_3][['tweet_id', 'text', 'doggo', 'floofer', 'pupper', 'puppo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(archive[cond_1 | cond_2 | cond_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`twitter-archive-enhanced.csv`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- `rating_numerator` : has values such as 1, 3.. e.t.c - **Data Quality Dimension - `Consistency`**.\n",
    "- `rating_denominator` : have values, less than 10, for example, the tweet_id - 666287406224695296 has the number 2 as its value - **Data Quality Dimension - `Consistency`**. \n",
    "- We see that, Articles - `a`, `an`, `the` have been used to name dogs, as well as words such as `such`, `quite` - **Data Quality Dimension - `Validity`**.\n",
    "- There are instances where the names of dogs are in lowercases - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- `rating_numerator` : has a maximum value of 1766 - **Data Quality Dimension - `Consistency`**. \n",
    "- `rating_denominator` : has a maximum value of 170 - **Data Quality Dimension - `Consistency`**.\n",
    "- All in all, this dataset appears to be clean, except for `expanded_url` - we have about 59 instances missing - **Data Quality Dimension - `Completeness`**.\n",
    "- We can see that there are more than one class assigned to tweets, analyze and assign proper dog class so that melting is easy - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- we can see that, there are four classes of dogs `doggo`, `floofer`, `puppo`, `pupper`; these should a part of one unit - `dog_class` - **Data Quality Dimension - `Consistency`**.\n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- `in_reply_to_status_id`, `retweeted_status_id`, `retweeted_status_user_id`, `in_reply_to_user_id` of type float64 must be converted into int - **Data Quality Dimension - `Validity`**.\n",
    "- `timestamp`, `retweeted_status_timestamp` of type object must be converted into datatime - **Data Quality Dimension - `Validity`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing img_predictions dataset\n",
    "img_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic Assessment - Information\n",
    "img_pre.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates\n",
    "img_pre[img_pre['jpg_url'].duplicated(keep = False)].sort_values(by = 'jpg_url')[['tweet_id', 'jpg_url']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **`image-predictions.tsv`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- We have few dog breeds that are represented in lowercase.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- We have about 281 images on a whole, that are missing with respect to our `twitter-archive-enhanced.csv` file - **Data Quality Dimension - `Completeness`**.\n",
    "- We can see that, we have about `66` duplicates **OR** a pair of tweets are pointing to same *`jpg_url`* - **Data Quality Dimension - `Accuracy`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- None. \n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing tweet_master dataset\n",
    "tweet_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_master.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **`tweet_master.txt`** table\n",
    "\n",
    "***1 Content Issues:***\n",
    "\n",
    "**1.1 Visual Assessment:**\n",
    "- None.\n",
    "\n",
    "**1.2 Programmatic Assessment:**\n",
    "- We have about 14 missing records - **Data Quality Dimension - `Completeness`**.\n",
    "\n",
    "***2 Structural Issues:***\n",
    "\n",
    "**2.1 Visual Assessment:**\n",
    "- None.\n",
    "\n",
    "**2.2 Programmatic Assessment:**\n",
    "- None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Assessing\n",
    "\n",
    "- Completed the second step.\n",
    "- The following are the insights:\n",
    "    - from `twitter-archive-enhanced.csv` datset, the rating_numerator and denominator need to be fixed.\n",
    "    - the dataset also represents row values as columns, which needs to be fixed.\n",
    "    - the dataset also has structural issues such as wrong datatype assigned to a column.\n",
    "    - from `images-preductions.tsv` dataset, there is consistency issue with naming dog breeds.\n",
    "    - the dataset isn't complete when compared to `twitter-archive-enhanced.csv`, we have about 281 missing tweets.\n",
    "    - Also we have `jpg_urls'` that are pointing to a pair of tweets.\n",
    "    - `tweet_master.txt` dataset has about 14 missing records.\n",
    "    - the dataset alone hold the information about retweets and favourites - bad form of schema normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Important!, before we get to cleaning, let's drop rows from image-predictions, that are false in dog_1,_2 and _3, as they are not related to our dataset.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select those rows that are either true or false and not all false\n",
    "img_pre = img_pre[~((img_pre.p1_dog == False) & (img_pre.p2_dog == False) & (img_pre.p3_dog == False))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asserting the lenght to be 0\n",
    "assert len(img_pre[(img_pre.p1_dog == False) & (img_pre.p2_dog == False) & (img_pre.p3_dog == False)]) == 0, \"Check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2356 entries, 0 to 2355\n",
      "Data columns (total 30 columns):\n",
      "tweet_id                      2356 non-null int64\n",
      "in_reply_to_status_id         78 non-null float64\n",
      "in_reply_to_user_id           78 non-null float64\n",
      "timestamp                     2356 non-null object\n",
      "source                        2356 non-null object\n",
      "text                          2356 non-null object\n",
      "retweeted_status_id           181 non-null float64\n",
      "retweeted_status_user_id      181 non-null float64\n",
      "retweeted_status_timestamp    181 non-null object\n",
      "expanded_urls                 2297 non-null object\n",
      "rating_numerator              2356 non-null int64\n",
      "rating_denominator            2356 non-null int64\n",
      "name                          2356 non-null object\n",
      "doggo                         2356 non-null object\n",
      "floofer                       2356 non-null object\n",
      "pupper                        2356 non-null object\n",
      "puppo                         2356 non-null object\n",
      "jpg_url                       1745 non-null object\n",
      "img_num                       1745 non-null float64\n",
      "p1                            1745 non-null object\n",
      "p1_conf                       1745 non-null float64\n",
      "p1_dog                        1745 non-null object\n",
      "p2                            1745 non-null object\n",
      "p2_conf                       1745 non-null float64\n",
      "p2_dog                        1745 non-null object\n",
      "p3                            1745 non-null object\n",
      "p3_conf                       1745 non-null float64\n",
      "p3_dog                        1745 non-null object\n",
      "favorites                     2342 non-null float64\n",
      "retweets                      2342 non-null float64\n",
      "dtypes: float64(10), int64(3), object(17)\n",
      "memory usage: 570.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# the master dataset\n",
    "master_set = archive.merge(img_pre, how = 'left', on = ['tweet_id'])\n",
    "master_set = master_set.merge(tweet_master, how = 'left', on = ['tweet_id'])\n",
    "files_list.append('dataset/master_set_raw.csv')\n",
    "master_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file to local disk.\n",
    "master_set.to_csv(files_list[-1], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the master set\n",
    "master_set = pd.read_csv(files_list[-1], encoding = 'utf-8')\n",
    "master_copy = master_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues to Clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Basic cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Assign proper class for the above 14 tweets before melting.\n",
    "- Delete *retweets* with *any duplicates* and get rid of *tweets with **no** images*.\n",
    "- Once done, drop the following columns:\n",
    "    1. `retweeted_status_id`\n",
    "    2. `retweeted_status_user_id`\n",
    "    3. `retweeted_status_timestamp`\n",
    "    4. `in_reply_to_status_id`\n",
    "    5. `in_reply_to_user_id`\n",
    "    \n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tweet_id  \\\n",
      "191   855851453814013952   \n",
      "200   854010172552949760   \n",
      "460   817777686764523521   \n",
      "531   808106460588765185   \n",
      "565   802265048156610565   \n",
      "575   801115127852503040   \n",
      "705   785639753186217984   \n",
      "733   781308096455073793   \n",
      "778   775898661951791106   \n",
      "822   770093767776997377   \n",
      "889   759793422261743616   \n",
      "956   751583847268179968   \n",
      "1063  741067306818797568   \n",
      "1113  733109485275860992   \n",
      "\n",
      "                                                                                                                                                                      text  \n",
      "191   Here's a puppo participating in the #ScienceMarch. Cleverly disguising her own doggo agenda. 13/10 would keep the planet habitable for https://t.co/cMhq16isel        \n",
      "200   At first I thought this was a shy doggo, but it's actually a Rare Canadian Floofer Owl. Amateurs would confuse the two. 11/10 only send dogs https://t.co/TXdT3tmuYk  \n",
      "460   This is Dido. She's playing the lead role in \"Pupper Stops to Catch Snow Before Resuming Shadow Box with Dried Apple.\" 13/10 (IG: didodoggo) https://t.co/m7isZrOBX7  \n",
      "531   Here we have Burke (pupper) and Dexter (doggo). Pupper wants to be exactly like doggo. Both 12/10 would pet at same time https://t.co/ANBpEYHaho                      \n",
      "565   Like doggo, like pupper version 2. Both 11/10 https://t.co/9IxWAXFqze                                                                                                 \n",
      "575   This is Bones. He's being haunted by another doggo of roughly the same size. 12/10 deep breaths pupper everything's fine https://t.co/55Dqe0SJNj                      \n",
      "705   This is Pinot. He's a sophisticated doggo. You can tell by the hat. Also pointier than your average pupper. Still 10/10 would pet cautiously https://t.co/f2wmLZTPHd  \n",
      "733   Pupper butt 1, Doggo 0. Both 12/10 https://t.co/WQvcPEpH2u                                                                                                            \n",
      "778   RT @dog_rates: Like father (doggo), like son (pupper). Both 12/10 https://t.co/pG2inLaOda                                                                             \n",
      "822   RT @dog_rates: This is just downright precious af. 12/10 for both pupper and doggo https://t.co/o5J479bZUC                                                            \n",
      "889   Meet Maggie &amp; Lila. Maggie is the doggo, Lila is the pupper. They are sisters. Both 12/10 would pet at the same time https://t.co/MYwR4DQKll                      \n",
      "956   Please stop sending it pictures that don't even have a doggo or pupper in them. Churlish af. 5/10 neat couch tho https://t.co/u2c9c7qSg8                              \n",
      "1063  This is just downright precious af. 12/10 for both pupper and doggo https://t.co/o5J479bZUC                                                                           \n",
      "1113  Like father (doggo), like son (pupper). Both 12/10 https://t.co/pG2inLaOda                                                                                            \n"
     ]
    }
   ],
   "source": [
    "# setting column width to -1\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "cond_1 = (master_copy['doggo'] == 'doggo') & (master_copy['floofer'] == 'floofer')\n",
    "cond_2 = (master_copy['doggo'] == 'doggo') & (master_copy['pupper'] == 'pupper')\n",
    "cond_3 = (master_copy['doggo'] == 'doggo') & (master_copy['puppo'] == 'puppo')\n",
    "print(master_copy[cond_1 | cond_2 | cond_3][['tweet_id', 'text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Assign the following:***\n",
    "1. 855851453814013952: puppo\n",
    "2. 854010172552949760: floofer\n",
    "3. 817777686764523521: pupper\n",
    "4. 808106460588765185: pupper\n",
    "5. 802265048156610565: pupper\n",
    "6. 801115127852503040: pupper\n",
    "7. 785639753186217984: pupper\n",
    "8. 781308096455073793: pupper\n",
    "9. 775898661951791106: pupper\n",
    "10. 770093767776997377: pupper\n",
    "11. 759793422261743616: pupper\n",
    "12. 751583847268179968: doggo\n",
    "13. 741067306818797568: doggo\n",
    "14. 733109485275860992: doggo\n",
    "\n",
    "**<span style=\"color: green\">I like puppies, so for most of the entries it is pupper!</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning values.\n",
    "master_copy.loc[master_copy['tweet_id'] == 855851453814013952, ['doggo', 'floofer', 'pupper']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 854010172552949760, ['doggo', 'pupper', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 817777686764523521, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 808106460588765185, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 802265048156610565, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 801115127852503040, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 785639753186217984, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 781308096455073793, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 775898661951791106, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 770093767776997377, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 759793422261743616, ['doggo', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 751583847268179968, ['pupper', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 741067306818797568, ['pupper', 'floofer', 'puppo']] = 'None'\n",
    "master_copy.loc[master_copy['tweet_id'] == 733109485275860992, ['pupper', 'floofer', 'puppo']] = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>855851453814013952</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>puppo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>854010172552949760</td>\n",
       "      <td>None</td>\n",
       "      <td>floofer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>817777686764523521</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>808106460588765185</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>802265048156610565</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>801115127852503040</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>785639753186217984</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>781308096455073793</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>775898661951791106</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>770093767776997377</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>759793422261743616</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>751583847268179968</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>741067306818797568</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>733109485275860992</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  doggo  floofer  pupper  puppo\n",
       "191   855851453814013952   None     None    None  puppo\n",
       "200   854010172552949760   None  floofer    None   None\n",
       "460   817777686764523521   None     None  pupper   None\n",
       "531   808106460588765185   None     None  pupper   None\n",
       "565   802265048156610565   None     None  pupper   None\n",
       "575   801115127852503040   None     None  pupper   None\n",
       "705   785639753186217984   None     None  pupper   None\n",
       "733   781308096455073793   None     None  pupper   None\n",
       "778   775898661951791106   None     None  pupper   None\n",
       "822   770093767776997377   None     None  pupper   None\n",
       "889   759793422261743616   None     None  pupper   None\n",
       "956   751583847268179968  doggo     None    None   None\n",
       "1063  741067306818797568  doggo     None    None   None\n",
       "1113  733109485275860992  doggo     None    None   None"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all values have been properly assigned\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "master_copy[cond_1 | cond_2 | cond_3][['tweet_id', 'doggo', 'floofer', 'pupper', 'puppo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting those, tweets that have no retweets\n",
    "master_copy = master_copy[pd.isnull(master_copy['retweeted_status_id'])]\n",
    "\n",
    "# deleting duplicates if any\n",
    "master_copy = master_copy.drop_duplicates()\n",
    "\n",
    "# deleting those tweets with no images.\n",
    "master_copy = master_copy.dropna(subset = ['jpg_url'])\n",
    "\n",
    "# reseting index\n",
    "master_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# droping columns\n",
    "master_copy = master_copy.drop(labels = ['retweeted_status_id',\n",
    "                                         'retweeted_status_user_id',\n",
    "                                         'retweeted_status_timestamp',\n",
    "                                         'in_reply_to_status_id', \n",
    "                                         'in_reply_to_user_id'],\n",
    "                               axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1685, 25)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after droping the columns, we should have about 25 dimensions/columns\n",
    "master_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Condense wide-format to long-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Condense `doggo`, `floofer`, `pupper`, `puppo` as `dog_class`.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Doggo: 57\n",
      "Count of Floofer: 8\n",
      "Count of Pupper: 173\n",
      "Count of Puppo: 22\n"
     ]
    }
   ],
   "source": [
    "# to make sure that we have \n",
    "doggo = master_copy.doggo.value_counts()['doggo']\n",
    "floofer = master_copy.floofer.value_counts()['floofer']\n",
    "pupper = master_copy.pupper.value_counts()['pupper']\n",
    "puppo = master_copy.puppo.value_counts()['puppo']\n",
    "\n",
    "# printing count of each class\n",
    "print(\"Count of Doggo: {}\\nCount of Floofer: {}\\nCount of Pupper: {}\\nCount of Puppo: {}\".format(doggo,\n",
    "                                                                                                 floofer,\n",
    "                                                                                                 pupper,\n",
    "                                                                                                 puppo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the columns that are to be melted\n",
    "columns_to_melt = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "columns_to_stay = [x for x in master_copy.columns.tolist() if x not in columns_to_melt]\n",
    "\n",
    "# melting the the columns into values\n",
    "master_copy = pd.melt(master_copy, id_vars = columns_to_stay, value_vars = columns_to_melt, \n",
    "                         var_name = 'stages', value_name = 'dog_class')\n",
    "\n",
    "# Delete column 'stages'\n",
    "master_copy = master_copy.drop('stages', 1)\n",
    "\n",
    "# dropping duplicates\n",
    "master_copy = master_copy.sort_values('dog_class').drop_duplicates('tweet_id', keep = 'last')\n",
    "master_copy.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assert\n",
    "assert doggo == master_copy.dog_class.value_counts()['doggo'], \"Some entries are missing\"\n",
    "assert floofer == master_copy.dog_class.value_counts()['floofer'], \"Some entries are missing\"\n",
    "assert pupper == master_copy.dog_class.value_counts()['pupper'], \"Some entries are missing\"\n",
    "assert puppo == master_copy.dog_class.value_counts()['puppo'], \"Some entries are missing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fix all inaccurate data.\n",
    "\n",
    "#### Define\n",
    "- fix names of dogs.\n",
    "- fix ratings.\n",
    "- check source column.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking source column\n",
    "master_copy.source.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red\">Okay! only three values, a categorical variable</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# assiging unique values to source.\n",
    "master_copy['source'] = master_copy['source'].apply(lambda x: re.findall(r'>(.*)<', x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>823322678127919110</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>This is Wyatt. He's got the fastest paws in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>678764513869611008</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Meet Wilson. He got caught humping the futon. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>841077006473256960</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>This is Dawn. She's just checking pup on you. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>831262627380748289</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>This is Tucker. He's feeling h*ckin festive an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>666049248165822465</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Here we have a 1949 1st generation vulpix. Enj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id              source  \\\n",
       "1023  823322678127919110  Twitter for iPhone   \n",
       "701   678764513869611008  Twitter for iPhone   \n",
       "936   841077006473256960  Twitter for iPhone   \n",
       "1059  831262627380748289  Twitter for iPhone   \n",
       "91    666049248165822465  Twitter for iPhone   \n",
       "\n",
       "                                                   text  \n",
       "1023  This is Wyatt. He's got the fastest paws in th...  \n",
       "701   Meet Wilson. He got caught humping the futon. ...  \n",
       "936   This is Dawn. She's just checking pup on you. ...  \n",
       "1059  This is Tucker. He's feeling h*ckin festive an...  \n",
       "91    Here we have a 1949 1st generation vulpix. Enj...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a look at sample of 5 rows\n",
    "master_copy.sample(5)[['tweet_id', 'source', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing names\n",
    "non_names = master_copy.name.str.islower()\n",
    "non_names = list(set(master_copy[non_names]['name'].tolist()))\n",
    "flag = master_copy.name.str.len() == 1 & master_copy.name.str.isupper()\n",
    "non_names.append(master_copy[flag][['tweet_id', 'name']]['name'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all garbage names with none, once done, we'll use the text field to extract names\n",
    "for name in master_copy.name:\n",
    "    if name in non_names:\n",
    "        master_copy.loc[master_copy['name'] == name, ['name']] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are any non_names after the operation\n",
    "assert len(master_copy[(master_copy.name.str.islower()) & (flag)]) == 0, \"Check code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The following are patterns observed in `text` field, we shall use the :***\n",
    "- This is [name] ..\n",
    "- Meet [name] ..\n",
    "- Say hello to [name] ..\n",
    "- .. named [name] ..\n",
    "- .. name is [name] ..\n",
    "\n",
    "We will treat those cases to get the names from the text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting names using regular expression.\n",
    "dog_names = []\n",
    "\n",
    "# assigning patterns\n",
    "pattern_1 = r'(T|t)his\\sis\\s([^.|,]*)'\n",
    "pattern_2 = r'Meet\\s([^.|,]*)'\n",
    "pattern_3 = r'Say\\shello\\sto\\s([^.|,]*)'\n",
    "pattern_4 = r'name\\sis\\s([^.|,]*)'\n",
    "\n",
    "# looping through text and extracting names\n",
    "for text in master_copy['text']:\n",
    "    # Start with 'This is '\n",
    "    if re.search(pattern_1, text):\n",
    "        # if our match has alternate name\n",
    "        if \"(\" in re.search(pattern_1, text).group(2):\n",
    "            dog_names.append(re.search(pattern_1, text).group(2).split()[0])\n",
    "        # if our match has AKA in it\n",
    "        elif \"AKA\" in re.search(pattern_1, text).group(2):\n",
    "            dog_names.append(re.search(pattern_1, text).group(2).split()[0])\n",
    "        # if our name has two dogs\n",
    "        elif '&amp;' in re.search(pattern_1, text).group(2):\n",
    "            temp = re.search(pattern_1, text).group(2).split()\n",
    "            if len(temp) == 1:\n",
    "                dog_names.append(temp[0])\n",
    "            elif len(temp) == 3:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-1])\n",
    "            else:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-2])\n",
    "        elif 'named' in re.search(pattern_1, text).group(2):\n",
    "            temp = re.search(pattern_1, text).group(2).split()\n",
    "            dog_names.append(temp[-1])\n",
    "        # just appending the name\n",
    "        else:\n",
    "            dog_names.append(re.search(pattern_1, text).group(2))\n",
    "    \n",
    "    # Start with 'Meet '\n",
    "    elif re.search(pattern_2, text):\n",
    "        # if our name has two dogs\n",
    "        if '&amp;' in re.search(pattern_2, text).group(1):\n",
    "            temp = re.search(pattern_2, text).group(1).split()\n",
    "            if len(temp) == 1:\n",
    "                dog_names.append(temp[0])\n",
    "            elif len(temp) == 3:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-1])\n",
    "            else:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-2])\n",
    "        # if our name has alternatives\n",
    "        elif '(' in re.search(pattern_2, text).group(1):\n",
    "            dog_names.append(re.search(pattern_2, text).group(1).split()[0])\n",
    "        # just appending the name\n",
    "        else:\n",
    "            dog_names.append(re.search(pattern_2, text).group(1))\n",
    "    \n",
    "    # Start with 'Say hello to '\n",
    "    elif re.search(pattern_3, text):\n",
    "        # if our match has alternate name\n",
    "        if '(' in re.search(pattern_3, text).group(1):\n",
    "            dog_names.append(re.search(pattern_3, text).group(1).split()[0])\n",
    "        # if our name has two dogs\n",
    "        elif '&amp;' in re.search(pattern_3, text).group(1):\n",
    "            temp = re.search(pattern_3, text).group(1).split()\n",
    "            if len(temp) == 1:\n",
    "                dog_names.append(temp[0])\n",
    "            elif len(temp) == 3:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-1])\n",
    "            else:\n",
    "                dog_names.append(temp[0]+\"|\"+temp[-2])\n",
    "        else:\n",
    "            dog_names.append(re.search(pattern_3, text).group(1))    \n",
    "    \n",
    "    # contains 'name is'\n",
    "    elif re.search(pattern_4, text):\n",
    "        if len(re.search(pattern_4, text).group(1).split()) == 1:\n",
    "            dog_names.append(re.search(pattern_4, text).group(1))\n",
    "        else:\n",
    "            temp = re.search(pattern_4, text).group(1).split()\n",
    "            dog_names.append(temp[0])\n",
    "        \n",
    "    # No name specified or other style\n",
    "    else:\n",
    "        dog_names.append('None')\n",
    "\n",
    "# adding this new set of names to our master_copy\n",
    "master_copy['dog_names'] = dog_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new non names.\n",
    "non_names = []\n",
    "pattern_4 = r'^[a-z].*'\n",
    "for name in master_copy['dog_names']:\n",
    "    if re.search(pattern_4, name):\n",
    "        master_copy.loc[master_copy['dog_names'] == name, ['dog_names']] = 'None'\n",
    "        non_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog_names with and to be replaced with |\n",
    "for name in master_copy['dog_names']:\n",
    "    master_copy['dog_names'] = master_copy['dog_names'].str.replace(pat = r'\\sand\\s', repl = \"|\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>dog_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>667495797102141441</td>\n",
       "      <td>This is Philippe from Soviet Russia. Commanding leader. Misplaced other boot. Hung flag himself. 9/10 charismatic af https://t.co/5NhPV8E45i</td>\n",
       "      <td>Philippe from Soviet Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>693280720173801472</td>\n",
       "      <td>This is Sadie and her 2 pups Shebang &amp;amp; Ruffalo. Sadie says single parenting is challenging but rewarding. All 10/10 https://t.co/UzbhwXcLne</td>\n",
       "      <td>Sadie|&amp;amp;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id  \\\n",
       "23   667495797102141441   \n",
       "160  693280720173801472   \n",
       "\n",
       "                                                                                                                                                text  \\\n",
       "23   This is Philippe from Soviet Russia. Commanding leader. Misplaced other boot. Hung flag himself. 9/10 charismatic af https://t.co/5NhPV8E45i      \n",
       "160  This is Sadie and her 2 pups Shebang &amp; Ruffalo. Sadie says single parenting is challenging but rewarding. All 10/10 https://t.co/UzbhwXcLne   \n",
       "\n",
       "                       dog_names  \n",
       "23   Philippe from Soviet Russia  \n",
       "160  Sadie|&amp;                  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to replace two cells, with names 'Sadie|&amp;', 'Phillippe ...',\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "master_copy[(master_copy['dog_names'] == \"Philippe from Soviet Russia\") |\n",
    "            (master_copy['dog_names'] == \"Sadie|&amp;\")][['tweet_id', 'text', 'dog_names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting them to correct ones\n",
    "# the following tweet_id we different from our regexs\n",
    "master_copy.loc[master_copy['dog_names'] == \"Philippe from Soviet Russia\", ['dog_names']] = 'Phillippe'\n",
    "master_copy.loc[master_copy['dog_names'] == \"Sadie|&amp;\", ['dog_names']] = 'Sadie|Shebang|Ruffalo'\n",
    "master_copy.loc[master_copy['tweet_id'] == 667509364010450944, ['dog_names']] = 'Tickles'\n",
    "master_copy.loc[master_copy['tweet_id'] == 667546741521195010, ['dog_names']] = 'George'\n",
    "master_copy.loc[master_copy['tweet_id'] == 667073648344346624, ['dog_names']] = 'Dave'\n",
    "master_copy.loc[master_copy['tweet_id'] == 667177989038297088, ['dog_names']] = 'Daryl'\n",
    "master_copy.loc[master_copy['tweet_id'] == 666835007768551424, ['dog_names']] = 'Cupit|Prencer'\n",
    "master_copy.loc[master_copy['tweet_id'] == 668221241640230912, ['dog_names']] = 'Bo|Smittens'\n",
    "master_copy.loc[master_copy['tweet_id'] == 668268907921326080, ['dog_names']] = 'Guss'\n",
    "master_copy.loc[master_copy['tweet_id'] == 666058600524156928, ['dog_names']] = 'Paul Rand'\n",
    "master_copy.loc[master_copy['tweet_id'] == 692142790915014657, ['dog_names']] = 'Teddy'\n",
    "master_copy.loc[master_copy['tweet_id'] == 684097758874210310, ['dog_names']] = 'Lupe'\n",
    "master_copy.loc[master_copy['tweet_id'] == 709198395643068416, ['dog_names']] = 'Cletus|Jerome|Alejandro|Burp|Titson'\n",
    "master_copy.loc[master_copy['tweet_id'] == 671743150407421952, ['dog_names']] = 'Jacob'\n",
    "master_copy.loc[master_copy['tweet_id'] == 669037058363662336, ['dog_names']] = 'Pancho|Peaches'\n",
    "master_copy.loc[master_copy['tweet_id'] == 669363888236994561, ['dog_names']] = 'Zeus'\n",
    "master_copy.loc[master_copy['tweet_id'] == 813217897535406080, ['dog_names']] = 'Atlas'\n",
    "master_copy.loc[master_copy['tweet_id'] == 856526610513747968, ['dog_names']] = 'Charlie'\n",
    "master_copy.loc[master_copy['tweet_id'] == 861288531465048066, ['dog_names']] = 'Boomer'\n",
    "master_copy.loc[master_copy['tweet_id'] == 863079547188785154, ['dog_names']] = 'Pipsy'\n",
    "master_copy.loc[master_copy['tweet_id'] == 844979544864018432, ['dog_names']] = 'Toby'\n",
    "master_copy.loc[master_copy['tweet_id'] == 836001077879255040, ['dog_names']] = 'Atlas'\n",
    "master_copy.loc[master_copy['tweet_id'] == 758041019896193024, ['dog_names']] = 'Teagan'\n",
    "master_copy.loc[master_copy['tweet_id'] == 765395769549590528, ['dog_names']] = 'Zoey'\n",
    "master_copy.loc[master_copy['tweet_id'] == 778408200802557953, ['dog_names']] = 'Loki'\n",
    "master_copy.loc[master_copy['tweet_id'] == 770069151037685760, ['dog_names']] = 'Carbon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweet_id',\n",
       " 'timestamp',\n",
       " 'source',\n",
       " 'text',\n",
       " 'expanded_urls',\n",
       " 'rating_numerator',\n",
       " 'rating_denominator',\n",
       " 'jpg_url',\n",
       " 'img_num',\n",
       " 'p1',\n",
       " 'p1_conf',\n",
       " 'p1_dog',\n",
       " 'p2',\n",
       " 'p2_conf',\n",
       " 'p2_dog',\n",
       " 'p3',\n",
       " 'p3_conf',\n",
       " 'p3_dog',\n",
       " 'favorites',\n",
       " 'retweets',\n",
       " 'dog_class',\n",
       " 'dog_names']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping column name\n",
    "master_copy = master_copy.drop(['name'], axis = 1)\n",
    "\n",
    "# printing columns in master_copy\n",
    "master_copy.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>dog_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>753655901052166144</td>\n",
       "      <td>\"The dogtor is in hahahaha no but seriously I'm very qualified and that tumor is definitely malignant\" 10/10 https://t.co/ULqThwWmLg</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>716080869887381504</td>\n",
       "      <td>Here's a super majestic doggo and a sunset 11/10 https://t.co/UACnoyi8zu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>822244816520155136</td>\n",
       "      <td>We only rate dogs. Please don't send pics of men capturing low level clouds. Thank you... 11/10 https://t.co/rLi83ZyCL5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>678396796259975168</td>\n",
       "      <td>These little fellas have opposite facial expressions. Both 12/10 https://t.co/LmThv0GWen</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>734787690684657664</td>\n",
       "      <td>This dog is more successful than I will ever be. 13/10 absolute legend https://t.co/BPoaHySYwA</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  \\\n",
       "1395  753655901052166144   \n",
       "1453  716080869887381504   \n",
       "1032  822244816520155136   \n",
       "693   678396796259975168   \n",
       "1227  734787690684657664   \n",
       "\n",
       "                                                                                                                                      text  \\\n",
       "1395  \"The dogtor is in hahahaha no but seriously I'm very qualified and that tumor is definitely malignant\" 10/10 https://t.co/ULqThwWmLg   \n",
       "1453  Here's a super majestic doggo and a sunset 11/10 https://t.co/UACnoyi8zu                                                               \n",
       "1032  We only rate dogs. Please don't send pics of men capturing low level clouds. Thank you... 11/10 https://t.co/rLi83ZyCL5                \n",
       "693   These little fellas have opposite facial expressions. Both 12/10 https://t.co/LmThv0GWen                                               \n",
       "1227  This dog is more successful than I will ever be. 13/10 absolute legend https://t.co/BPoaHySYwA                                         \n",
       "\n",
       "     dog_names  \n",
       "1395  None      \n",
       "1453  None      \n",
       "1032  None      \n",
       "693   None      \n",
       "1227  None      "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting a dog_names that are none.\n",
    "master_copy[master_copy['dog_names'] == 'None'].sample(5)[['tweet_id', 'text', 'dog_names']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Define\n",
    "- Let's get cleaning the ratings.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are aware that there are two ratings in the text columns, lets use the\n",
    "# regex to extract and replace the wrong ones.\n",
    "ratings = master_copy['text'].apply(lambda x: re.findall(r'(\\d+(\\.\\d+)|(\\d+))\\/(\\d+0)', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's scale number from 0 to 15\n",
    "def scale_rate(number, mini = 0, maxi = 15):\n",
    "    return (number - mini)/(maxi - mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# our temp variables\n",
    "rate_num = []\n",
    "rate_denom = []\n",
    "\n",
    "# let's loop over and assign values properly\n",
    "for rate in ratings:\n",
    "    # if our regex didn't return any value.\n",
    "    if len(rate) == 0:\n",
    "        rate_num.append(0)\n",
    "        rate_denom.append(0)\n",
    "    \n",
    "    # if regex's leght equals to one\n",
    "    elif len(rate) == 1:\n",
    "        \n",
    "        temp = float(rate[0][0])\n",
    "        \n",
    "        # if we the value falls in between  [30,100]\n",
    "        if 30 < temp < 100:\n",
    "            temp_2 = int(rate[0][0]) / int(rate[0][-1][0])\n",
    "            rate_num.append(math.ceil(temp_2))\n",
    "            rate_denom.append(10)\n",
    "        \n",
    "        # else if our number falls between [100, 200]\n",
    "        elif 100 < temp < 200:\n",
    "            temp_2 = int(rate[0][0]) / int(rate[0][-1][:2])\n",
    "            rate_num.append(math.ceil(temp_2))\n",
    "            rate_denom.append(10)\n",
    "        \n",
    "        # else just ceiling the number\n",
    "        else:\n",
    "            rate_num.append(math.ceil(temp))\n",
    "            rate_denom.append(10)\n",
    "    \n",
    "    # if our regex returned two ratings\n",
    "    elif len(rate) == 2:\n",
    "        \n",
    "        # restricting our ratings to a max of 15\n",
    "        if int(rate[0][0]) + int(rate[1][0]) > 15:\n",
    "            temp = (int(rate[0][0]) + int(rate[1][0]))/2\n",
    "            rate_num.append(math.ceil(temp))\n",
    "            rate_denom.append(10)\n",
    "        # if it is < 15\n",
    "        else:\n",
    "            rate_num.append(int(rate[0][0]) + int(rate[1][0]))\n",
    "            rate_denom.append(10)\n",
    "    \n",
    "    # all others lenghts\n",
    "    else:\n",
    "        temp_sum = 0\n",
    "        for i in range(len(rate)):\n",
    "            temp_sum += int(rate[i][0])\n",
    "        \n",
    "        scaled = scale_rate(temp_sum)\n",
    "        \n",
    "        rate_num.append(math.ceil(scaled))\n",
    "        rate_denom.append(10)\n",
    "\n",
    "# assigning the values to rating_numerator and denominator\n",
    "master_copy['rating_numerator'] = rate_num\n",
    "master_copy['rating_denominator'] = rate_denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1685 1685\n"
     ]
    }
   ],
   "source": [
    "# lenght of the newly obtained values equal to the number of datapoint in our dataset\n",
    "# i.e. 1686\n",
    "print(len(master_copy['rating_numerator']), len(master_copy['rating_denominator']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fix the structure of the table\n",
    "\n",
    "#### Define\n",
    "- Assign each column with appropriate type\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1685 entries, 0 to 1684\n",
      "Data columns (total 22 columns):\n",
      "tweet_id              1685 non-null int64\n",
      "timestamp             1685 non-null object\n",
      "source                1685 non-null object\n",
      "text                  1685 non-null object\n",
      "expanded_urls         1685 non-null object\n",
      "rating_numerator      1685 non-null int64\n",
      "rating_denominator    1685 non-null int64\n",
      "jpg_url               1685 non-null object\n",
      "img_num               1685 non-null float64\n",
      "p1                    1685 non-null object\n",
      "p1_conf               1685 non-null float64\n",
      "p1_dog                1685 non-null object\n",
      "p2                    1685 non-null object\n",
      "p2_conf               1685 non-null float64\n",
      "p2_dog                1685 non-null object\n",
      "p3                    1685 non-null object\n",
      "p3_conf               1685 non-null float64\n",
      "p3_dog                1685 non-null object\n",
      "favorites             1685 non-null float64\n",
      "retweets              1685 non-null float64\n",
      "dog_class             1685 non-null object\n",
      "dog_names             1685 non-null object\n",
      "dtypes: float64(6), int64(3), object(13)\n",
      "memory usage: 289.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# info about master_copy\n",
    "master_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing numpy\n",
    "import numpy as np\n",
    "\n",
    "# assign each column with appropriate type\n",
    "master_copy['tweet_id'] = master_copy['tweet_id'].astype(object)\n",
    "master_copy['timestamp'] = pd.to_datetime(master_copy.timestamp)\n",
    "master_copy['source'] = master_copy['source'].astype('category')\n",
    "master_copy['favorites'] = master_copy['favorites'].astype(np.int64)\n",
    "master_copy['retweets'] = master_copy['retweets'].astype(np.int64)\n",
    "master_copy['dog_class'] = master_copy['dog_class'].astype('category')\n",
    "master_copy['rating_numerator'] = master_copy['rating_numerator'].astype(np.int64)\n",
    "master_copy['rating_denominator'] = master_copy['rating_denominator'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1685 entries, 0 to 1684\n",
      "Data columns (total 22 columns):\n",
      "tweet_id              1685 non-null object\n",
      "timestamp             1685 non-null datetime64[ns]\n",
      "source                1685 non-null category\n",
      "text                  1685 non-null object\n",
      "expanded_urls         1685 non-null object\n",
      "rating_numerator      1685 non-null int64\n",
      "rating_denominator    1685 non-null int64\n",
      "jpg_url               1685 non-null object\n",
      "img_num               1685 non-null float64\n",
      "p1                    1685 non-null object\n",
      "p1_conf               1685 non-null float64\n",
      "p1_dog                1685 non-null object\n",
      "p2                    1685 non-null object\n",
      "p2_conf               1685 non-null float64\n",
      "p2_dog                1685 non-null object\n",
      "p3                    1685 non-null object\n",
      "p3_conf               1685 non-null float64\n",
      "p3_dog                1685 non-null object\n",
      "favorites             1685 non-null int64\n",
      "retweets              1685 non-null int64\n",
      "dog_class             1685 non-null category\n",
      "dog_names             1685 non-null object\n",
      "dtypes: category(2), datetime64[ns](1), float64(4), int64(4), object(11)\n",
      "memory usage: 266.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# printing information\n",
    "master_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Getting rid of predictions and add in final touches\n",
    "\n",
    "#### Define\n",
    "- Condense the `p_[1|2|3]` to `predicted_dog` and `conf_[1|2|3]` to `accuracy`.\n",
    "- drop columns `img_num`, `p1`, `p1_conf`, `p1_dog`, `p2`, `p2_conf`, `p2_dog`, `p3`, `p3_conf`, `p3_dog`.\n",
    "- rename columns that are apt to this dataset.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will store the fisrt true algorithm\n",
    "# with it's level of confidence\n",
    "predicted_dog_breed = []\n",
    "accuracy = []\n",
    "\n",
    "# funvtion for getting the levels\n",
    "def condense_predictions(dataframe):\n",
    "    '''\n",
    "    takes in the dataframe and extracts information for predicted dog breed.\n",
    "    dataframe: input to the fuction\n",
    "    '''\n",
    "    if dataframe['p1_dog'] == True:\n",
    "        predicted_dog_breed.append(dataframe['p1'])\n",
    "        accuracy.append(dataframe['p1_conf'])\n",
    "    elif dataframe['p2_dog'] == True:\n",
    "        predicted_dog_breed.append(dataframe['p2'])\n",
    "        accuracy.append(dataframe['p2_conf'])\n",
    "    elif dataframe['p3_dog'] == True:\n",
    "        predicted_dog_breed.append(dataframe['p3'])\n",
    "        accuracy.append(dataframe['p3_conf'])\n",
    "    else:\n",
    "        predicted_dog_breed.append('NaN')\n",
    "        accuracy.append(0)\n",
    "\n",
    "master_copy.apply(condense_predictions, axis=1)\n",
    "master_copy['dog_breeds'] = predicted_dog_breed\n",
    "master_copy['accuracy'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns\n",
    "master_copy.drop(['img_num',\n",
    "                  'p1', 'p1_conf', 'p1_dog',\n",
    "                  'p2', 'p2_conf', 'p2_dog',\n",
    "                  'p3', 'p3_conf', 'p3_dog'],\n",
    "                 axis = 1,\n",
    "                 inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final tocuh - renaming\n",
    "master_copy.rename(columns={'source': 'tweet_source',\n",
    "                            'text': 'tweet',\n",
    "                            'timestamp': 'tweet_date',\n",
    "                            'expanded_urls' : 'tweet_urls',\n",
    "                            'jpg_url': 'image_url',\n",
    "                            'favorites': 'tweet_favorites',\n",
    "                            'retweets': 'tweet_retweets'},\n",
    "                   inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1685 entries, 0 to 1684\n",
      "Data columns (total 14 columns):\n",
      "tweet_id              1685 non-null object\n",
      "tweet_date            1685 non-null datetime64[ns]\n",
      "tweet_source          1685 non-null category\n",
      "tweet                 1685 non-null object\n",
      "tweet_urls            1685 non-null object\n",
      "rating_numerator      1685 non-null int64\n",
      "rating_denominator    1685 non-null int64\n",
      "image_url             1685 non-null object\n",
      "tweet_favorites       1685 non-null int64\n",
      "tweet_retweets        1685 non-null int64\n",
      "dog_class             1685 non-null category\n",
      "dog_names             1685 non-null object\n",
      "dog_breeds            1685 non-null object\n",
      "accuracy              1685 non-null float64\n",
      "dtypes: category(2), datetime64[ns](1), float64(1), int64(4), object(6)\n",
      "memory usage: 161.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# printing information\n",
    "master_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing\n",
    "\n",
    "**Done with the process of cleaning, let's store and analyse this dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archive_copy.csv',\n",
       " 'image-predictions.tsv',\n",
       " 'master_set_raw.csv',\n",
       " 'tweet_json.txt',\n",
       " 'tweet_master.csv',\n",
       " 'twitter-archive-enhanced.csv',\n",
       " 'twitter_archive_master.csv']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the file to local disk\n",
    "master_copy.to_csv(folder+'/'+'twitter_archive_master.csv', index=False, encoding = 'utf-8')\n",
    "\n",
    "# listing the dataset folder\n",
    "os.listdir(folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
