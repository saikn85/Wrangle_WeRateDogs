{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "## Dataset - WeRateDogs&trade; Twitter Archive\n",
    "\n",
    "***By Kartik Nanduri***<br>\n",
    "**Dated: 21st Nov, 2018.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests as req\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [x] **The file given at hand `twitter-archive-enhanced.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the requried files for this project are in the list files_list\n",
    "files_list = ['twitter-archive-enhanced.csv', 'image-predictions.tsv', 'tweet_json.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the twitter archive file\n",
    "archive = pd.read_csv(files_list[0])\n",
    "\n",
    "# taking at random file entries for the archive file\n",
    "archive.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [x] **Fetching the data from url and saving it to local drive - `image-predictions.tsv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file from internet using the requests library\n",
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "res = req.get(url)\n",
    "\n",
    "with open(url.split('/')[-1], mode = \"wb\") as op_file:\n",
    "    op_file.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if fetched the data right way\n",
    "img_pre_test = pd.read_csv(files_list[1], delimiter = \"\\t\", encoding = 'utf-8')\n",
    "img_pre_test.sample(5)\n",
    "\n",
    "# we did it the right way, Yay! it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [x] **Getting data from Twitter&trade;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries for accessing Twitter via API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up all the necessary placeholders for API\n",
    "consumer_key = 'x'\n",
    "consumer_secret = 'x'\n",
    "access_token = 'x'\n",
    "access_secret = 'x'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth_handler = auth,\n",
    "                 wait_on_rate_limit = True, \n",
    "                 wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save(ids, api_ins, one_id = None):\n",
    "    '''\n",
    "    This function will fetch data with associated id in ids list\n",
    "    ids (List Object): a list all tweets\n",
    "    api_ins (Tweepy Object): api object instance, will be used to query twitter for data\n",
    "    one_id (int): use when you want to query only for one tweet\n",
    "    failed_ids (List Object): a list will be retured so that, this fuction can be called once again on those ids\n",
    "    '''\n",
    "    new_file_name = ''; failed_ids = []\n",
    "    \n",
    "    # checking if file exists\n",
    "    if os.path.exists(files_list[2]):\n",
    "        temp = [s for s in os.listdir() if \"tweet_json\" in s]\n",
    "        new_file_name = files_list[2].split('.')[0] + \"_\" + str(len(temp)) + \".txt\"\n",
    "    else:\n",
    "        new_file_name = files_list[2]\n",
    "    \n",
    "    # querying a list of ids\n",
    "    if one_id == None:\n",
    "        with open(new_file_name, mode = 'w') as outfile:\n",
    "            for one_id in ids:\n",
    "                try:\n",
    "                    page = api_ins.get_status(one_id, tweet_mode='extended')\n",
    "                    json.dump(page._json, outfile)\n",
    "                    outfile.write('\\n')\n",
    "                except Exception as e:\n",
    "                    print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "                    failed_ids.append(one_id)\n",
    "    \n",
    "    # querying a single id\n",
    "    else:\n",
    "        with open(new_file_name, mode = 'w') as outfile:\n",
    "            try:\n",
    "                page = api_ins.get_status(one_id, tweet_mode='extended')\n",
    "                json.dump(page._json, outfile)\n",
    "                outfile.write('\\n')\n",
    "            except Exception as e:\n",
    "                print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "                failed_ids.append(one_id)\n",
    "    \n",
    "    return failed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# passing the list of ids to the fuction fetch_and_save()\n",
    "tweet_ids = archive['tweet_id'].tolist()\n",
    "\n",
    "# fetching data 1st iteration\n",
    "test_one = fetch_and_save(tweet_ids, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of erroneous ids\n",
    "len(test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if test_one has duplicate ids\n",
    "assert len(test_one) == len(set(test_one))\n",
    "\n",
    "# we can see that there are no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing this failed list to the fetch_and_save function\n",
    "\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# fetching data 2nd iteration\n",
    "test_two = fetch_and_save(test_one, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} secs.\".format(round(end - start, 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
