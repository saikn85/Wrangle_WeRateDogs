{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "## Dataset - WeRateDogs&trade; Twitter Archive\n",
    "\n",
    "***By: Kartik Nanduri***<br>\n",
    "**Date: 21st Nov, 2018.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests as req"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***<span style=\"color: red\">Important! uncomment the following files to run the book with out errors.</span>***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the folder structure.\n",
    "#os.rename('dataset/twitter-archive-enhanced.csv', 'twitter-archive-enhanced.csv')\n",
    "#import shutil\n",
    "#shutil.rmtree('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***<span style=\"color: green\">Important! once done, please recomment.</span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [x] **The file given at hand `twitter-archive-enhanced.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the requried files for this project are in the list files_list\n",
    "files_list = ['twitter-archive-enhanced.csv', 'image-predictions.tsv', 'tweet_json.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2356, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the twitter archive file\n",
    "archive = pd.read_csv(files_list[0])\n",
    "\n",
    "# taking at random entries for the archive file\n",
    "archive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [x] **Fetching the data from url and saving it to local drive - `image-predictions.tsv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file from internet using the requests library\n",
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "res = req.get(url)\n",
    "\n",
    "with open(url.split('/')[-1], mode = \"wb\") as op_file:\n",
    "    op_file.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1_conf</th>\n",
       "      <th>p1_dog</th>\n",
       "      <th>p2</th>\n",
       "      <th>p2_conf</th>\n",
       "      <th>p2_dog</th>\n",
       "      <th>p3</th>\n",
       "      <th>p3_conf</th>\n",
       "      <th>p3_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>700062718104104960</td>\n",
       "      <td>https://pbs.twimg.com/media/CbcfUxoUAAAlHGK.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>hummingbird</td>\n",
       "      <td>0.180998</td>\n",
       "      <td>False</td>\n",
       "      <td>peacock</td>\n",
       "      <td>0.135179</td>\n",
       "      <td>False</td>\n",
       "      <td>eel</td>\n",
       "      <td>0.075371</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>827600520311402496</td>\n",
       "      <td>https://pbs.twimg.com/media/C3w6RYbWQAAEQ25.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Pembroke</td>\n",
       "      <td>0.325638</td>\n",
       "      <td>True</td>\n",
       "      <td>golden_retriever</td>\n",
       "      <td>0.317235</td>\n",
       "      <td>True</td>\n",
       "      <td>Labrador_retriever</td>\n",
       "      <td>0.116087</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                          jpg_url  \\\n",
       "903   700062718104104960  https://pbs.twimg.com/media/CbcfUxoUAAAlHGK.jpg   \n",
       "1770  827600520311402496  https://pbs.twimg.com/media/C3w6RYbWQAAEQ25.jpg   \n",
       "\n",
       "      img_num           p1   p1_conf  p1_dog                p2   p2_conf  \\\n",
       "903         1  hummingbird  0.180998   False           peacock  0.135179   \n",
       "1770        1     Pembroke  0.325638    True  golden_retriever  0.317235   \n",
       "\n",
       "      p2_dog                  p3   p3_conf  p3_dog  \n",
       "903    False                 eel  0.075371   False  \n",
       "1770    True  Labrador_retriever  0.116087    True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if fetched the data right way\n",
    "img_pre_test = pd.read_csv(files_list[1], delimiter = \"\\t\", encoding = 'utf-8')\n",
    "img_pre_test.sample(2)\n",
    "\n",
    "# we did it the right way, Yay! it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [x] **Getting data from Twitter&trade;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries for accessing Twitter via API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up all the necessary placeholders for API\n",
    "consumer_key = 'xxx.xxx.xxx.xxx'\n",
    "consumer_secret = 'xxx.xxx.xxx.xxx'\n",
    "access_token = 'xxx.xxx.xxx.xxx'\n",
    "access_secret = 'xxx.xxx.xxx.xxx'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth_handler = auth,\n",
    "                 parser = tweepy.parsers.JSONParser(),\n",
    "                 wait_on_rate_limit = True, \n",
    "                 wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save(ids, api_ins, one_id = None):\n",
    "    '''\n",
    "    This function will fetch data with associated id in ids list\n",
    "    ids (List Object): a list all tweets\n",
    "    api_ins (Tweepy Object): api object instance, will be used to query twitter for data\n",
    "    one_id (int): use when you want to query only for one tweet\n",
    "    failed_ids (List Object): a list will be retured so that, this fuction can be called once again on those ids\n",
    "    '''\n",
    "    new_file_name = ''; failed_ids = []; tweet_df = []\n",
    "    \n",
    "    # checking if file exists\n",
    "    if os.path.exists(files_list[2]):\n",
    "        temp = [s for s in os.listdir() if \"tweet_json\" in s]\n",
    "        new_file_name = files_list[2].split('.')[0] + \"_\" + str(len(temp)) + \".txt\"\n",
    "    else:\n",
    "        new_file_name = files_list[2]\n",
    "    \n",
    "    # querying a list of ids\n",
    "    if one_id == None:\n",
    "        with open(new_file_name, mode = 'w') as outfile:\n",
    "            for one_id in ids:\n",
    "                try:\n",
    "                    content = api_ins.get_status(one_id, tweet_mode='extended')\n",
    "                    json.dump(content, outfile)\n",
    "                    outfile.write('\\n')\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "                    failed_ids.append(one_id)\n",
    "    \n",
    "    # querying a single id\n",
    "    else:\n",
    "        try:\n",
    "            content = api_ins.get_status(one_id, include_entities = True)\n",
    "            favorites = content['favorite_count']\n",
    "            retweets = content['retweet_count']\n",
    "            \n",
    "            tweet_df.append({'tweet_id': int(one_id),\n",
    "                        'favorites': int(favorites),\n",
    "                        'retweets': int(retweets)})\n",
    "            \n",
    "            return tweet_df\n",
    "                           \n",
    "        except Exception as e:\n",
    "            print(\"Error for: \" + str(one_id) + \" - \" + str(e))\n",
    "            failed_ids.append(one_id)\n",
    "\n",
    "    return failed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 900 556 2356\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# passing the list of ids to the fuction fetch_and_save(), but in batches\n",
    "# given that we can request 900 request/15min - window, let's break our ids into\n",
    "tweet_ids = archive['tweet_id'].tolist()\n",
    "\n",
    "# set_one, two and three\n",
    "set_one = tweet_ids[0:900]; set_two = tweet_ids[900:1800]; set_three = tweet_ids[1800:]\n",
    "\n",
    "# checking the lengths so that we send 900 ids/requests.\n",
    "print(len(set_one), len(set_two), len(set_three), len(set_one)+len(set_two)+len(set_three))\n",
    "print(len(set_one)+len(set_two)+len(set_three) == len(tweet_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for: 888202515573088257 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 873697596434513921 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 872668790621863937 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 869988702071779329 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 866816280283807744 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 861769973181624320 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 845459076796616705 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 842892208864923648 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 837012587749474308 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 827228250799742977 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 802247111496568832 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 775096608509886464 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 770743923962707968 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "That took about 6.6 mins.\n"
     ]
    }
   ],
   "source": [
    "# fetching data 1st iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_one = fetch_and_save(set_one, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have about 13 failed requests.\n"
     ]
    }
   ],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_one)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for: 758740312047005698 - Failed to send request: ('Connection aborted.', OSError(\"(10054, 'WSAECONNRESET')\"))\n",
      "Error for: 754011816964026368 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "That took about 13.7 mins.\n"
     ]
    }
   ],
   "source": [
    "# fetching data 2nd iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_two = fetch_and_save(set_two, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have about 2 failed requests.\n"
     ]
    }
   ],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_two)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for: 676957860086095872 - Failed to send request: ('Connection aborted.', OSError(\"(10054, 'WSAECONNRESET')\"))\n",
      "That took about 9.8 mins.\n"
     ]
    }
   ],
   "source": [
    "# fetching data 3rd iteration\n",
    "# starting the timer\n",
    "start = timer()\n",
    "\n",
    "# querying\n",
    "test_three = fetch_and_save(set_three, api)\n",
    "\n",
    "# ending the timer\n",
    "end = timer()\n",
    "\n",
    "# calculating the runtime for fetch_and_save\n",
    "print(\"That took about {} mins.\".format(round((end - start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have about 1 failed requests.\n"
     ]
    }
   ],
   "source": [
    "# no of erroneous ids\n",
    "print(\"we have about {} failed requests.\".format(len(test_three)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total failed request are: 16. \n",
      "\n",
      "Error for: 888202515573088257 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 873697596434513921 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 872668790621863937 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 869988702071779329 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 866816280283807744 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 861769973181624320 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 845459076796616705 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 842892208864923648 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 837012587749474308 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 827228250799742977 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 802247111496568832 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 775096608509886464 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 770743923962707968 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "Error for: 754011816964026368 - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "\n",
      "We were able to retrieve 2 records, others failed.\n"
     ]
    }
   ],
   "source": [
    "# lets save the failed ids into one master list\n",
    "failed_ids = test_one + test_two + test_three\n",
    "print(\"Total failed request are: {}. \\n\".format(len(failed_ids)))\n",
    "\n",
    "# ids that failed and the ones that passed\n",
    "indi_fail = []; success = []\n",
    "\n",
    "#for each failed id, lets try to fetch status individually.\n",
    "for failed_id in failed_ids:\n",
    "    temp = fetch_and_save(ids = None, api_ins = api, one_id = failed_id)\n",
    "    indi_fail.append(temp[0])\n",
    "\n",
    "# removing empty elements from list\n",
    "success = [x for x in indi_fail if not isinstance(x, (int))]\n",
    "indi_fail = [x for x in indi_fail if isinstance(x, (int))]\n",
    "\n",
    "# checking if there is change\n",
    "print(\"\\nWe were able to retrieve {} records, others failed.\".format(len(failed_ids) - len(indi_fail)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [x] **Okay, let's combine the successful jsons into one file, called the `tweet_master.txt`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((887, 32), (898, 32), (555, 28))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all successful jsons into one master file\n",
    "json_1 = pd.read_json('tweet_json.txt', lines = True, encoding = 'utf-8')\n",
    "json_2 = pd.read_json('tweet_json_1.txt', lines = True, encoding = 'utf-8')\n",
    "json_3 = pd.read_json('tweet_json_2.txt', lines = True, encoding = 'utf-8')\n",
    "\n",
    "# total rows that we need to have in our resulting dataframe\n",
    "json_1.shape, json_2.shape, json_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_master = pd.concat([json_1, json_2, json_3], ignore_index = True, join = 'outer', sort = True)\n",
    "json_master.to_json('tweet_master.txt', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing objects that are not required.\n",
    "del archive, img_pre_test\n",
    "del json_1, json_2, json_3, json_master\n",
    "del indi_fail, end, start, test_one, test_two, test_three, set_one, set_two, set_three\n",
    "del consumer_key, consumer_secret, access_token, access_secret, auth, api\n",
    "\n",
    "# we are not removing success and files_list, making sure we stick to good programming practices - reusablity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [x] **Last thing to do is to tidy up our folder, let's get going.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'dataset',\n",
       " 'error.png',\n",
       " 'New Text Document.txt',\n",
       " 'README.md',\n",
       " 'wrangle_act.ipynb']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moving all data files under one folder - dataset\n",
    "# removing the temporary files, that acted as placeholders\n",
    "\n",
    "# creating the folder\n",
    "folder = 'dataset'\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "# we know that our master datasets for this project are\n",
    "# 1. twitter-archive-enhanced.csv\n",
    "# 2. image-predictions.tsv\n",
    "# 3. tweet_json_master.txt\n",
    "# let us move these files\n",
    "\n",
    "# updating our files_list\n",
    "files_list[-1] = 'tweet_master.txt'\n",
    "\n",
    "# moving only required files\n",
    "for file in files_list:\n",
    "    if os.path.exists(file):\n",
    "        os.rename(file, folder+'/'+file)\n",
    "\n",
    "# removing the tweet_json and tweet_json_1 files as they are not required anymore\n",
    "for file in [s for s in os.listdir() if \"tweet_json\" in s]:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    \n",
    "# lisitng the current directory\n",
    "os.listdir()\n",
    "\n",
    "# clean and neat, lets get with assessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming files_list\n",
    "for i in range(3):\n",
    "    files_list[i] = folder + '/'+ files_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [x] **Last thing to do is to extract `retweet_count` and `favourite_count` from `tweet_master.txt`, saving the result as .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>668268907921326080</td>\n",
       "      <td>243</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>810254108431155201</td>\n",
       "      <td>3699</td>\n",
       "      <td>15766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>675109292475830276</td>\n",
       "      <td>1190</td>\n",
       "      <td>2884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  retweets  favorites\n",
       "2211  668268907921326080       243        565\n",
       "510   810254108431155201      3699      15766\n",
       "1861  675109292475830276      1190       2884"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_json = pd.read_json(files_list[2], lines = True, encoding = 'utf-8')\n",
    "tweet_json = tweet_json[['id', 'retweet_count','favorite_count']]\n",
    "tweet_json.rename(index = str, columns={'id' : 'tweet_id', 'retweet_count': 'retweets','favorite_count': 'favorites'}, inplace = True)\n",
    "tweet_json.append(success, ignore_index = True)\n",
    "tweet_json.to_json(files_list[2], orient = 'records', lines = True)\n",
    "tweet_json.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We know, that gathering is a the first step in wrangling.\n",
    "- We were successful in gathering from three different sources with different techniques:\n",
    "    - Data given at hand.\n",
    "    - Fetch from flat file stored on a server.\n",
    "    - From API.\n",
    "\n",
    "- There a total of 14 missing data points, tried a different ways for retrieving them, using the API as well as `twurl` of the `Ruby` package, but they were not to be found, as stated below in the highlighted section.\n",
    "\n",
    "***<span style=\"color: ##6c6cff\">So let's start with assessing the data.</span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![error](error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
